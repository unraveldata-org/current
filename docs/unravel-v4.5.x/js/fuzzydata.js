$(document).ready(function () {indexDict['en'] = [{ "title" : "Installation", 
"url" : "102047-install.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation", 
"snippet" : "You must install MySQL as the Unravel DB. Each's platform installation walks you through installing MySQL when the necessary....", 
"body" : "You must install MySQL as the Unravel DB. Each's platform installation walks you through installing MySQL when the necessary. " }, 
{ "title" : "Quick start guide", 
"url" : "102048-install-quickstart.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Quick start guide", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "New installations", 
"url" : "102048-install-quickstart.html#UUID-838ae1bc-233b-4e1d-1e9e-d9d7f0afa073_section-5ccb7d1320523-idm45390849814496", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Quick start guide \/ New installations", 
"snippet" : "Follow the installation guide for your platform . Add more instrumentation and configure optional functionality ....", 
"body" : "Follow the installation guide for your platform . Add more instrumentation and configure optional functionality . " }, 
{ "title" : "Upgrades", 
"url" : "102048-install-quickstart.html#UUID-838ae1bc-233b-4e1d-1e9e-d9d7f0afa073_section-5ccb7d8bf077a-idm46399446301408", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Quick start guide \/ Upgrades", 
"snippet" : "For upgrade instructions, see ....", 
"body" : "For upgrade instructions, see . " }, 
{ "title" : "Uninstallations", 
"url" : "102048-install-quickstart.html#UUID-838ae1bc-233b-4e1d-1e9e-d9d7f0afa073_section-5ccb7df38f117-idm45390849682432", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Quick start guide \/ Uninstallations", 
"snippet" : "For uninstallation instructions, see ....", 
"body" : "For uninstallation instructions, see . " }, 
{ "title" : "Unravel deployment for high availability", 
"url" : "102049-install-ha-deploy.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Unravel deployment for high availability", 
"snippet" : "The Unravel service can preserve business continuity after a disruptive event, for example, a node failure. To preserve continuity, you must: Run two Unravel servers in Active-Passive mode. Have a load balancer fronting the network traffic from Unravel sensors, Users, and REST API calls. Run Unravel...", 
"body" : "The Unravel service can preserve business continuity after a disruptive event, for example, a node failure. To preserve continuity, you must: Run two Unravel servers in Active-Passive mode. Have a load balancer fronting the network traffic from Unravel sensors, Users, and REST API calls. Run Unravel’s data stores in the HA configuration. Impact of the switch over from the failed to passive server During the brief time period traffic from Unravel sensors and REST API calls is lost. User Sessions are not preserved.  Deployment architecture " }, 
{ "title" : "Platforms", 
"url" : "102050-install-platforms.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Cloudera Distribution of Apache Hadoop (CDH)", 
"url" : "102051-install-cdh.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Cloudera Distribution of Apache Hadoop (CDH)", 
"snippet" : "CDH is Cloudera's distribution of Apache Hadoop and related process, management, storage, and security components (Spark, Hive, Pig, MapReduce, Impala, HBase, Kafka, Sentry, and so on). This section explains how to deploy Unravel Server and Sensors on the Cloudera distribution of Apache Hadoop (CDH)...", 
"body" : "CDH is Cloudera's distribution of Apache Hadoop and related process, management, storage, and security components (Spark, Hive, Pig, MapReduce, Impala, HBase, Kafka, Sentry, and so on). This section explains how to deploy Unravel Server and Sensors on the Cloudera distribution of Apache Hadoop (CDH), with Cloudera Manager (CM). This section shows you how to: Do a pre-installation check. Perform the MySQL pre-installation steps. Install the Unravel RPM. Perform the MySQL post-installation steps. Configure Unravel Server. Distribute Unravel's sensor JARs to desired nodes in the cluster. Adjust your cluster's configuration to integrate with Unravel. Restart needed components on your cluster. Configure additional integrations like Kafka and LDAP. " }, 
{ "title" : "Prerequisites", 
"url" : "102052-cdh-pre.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Cloudera Distribution of Apache Hadoop (CDH) \/ Prerequisites", 
"snippet" : "To deploy Unravel, first ensure that your environment meets these requirements. You must use an independent host for the Unravel server. This host must: Be managed by Cloudera. Have Hadoop clients pre-installed. Have no other Hadoop service or third-party applications installed. Accessible to only H...", 
"body" : "To deploy Unravel, first ensure that your environment meets these requirements. You must use an independent host for the Unravel server. This host must: Be managed by Cloudera. Have Hadoop clients pre-installed. Have no other Hadoop service or third-party applications installed. Accessible to only Hadoop and Unravel Admins. Platform Each version of Unravel has specific platform requirements. Confirm that your cluster meets the requirements for the version of Unravel that you're installing . Your CDH environment must be running Cloudera Manager (CM). compmatrix-platform Sizing You must have separate nodes for the Unravel server and the external MySQL database . Unravel Server Architecture: x86_64 vm.max_map_count is set to 262144 Minimum requirements for cores, RAM, and disks: The table below lists the minimum requirements for cores, RAM, and disks for a typical environment with default data retention and lookback settings . \/usr\/local\/unravel is the storage location for Unravel binaries. \/srv\/unravel is used for Elasticsearch (ES) and the bundled database. In production environments, put \/usr\/local\/unravel and \/srv\/unravel on separate disks. Putting \/srv\/unravel on a separate high spin HDD with its own SATAIII (or equivalent) bus significantly increases IO bandwidth. If \/usr\/local\/unravel or \/srv\/unravel doesn't have the minimum free space shown in the table below, create symbolic links for them to another disk. To check the space on a volume use the df command. For example, df -h \/srv Jobs per day Cores RAM \/usr\/local\/unravel \/srv\/unravel Less than 50,000 8 96 GB 8 GB free 500 GB free 50,000 to 100,000 to 8 128GB 8 GB free 500 GB free Over 100,000 Contact Unravel Support All volumes are mounted. \/tmp is mounted with executable permissions. To re-mount \/tmp with executable permissions use the following command: mount -o remount,exec \/tmp MySQL Server Minimum requirements for cores, RAM, and disk. Jobs per day Data retention length Cores RAM Disk Less than 50,000 30 days 4 32 GB 1 TB 60 days 4 32 GB 2 TB 50,000 to 100,000 to 30 days 8 64 GB 2 TB 60 days 8 64 GB 4 TB Over 100,000 Contact Unravel Support Software If the Unravel host is running Red Hat Enterprise Linux (RHEL) 6.x, set its bootstrap.system_call_filter to false in elasticsearch.yml : bootstrap.system_call_filter: false libaio.x86_64 is installed. If you're installing Unravel version 4.5.0.0, set SELINUX to permissive or disabled in \/etc\/sysconfig\/selinux . If you're installing Unravel version 4.5.0.1+, SELINUX can be set to enabled . PATH includes the path to the HDFS+Hive+YARN+Spark client\/gateway, Hadoop commands, and Hive commands. If Spark2 service is installed, the Unravel host should be a client\/gateway. Zookeeper is not installed on the same host as the Unravel host. NTP is running and in-sync with the cluster. Permissions The installation creates a local user unravel:unravel , but you can change this later. You must have root access or \"sudo root\" permission in order to install the Unravel Server RPM. If you're using Kerberos, we'll explain how to create a principal and keytab for Unravel daemons to use to access these HDFS resources: MapReduce logs ( hdfs:\/\/user\/history ) YARN's log aggregation directory ( hdfs:\/\/tmp\/logs ) Spark and Spark2 event logs ( hdfs:\/\/user\/spark\/applicationHistory and hdfs:\/\/user\/spark\/spark2ApplicationHistory ) File and partition sizes in the Hive warehouse directory (typically hdfs:\/\/apps\/hive\/warehouse ) Unravel needs access to the YARN Resource Manager's REST API (so that the principal can determine which resource manager is active). Unravel needs access to the JDBC access to the Hive Metastore. Read-only access is sufficient. If you're using Impala, Unravel needs access to the Cloudera Manager API. Read-only access is sufficient. Network On the new node, open the following ports: Port(s) Direction Description 3000 Both Traffic to and from Unravel UI 3316 Both Database traffic 4020 Both Unravel APIs 4021 Both Host monitoring of JMX on localhost 4031 Both Database traffic 4043 In UDP and TCP ingest traffic from the entire cluster to Unravel Server(s) 4044-4049 In UDP and TCP ingest spares for unravel_lr* 4091-4099 Both Kafka brokers 4171-4174, 4176-4179 Both ElasticSearch; localhost communication between Unravel daemons or Unravel Servers in a multi-host deployment 4181-4189 Both Zookeeper daemons 4210 Both Cluster access service HDFS ports Both Traffic to\/from the cluster to Unravel Server(s) Hive metadata database port Out For YARN only. Traffic from Hive to Unravel Server(s) for partition reporting 8088 Out Traffic from Unravel Server(s) to the Resource Manager API 8188 Out Traffic from Unravel Server(s) to the ATS server(s) 11000 Out For Oozie only. Traffic from Unravel Server(s) to the Oozie server CDH-specific port requirements Port(s) Direction Description 3000 Both Traffic to and from Unravel UI If you plan to use Cloudera Manager to install Unravel's sensors, the Cloudera Manager service must also be able to reach the Unravel host on port 3000. 7180 (or 7183 for HTTPS) Out Traffic from Unravel Server(s) to Cloudera Manager " }, 
{ "title" : "Part 1: Installing Unravel Server on CDH+CM", 
"url" : "102053-cdh-part1.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Cloudera Distribution of Apache Hadoop (CDH) \/ Part 1: Installing Unravel Server on CDH+CM", 
"snippet" : "This topic explains how to deploy Unravel Server on Cloudera Distribution of Hadoop (CDH). Your CDH environment must be running Cloudera Manager (CM). If you have not already done so, confirm your cluster meets Unravel's hosting requirements . 1. Configure the host Use Cloudera Manager to allocate a...", 
"body" : "This topic explains how to deploy Unravel Server on Cloudera Distribution of Hadoop (CDH). Your CDH environment must be running Cloudera Manager (CM). If you have not already done so, confirm your cluster meets Unravel's hosting requirements . 1. Configure the host Use Cloudera Manager to allocate a cluster gateway\/edge\/client host with HDFS access, and create a gateway configuration for the host. The gateway configuration must have client roles for HDFS, YARN, Spark, Hive, and optionally, Spark2. 3. Install Unravel Server on the host Download the Unravel Server RPM. downloads Ensure that the host machine's local disks have the minimum space required . Unravel Server uses two separate disks: one for binaries ( \/usr\/local\/unravel ) and one for data ( \/srv\/unravel ). The separate disk \/srv\/unravel is beneficial for performance. If either disk doesn't have the minimum space required, create symbolic links for them to another disk drive. To check the space on a volume use the df command. For example, df -h \/srv Install the Unravel Server RPM. sudo rpm -Uvh unravel- version .rpm The installation creates the following items: \/usr\/local\/unravel\/ , which contains executables, scripts, properties file ( unravel.properties ), and logs. \/etc\/init.d\/unravel_* , which contains scripts for controlling services, such as unravel_all.sh for manually stopping, starting, and getting the status of all daemons in proper order. User unravel if it doesn't exist already. 5. Configure Unravel Server with basic options (Optional) Enable additional daemons for high-volume workloads . In \/usr\/local\/unravel\/etc\/unravel.properties , set general properties for Unravel Server. Property\/Description Set by user Unit Default com.unraveldata.customer.organization Customer name. Used to identify your installation for reporting and notification purposes in Unravel UI. Optional string Not Set com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. Example: http:\/\/unravelserver.company.com:3000   string http:\/\/{host}:3000 com.com.unraveldata.hdfs.timezone Timezone of HDFS, for example, US\/Eastern, Etc\/GMT-4, America\/New_York. If the timezone is not set then an error message is logged and UTC timezone is used. Possible timezones can be obtained by calling TimeZone.getAvailableIDs() . string - com.unraveldata.tmpdir The base location for Unravel process control files where Unravel's temp files reside. string (path) \/srv\/unravel\/tmp com.unraveldata.history.maxSize.weeks Number of weeks retained for search results in Elastic Search. integer 5 com.unraveldata.retention.max.days Number of days to keep the heaviest data (such as error logs and drill-down details) in the SQL Database. integer 30 Point Unravel Server to logs on HDFS. Unravel collects HDFS logs for analysis. To point Unravel Server to these logs, set the following properties in \/usr\/local\/unravel\/etc\/unravel.properties : Property\/Description Set by user Unit Default com.unraveldata.job.collector.done.log.base HDFS path to \"done\" directory of MR logs. Don't include the hdfs:\/\/ prefix For HDP set this to: \/mr-history\/done . string \/user\/history\/done com.unraveldata.job.collector.log.aggregation.base HDFS path to the aggregated container logs (logs to process). Don't include the hdfs:\/\/ prefix. The log format defaults to TFile. You can specify multiple logs and log formats (TFile or IndexedFormat). Example: TFile:\/tmp\/logs\/*\/logs\/,IndexedFormat:\/tmp\/logs\/*\/logs-ifile\/. For HDP set this to: IndexedFormat:\/app-logs\/*\/logs\/ . CSL \/tmp\/logs\/*\/logs\/ com.unraveldata.spark.eventlog.location Comma-separated list of HDFS paths to the Spark event logs. Each path must include the hdfs:\/\/\/ prefix. For HDP set this to: hdfs:\/\/\/spark1-history\/,hdfs:\/\/\/spark2-history\/ . CSL hdfs:\/\/\/user\/spark\/applicationHistory\/ For example, com.unraveldata.job.collector.done.log.base=\/user\/history\/done\ncom.unraveldata.job.collector.log.aggregation.base=\/tmp\/logs\ncom.unraveldata.spark.eventlog.location=hdfs:\/\/user\/spark\/applicationHistory,hdfs:\/\/user\/spark\/spark2 To confirm that you have the right path, use the hdfs dfs -ls command. For example, hdfs dfs -ls \/user\/history\/done\nhdfs dfs -ls \/tmp\/logs If Kerberos is enabled, create or identify a principal and keytab for Unravel daemons to use for access to HDFS and the REST API. If Sentry is enabled: Create your own alternate principal with narrow privileges and HDFS access permissions . Verify that the user running the Unravel daemon \/etc\/unravel_ctl has the permissions shown in the table below. Resource Principal Permission Purpose hdfs:\/\/user\/spark\/applicationHistory Your alt principal read+execute Spark event log hdfs:\/\/user\/spark\/spark2ApplicationHistory Your alt principal read+execute Spark2 event log (if Spark2 is installed) hdfs:\/\/user\/history Your alt principal read+execute MapReduce logs hdfs:\/\/tmp\/logs Your alt principal read+execute YARN aggregation folder hdfs:\/\/user\/hive\/warehouse Your alt principal read+execute Obtain table partition sizes with \"stat\" only 8. Start Unravel services Run the following command to start all Unravel services: sudo \/etc\/init.d\/unravel_all.sh start\nsleep 60 This completes the basic\/core configuration. 9. Log into Unravel UI Find the hostname of Unravel Server. echo \"http:\/\/$(hostname -f):3000\/\" If you're using an SSH tunnel or HTTP proxy, you might need to make adjustments. Using a supported web browser , navigate to http:\/\/ unravel-host :3000 and log in with username admin with password unraveldata . compmatrix-platform Unravel UI displays collected data . 10. Enable additional instrumentation " }, 
{ "title" : "2. Install MySQL", 
"url" : "102053-cdh-part1.html#UUID-0d456dd4-458c-f0dc-a88e-643bd2b41cc5_section-5cd09b081436d-idm45444639878864", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Cloudera Distribution of Apache Hadoop (CDH) \/ Part 1: Installing Unravel Server on CDH+CM \/ 2. Install MySQL", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "4. Configure MySQL", 
"url" : "102053-cdh-part1.html#UUID-0d456dd4-458c-f0dc-a88e-643bd2b41cc5_section-5cd09bd696d67-idm45444640181408", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Cloudera Distribution of Apache Hadoop (CDH) \/ Part 1: Installing Unravel Server on CDH+CM \/ 4. Configure MySQL", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "6. Change the run-as user and group for Unravel daemons", 
"url" : "102053-cdh-part1.html#UUID-0d456dd4-458c-f0dc-a88e-643bd2b41cc5_section-5d154ff93e476-idm45375029785472", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Cloudera Distribution of Apache Hadoop (CDH) \/ Part 1: Installing Unravel Server on CDH+CM \/ 6. Change the run-as user and group for Unravel daemons", 
"snippet" : "Unravel daemons run under the local user unravel by default. However, if you have Kerberos or Sentry enabled, or a non-Kerberos cluster with simple Unix security, or a different username for the Unravel user, or a non-local user such as an LDAP user, run switch_to_user.sh script to change the Unix o...", 
"body" : "Unravel daemons run under the local user unravel by default. However, if you have Kerberos or Sentry enabled, or a non-Kerberos cluster with simple Unix security, or a different username for the Unravel user, or a non-local user such as an LDAP user, run switch_to_user.sh script to change the Unix owner and group of the Unravel daemons. " }, 
{ "title" : "7. Connect to the Hive metastore", 
"url" : "102053-cdh-part1.html#UUID-0d456dd4-458c-f0dc-a88e-643bd2b41cc5_section-5d15503767fe2-idm45764268545072", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Cloudera Distribution of Apache Hadoop (CDH) \/ Part 1: Installing Unravel Server on CDH+CM \/ 7. Connect to the Hive metastore", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Part 2: Enabling additional instrumentation", 
"url" : "102054-cdh-part2.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Cloudera Distribution of Apache Hadoop (CDH) \/ Part 2: Enabling additional instrumentation", 
"snippet" : "This topic explains how to enable additional instrumentation on your gateway\/edge\/client nodes that are used to submit jobs to your big data platform. Additional instrumentation can include: Hive queries in Hadoop, pushed to Unravel Server by the Hive Hook sensor, a JAR file Spark job performance me...", 
"body" : "This topic explains how to enable additional instrumentation on your gateway\/edge\/client nodes that are used to submit jobs to your big data platform. Additional instrumentation can include: Hive queries in Hadoop, pushed to Unravel Server by the Hive Hook sensor, a JAR file Spark job performance metrics, pushed to Unravel Server by the Spark sensor, a JAR file Impala queries, pulled from Cloudera Manager Sensor JARs are packaged in a parcel on Unravel Server. 1. Distribute the Unravel parcel In Cloudera Manager, go to the Parcels page by clicking the parcels glyph ( ) on the top of the page. Click Configuration to see the Parcel Settings pop-up. In the Parcel Settings pop-up, go to the Remote Parcel Repository URLs section, and click the + glyph to add a new entry. In a new browser tab, copy the exact directory name for your CDH version from the http:\/\/ unravel-host :3000\/parcels\/ directory. For example, the exact directory name might be cdh5.16 or cdh6.0 . Add http:\/\/ unravel-host :3000\/parcels\/ cdh-version \/ (including the trailing slash). Where: cdh-version is your version of CDH. For example, cdh5.16 or cdh6.0 . unravel-host is the host name or LAN IP address of Unravel Server. On a multi-host Unravel Server, this would be the host where the unravel_lr daemon is running. If you're using Active Directory Kerberos, unravel-host must be a fully qualified domain name or IP address. If you're running more than one version of CDH (for example, you have multiple clusters), you can add more than one parcel entry for unravel-host . Click Save . Click Check for New Parcels . On the Parcels page, pick a target cluster in the Location box. In the list of Parcel Names , find the UNRAVEL_SENSOR parcel that matches the version of the target cluster and click Download . Click Distribute . If you have an old parcel from Unravel, deactivate it now. On the new parcel, click Activate . 2. Put the Hive Hook JAR in AUX_CLASSPATH In Cloudera Manager, select the target cluster, click Hive | Configuration , and search for hive-env . In Gateway Client Environment Advanced Configuration Snippet (Safety Valve) for hive-env.sh enter the following exactly as shown, with no subsitutions: AUX_CLASSPATH=${AUX_CLASSPATH}:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar Click YARN | Configuration , and search for hadoop-env . In Gateway Client Environment Advanced Configuration Snippet (Safety Valve) for hadoop-env.sh , enter the following exactly as shown, with no subsitutions: HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar If Sentry is enabled, grant privileges on the JAR files to the Sentry roles that run Hive queries. Sentry commands may also be needed to enable access to the Hive Hook JAR file. Grant privileges on the JAR files to the roles that run hive queries. Log into Beeline as user hive and use the Hive SQL GRANT statement to do so. For example (substitute role as appropriate), GRANT ALL ON URI 'file:\/\/\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar' TO ROLE role 4. Deploy the Hive Hook JAR Copy the Hive Hook snippet into hive-site.xml : From the Unravel server OS use the command cat \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip , copy the contents, and paste them into hive-site.xml . On a multi-host Unravel Server deployment, use the \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip snippet from host2. In Cloudera Manager, go to the Hive service. Select the Configuration tab. Search for hive-site.xml in the middle of the page. Add the snippet to Hive Client Advanced Configuration Snippet for hive-site.xml (Gateway Default Group). To edit, click View as XML . If you configured CDH with Cloudera Navigator's safety valve settings, your CDH hive.exec.post.hooks property has existing value(s); in which case, prepend Unravel's value into the existing values with a comma and no space, like this: <property> \n<name>hive.exec.post.hooks<\/name> \n<value>com.unraveldata.dataflow.hive.hook.HivePostHook,com.cloudera.navigator.audit.hive.HiveExecHookContext,org.apache.hadoop.hive.ql.hooks.LineageLogger<\/value> \n<description>for Unravel, from unraveldata.com<\/description>\n<\/property> The Hive Client Advanced Configuration Snippet for hive-site.xml should only have the Unravel class. Add the snippet to HiveServer2 Advanced Configuration Snippet for hive-site.xml . To edit, click View as XML . Like the step above, if the CDH hive.exec.post.hooks property exists already, prepend Unravel's value to it, with a comma and no space, like this: <property>\n<name>hive.exec.post.hooks<\/name> \n<value>com.unraveldata.dataflow.hive.hook.HivePostHook,com.cloudera.navigator.audit.hive.HiveExecHookContext,org.apache.hadoop.hive.ql.hooks.LineageLogger<\/value> \n<description>for Unravel, from unraveldata.com<\/description>\n<\/property> The HiveServer2 Advanced Configuration Snippet for hive-site.xml should have all 3 classes: 2 from Navigator and 1 from Unravel. Save the changes with optional comment Unravel snippet in hive-site.xml . Deploy the Hive client configuration by clicking the deploy glyph ( ) or by using the Actions pull-down menu. Restart the Hive service. Cloudera Manager recommends a restart, which is not necessary for activating these changes. Don't restart now; you can restart later. Check Unravel UI to see if all Hive queries are running. If queries are running fine and appearing in Unravel UI, you are done. If queries are failing with a class not found error or permission problems: Undo the hive-site.xml changes in Cloudera Manager. Deploy the hive client configuration. Restart the Hive service. Follow the steps in Troubleshooting . 5. Deploy the Spark JAR In Cloudera Manager, select the target cluster, then select the Spark service. Select Configuration . Search for spark-defaults . In Spark Client Advanced Configuration Snippet (Safety Valve) for spark-conf\/spark-defaults.conf , enter the following text, replacing placeholders with your particular values: spark.unravel.server.hostport= unravel-host :4043 \nspark.driver.extraJavaOptions=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=config=driver,libs= spark-version \nspark.executor.extraJavaOptions=-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=config=executor,libs= spark-version \nspark.eventLog.enabled=true On a multi-host Unravel Server deployment, use host2's FQDN or logical hostname for unravel-host . For spark-version , use a Spark version that is compatible with this version of Unravel . For example, compmatrix-platform spark-2.0 for Spark 2.0.x spark-2.1 for Spark 2.1.x spark-2.2 for Spark 2.2.x spark-2.3 for Spark 2.3.x Save changes. Deploy the client configuration by clicking the deploy glyph ( ) or by using the Actions pull-down menu. Your spark-shell will ensure new JVM containers are created with the necessary extraJavaOptions for the Spark drivers and executors. Enable Spark streaming. The Spark streaming probe is disabled by default and you must enable it manually by editing spark-defaults.conf . Search for spark.driver.extraJavaOptions and set it to the following. Be sure to substitute the correct version of Spark for spark-version . Unravel supports the Spark streaming feature for Spark 1.6.x, 2.0.x, 2.1.x, and 2.2.x only. Support for Spark apps using the Structured Streaming API introduced in Spark 2 is limited. javaagent: unravel-sensor-path \/btrace-agent.jar=script=DriverProbe.class:SQLProbe.class:StreamingProbe.class,libs=spark- spark-version . Check Unravel UI to see if all Spark jobs are running. If jobs are running fine and appearing in Unravel UI, you're done. If queries are failing with a class not found error or permission problems: Undo the spark-defaults.conf changes in Cloudera Manager. Deploy the client configuration. Investigate and fix the issue. Follow the steps in Troubleshooting . If you have YARN-client mode applications, the default Spark configuration is not sufficient, because the driver JVM starts before the configuration set through the SparkConf is applied. For more information, see Apache Spark Configuration . In this case, configure the Unravel Sensor for Spark to profile specific Spark applications only (in other words, per-application profiling rather than cluster-wide profiling ). 6. Configure YARN-MapReduce JVM sensor cluster-wide In Cloudera Manager, go to YARN service . Select the Configuration tab. Search for Application Master Java Opts Base and concatenate the following xml block properties snippet (ensure to start with a space and add below). Make sure that \"-\" is a minus sign. You need to modify the value of unravel-host with your Unravel Server IP address or a fully qualified DNS. For multi-host Unravel installation, use the IP address of Host2. -javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= unravel-host :4043 Search for MapReduce Client Advanced Configuration Snippet (Safety Valve) for mapred-site.xml in the middle of the page. Enter following xml four block properties snippet to Gateway Default Group . (Click View as XML .) <property>\n<name>mapreduce.task.profile<\/name>\n<value>true<\/value>\n<\/property> \n<property>\n<name>mapreduce.task.profile.maps<\/name>\n<value>0-5<\/value>\n<\/property> \n<property>\n<name>mapreduce.task.profile.reduces<\/name>\n<value>0-5<\/value>\n<\/property> \n\/\/ this is one line \n<property>\n<name>mapreduce.task.profile.params<\/name>\n<value>-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= unravel-host :4043<\/value><\/property> Save the changes. Deploy the client configuration by clicking the deploy glyph ( ) or by using the Actions pull-down menu. Cloudera Manager will specify a restart which is not necessary to effect these changes. (Click Restart Stale Services if that is visible. However, you can also perform this later when you have a planned maintenance.) The restart is important for MR sensor to be picked up by queries submitted via Hiveserver2. Use the Unravel UI to monitor the situation. When you view the MapReduce APM page for any completed MRjob you should see mappers and reducers in the Resource Usage tab. 7. Retrieve Impala data from Cloudera Manager Configure Unravel Server to retrieve Impala query data from Cloudera Manager as follows: Add com.unraveldata.data.source=cm in \/usr\/local\/unravel\/etc\/unravel.properties on Unravel Server. Tell Unravel Server some information about your Cloudera Manager's URL, port number, login credentials, and so on. You do this by adding the following properties to \/usr\/local\/unravel\/etc\/unravel.properties on Unravel Server: Property\/Description Set by user Unit Default com.unraveldata.cloudera.manager.url URL of Cluster Manager, for example, http:\/\/$clouderaserver In order to properly track Impala jobs, make sure that the value of this property does not contain a port number since there is already a separate config for the port. Required string - com.unraveldata.cloudera.manager.username Cloudera manager username. Required string - com.unraveldata.cloudera.manager.password Cloudera manager password. Required string - com.unraveldata.cloudera.manager.port You only need to specify this if your Cloudera Manager is not on port 7180. Required integer - For example, com.unraveldata.data.source=cm \ncom.unraveldata.cloudera.manager.url=http:\/\/ my-cm-url \ncom.unraveldata.cloudera.manager.port=9997 \ncom.unraveldata.cloudera.manager.username= mycmname \ncom.unraveldata.cloudera.manager.password= mycmpassword Make sure that the Cloudera Manager user in com.unraveldata.cloudera.manager.username has read access to Cloudera Manager REST APIs. You can verify this by running a curl command such as the following, substituting your local values for the variables: curl --user clouderamanager-username : clouderamanager-password 'http:\/\/ clouderamanager-url : clouderamanager-port \/api\/v13\/clusters'\ncurl --user clouderamanager-username : clouderamanager-password 'http:\/\/ clouderamanager-url : clouderamanager-port \/api\/v13\/clusters\/ cluster-name \/services' By default, the Impala sensor task is enabled. To disable it, specify the following option in \/usr\/local\/unravel\/etc\/unravel.properties on Unravel Server: com.unraveldata.sensor.tasks.disabled=iw (Optional) Change the Impala lookback window. By default, when Unravel Server starts, it retrieves the last 5 minutes of Impala queries. To change this, do the following: On Unravel Server, change com.unraveldata.cloudera.manager.impala.look.back.minutes in \/usr\/local\/unravel\/etc\/unravel.properties . For example, to set the lookback to 7 minutes: com.unraveldata.cloudera.manager.impala.look.back.minutes=-7 Include a minus sign in front of the new value. Restart the unravel_us daemon. References For more information on creating permanent functions, see Cloudera documentation . " }, 
{ "title" : "For Upgrades, start here", 
"url" : "102054-cdh-part2.html#UUID-3d356f86-b5dd-c2f8-7d0f-a019268c4385_section-5ce756dfdeb6b-idm45444631468640", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Cloudera Distribution of Apache Hadoop (CDH) \/ Part 2: Enabling additional instrumentation \/ For Upgrades, start here", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "3. For Oozie, copy the Hive Hook and BTrace JARs to the HDFS shared library path", 
"url" : "102054-cdh-part2.html#UUID-3d356f86-b5dd-c2f8-7d0f-a019268c4385_step3", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Cloudera Distribution of Apache Hadoop (CDH) \/ Part 2: Enabling additional instrumentation \/ 3. For Oozie, copy the Hive Hook and BTrace JARs to the HDFS shared library path", 
"snippet" : "Copy the Hive Hook JAR, \/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar and the Btrace JAR, \/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar to the shared lib path specified by oozie.libpath . If you don't do this, jobs controlled by Oozie 2.3+ will fail....", 
"body" : "Copy the Hive Hook JAR, \/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/unravel_hive_hook.jar and the Btrace JAR, \/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar to the shared lib path specified by oozie.libpath . If you don't do this, jobs controlled by Oozie 2.3+ will fail. " }, 
{ "title" : "8. Add more configuration and instrumentation options", 
"url" : "102054-cdh-part2.html#UUID-3d356f86-b5dd-c2f8-7d0f-a019268c4385_section-5cb9febe9c32d-idm45527665175312", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Cloudera Distribution of Apache Hadoop (CDH) \/ Part 2: Enabling additional instrumentation \/ 8. Add more configuration and instrumentation options", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Part 3: Next steps", 
"url" : "102055-cdh-part3.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Cloudera Distribution of Apache Hadoop (CDH) \/ Part 3: Next steps", 
"snippet" : "Add more configuration and instrumentation options ....", 
"body" : "Add more configuration and instrumentation options . " }, 
{ "title" : "Creating an alternate principal", 
"url" : "102056-cdh-alt-principal.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Cloudera Distribution of Apache Hadoop (CDH) \/ Creating an alternate principal", 
"snippet" : "For quick initial installation, you can use the hdfs principal and its keytab, but for production use, you might want to create an alternate principal that has restricted access to specific areas, and use its corresponding keytab. This topic explains how to do this. You can name the alternate princi...", 
"body" : "For quick initial installation, you can use the hdfs principal and its keytab, but for production use, you might want to create an alternate principal that has restricted access to specific areas, and use its corresponding keytab. This topic explains how to do this. You can name the alternate principal whatever you prefer; these steps name it unravel . Its name doesn't need to be the same as the local username. The steps apply only to CDH and have been tested using Cloudera Manager with the recommended Sentry configuration. Check the HDFS default umask. For access via ACL, the group part of the HDFS default umask needs to have read and execute access. This allows Unravel to see subdirectories and read files. The default umask setting on HDFS for both CDH and HDP is 022 . The middle digit controls the group mask and ACLs are masked using this default group mode. You can check the HDFS umask setting from either Cloudera Manager or in hdfs-site.xml : In Cloudera Manager, check the value of dfs.umaskmode and make sure the middle digit is 2 or 0 . In hdfs-site.xml file search for fs.permissions.umask-mode and make sure the middle digit is 2 or 0 . Enable ACL inheritance. In Cloudera Manager's HDFS configuration, search for namenode advanced configuration snippet , and set its dfs.namenode.posix.acl.inheritance.enabled property to true in hdfs-site.xml . This is a workaround for an issue where HDFS was not compliant with the Posix standard for ACL inheritance. For details, see  Apache JIRA HDFS-6962 . Cloudera backported the fix for this issue into CDH5.8.4, CDH5.9.1, and later, setting dfs.namenode.posix.acl.inheritance.enabled to false in Hadoop 2.x and true in Hadoop 3.x. Restart the cluster to effect the change of dfs.namenode.posix.acl.inheritance.enabled to true . Change the ACLs of the target HDFS directories. Run the following commands as global hdfs to change the ACLs of the following HDFS directories. Run these in the order presented. Set the ACL for future directories. Be sure to set the permissions at the \/user\/history level. Files are first written to an intermediate_done folder under \/user\/history and then moved to \/user\/history\/done . hadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/spark\/applicationHistory\nhadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/history\nhadoop fs -setfacl -R -m default:user:unravel:r-x \/tmp\/logs\nhadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/hive\/warehouse If you have Spark2 installed, set the ACL of the Spark2 application history folder: hadoop fs -setfacl -R -m default:user:unravel:r-x \/user\/spark\/spark2ApplicationHistory Set ACL for existing directories. hadoop fs -setfacl -R -m user:unravel:r-x \/user\/spark\/applicationHistory\nhadoop fs -setfacl -R -m user:unravel:r-x \/user\/history\nhadoop fs -setfacl -R -m user:unravel:r-x \/tmp\/logs\nhadoop fs -setfacl -R -m user:unravel:r-x \/user\/hive\/warehouse If you have Spark2 installed, set the ACL of the Spark2 application history folder: hadoop fs -setfacl -R -m user:unravel:r-x \/user\/spark\/spark2ApplicationHistory Verify the ACL of the target HDFS directories. hdfs dfs -getfacl \/user\/spark\/applicationHistory\nhdfs dfs -getfacl \/user\/spark\/spark2ApplicationHistory\nhdfs dfs -getfacl \/user\/history\nhdfs dfs -getfacl \/tmp\/logs\nhdfs dfs -getfacl \/user\/hive\/warehouse On the Unravel Server, verify HDFS permission on folders as the target user ( unravel , hdfs , mapr , or custom) with a valid kerberos ticket corresponding to the keytab principal. sudo -u unravel kdestroy\nsudo -u unravel kinit -kt keytab-file principal \nsudo -u unravel hadoop fs -ls \/user\/history\nsudo -u unravel hadoop fs -ls \/tmp\/logs\nsudo -u unravel hadoop fs -ls \/user\/hive\/warehouse\n Find and verify the keytab: klist -kt keytab-file If you're using KMS and HDFS encryption and the hdfs principal, you might need to adjust kms-acls.xml permissions in Cloudera Manager for DECRYPT_EEK if access is denied. In particular, the \"done\" directory might not allow decryption of logs by the hdfs principal. If you're using \"JNI\" based groups for HDFS (a setting in Cloudera Manager), you need to add this line to \/usr\/local\/unravel\/etc\/unravel.ext.sh : export LD_LIBRARY_PATH=\/opt\/cloudera\/parcels\/CDH\/lib\/hadoop\/lib\/native If Kerberos is enabled, specify the new values for keytab-file and principal in \/usr\/local\/unravel\/etc\/unravel.properties and unravel.ext.sh : Whenever you change Kerberos tokens or principal update these properties to ensure the latest Kerberos keytab file for Unravel is available on Unravel server and the restart all services, sudo \/etc\/init.d\/unravel_all.sh start . Property\/Description Set by user Unit Default com.unraveldata.kerberos.principal Name of the Kerberos principal for Unravel daemons to use, along with its host name, domain name, and realm. Example: unravel\/myhost.mydomain@MYREALM string - com.unraveldata.kerberos.keytab.path Path to keytab file, on Unravel Server, corresponding to the Kerberos principal for Unravel daemons to use. Example: \/usr\/local\/unravel\/etc\/unravel.keytab You can verify the principal in a keytab by using klist -kt KETYAB_FILE . The keytab file should have chmod bits 500 and be owned by unravel local user (default) or by the user you want to use, as explained in  Run Unravel Daemons with Custom User . string " }, 
{ "title" : "Adding a new node in an existing CDH cluster", 
"url" : "102057-install-cdh-new-node.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Cloudera Distribution of Apache Hadoop (CDH) \/ Adding a new node in an existing CDH cluster", 
"snippet" : "You do not need to do anything when you add a new node to a CDH cluster. If you have changed your Kerberos tokens or principal you must perform the following steps: Update the following properties to ensure the latest Kerberos keytab file for Unravel is available on Unravel servers. com.unraveldata....", 
"body" : "You do not need to do anything when you add a new node to a CDH cluster. If you have changed your Kerberos tokens or principal you must perform the following steps: Update the following properties to ensure the latest Kerberos keytab file for Unravel is available on Unravel servers. com.unraveldata.kerberos.principal= new principal \ncom.unraveldata.kerberos.keytab.path= new path Make sure the new file's ownership\/permission is restored to the original setup. Restart all services. sudo \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "Troubleshooting", 
"url" : "102058-cdh-troubleshoot.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Cloudera Distribution of Apache Hadoop (CDH) \/ Troubleshooting", 
"snippet" : "Symptom Problem Remedy hadoop fs -ls \/user\/unravel\/HOOK_RESULT_DIR\/ indicates that the directory does not exist Unravel Server RPM is not yet installed, or Unravel Server RPM is installed on a different HDFS cluster, or HDFS home directory for Unravel does not exist, or kerberos\/sentry actions are n...", 
"body" : "Symptom Problem Remedy hadoop fs -ls \/user\/unravel\/HOOK_RESULT_DIR\/ indicates that the directory does not exist Unravel Server RPM is not yet installed, or Unravel Server RPM is installed on a different HDFS cluster, or HDFS home directory for Unravel does not exist, or kerberos\/sentry actions are needed Install Unravel RPM on Unravel host. or Verify that user unravel user exists and has a \/user\/unravel\/ directory in HDFS with write access to it. ClassNotFound error for com.unraveldata.dataflow.hive.hook.UnravelHiveHook during Hive query execution Unravel hive hook JAR was not found in in $HIVE_HOME\/lib\/ . Confirm that the UNRAVEL_SENSOR parcel was distributed and activated in Cloudera Manager. or Put the Unravel hive-hook JAR corresponding to hive-version in jar-destination on each gateway as follows: cd \/usr\/local\/unravel\/hive-hook\/;\ncp unravel-hive- hive-version *hook.jar jar-destination " }, 
{ "title" : "Hortonworks Data Platform (HDP)", 
"url" : "102059-install-hdp.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Hortonworks Data Platform (HDP)", 
"snippet" : "This topic explains how to deploy Unravel on Hortonworks Data Platform (HDP). This section shows you how to: Do a pre-installation check. Perform the MySQL pre-installation steps. Install the Unravel RPM. Perform the MySQL post-installation steps. Configure Unravel Server. Distribute Unravel's senso...", 
"body" : "This topic explains how to deploy Unravel on Hortonworks Data Platform (HDP). This section shows you how to: Do a pre-installation check. Perform the MySQL pre-installation steps. Install the Unravel RPM. Perform the MySQL post-installation steps. Configure Unravel Server. Distribute Unravel's sensor JARs to desired nodes in the cluster. Adjust your cluster's configuration to integrate with Unravel. Restart needed components on your cluster. Configure additional integrations like Kafka and LDAP. " }, 
{ "title" : "Prerequisites", 
"url" : "102060-hdp-pre.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Hortonworks Data Platform (HDP) \/ Prerequisites", 
"snippet" : "To deploy Unravel, first ensure that your environment meets these requirements. You must use an independent host for the Unravel server. This host must: Be managed by Ambari. Have Hadoop clients pre-installed. Have no other Hadoop service or third-party applications installed. Accessible to only Had...", 
"body" : "To deploy Unravel, first ensure that your environment meets these requirements. You must use an independent host for the Unravel server. This host must: Be managed by Ambari. Have Hadoop clients pre-installed. Have no other Hadoop service or third-party applications installed. Accessible to only Hadoop and Unravel Admins. Platform Each version of Unravel has specific platform requirements. Confirm that your cluster meets the requirements for the version of Unravel that you're installing . Your cluster must be deployed using Ambari. compmatrix-platform Sizing Use Ambari Web UI to allocate a new node in your cluster. This node will be the host machine for Unravel. Provision it as described below. You must have separate nodes for the Unravel server and the external MySQL database . Unravel Server Architecture: x86_64 vm.max_map_count is set to 262144 Minimum requirements for cores, RAM, and disks: The table below lists the minimum requirements for cores, RAM, and disks for a typical environment with default data retention and lookback settings . \/usr\/local\/unravel is the storage location for Unravel binaries. \/srv\/unravel is used for Elasticsearch (ES) and the bundled database. In production environments, put \/usr\/local\/unravel and \/srv\/unravel on separate disks. Putting \/srv\/unravel on a separate high spin HDD with its own SATAIII (or equivalent) bus significantly increases IO bandwidth. If \/usr\/local\/unravel or \/srv\/unravel doesn't have the minimum free space shown in the table below, create symbolic links for them to another disk. To check the space on a volume use the df command. For example, df -h \/srv Jobs per day Cores RAM \/usr\/local\/unravel \/srv\/unravel Less than 50,000 8 96 GB 8 GB free 500 GB free 50,000 to 100,000 to 8 128GB 8 GB free 500 GB free Over 100,000 Contact Unravel Support All volumes are mounted. \/tmp is mounted with executable permissions. To re-mount \/tmp with executable permissions use the following command: mount -o remount,exec \/tmp MySQL Server Minimum requirements for cores, RAM, and disk. Jobs per day Data retention length Cores RAM Disk Less than 50,000 30 days 4 32 GB 1 TB 60 days 4 32 GB 2 TB 50,000 to 100,000 to 30 days 8 64 GB 2 TB 60 days 8 64 GB 4 TB Over 100,000 Contact Unravel Support Software On the Unravel host, confirm the following configuration: All default clients are running: If the Unravel host is running Red Hat Enterprise Linux (RHEL) 6.x, set its bootstrap.system_call_filter to false in elasticsearch.yml : boostrap.system_call_filter: false libaio.x86_64 is installed. If you're installing Unravel version 4.5.0.0, set SELINUX to permissive or disabled in \/etc\/sysconfig\/selinux . If you're installing Unravel version 4.5.0.1+, SELINUX can be set to enabled . PATH includes the path to the HDFS+Hive+YARN+Spark client\/gateway, Hadoop commands, and Hive commands. Zookeeper is not installed. Permissions This is a list of permissions you'll need to grant to the Unravel username after you install the Unravel Server RPM. We'll walk you through setting these up later. The only permission you need to set up right now on the Unravel host is root access or \"sudo root\" permission in order to install the Unravel Server RPM. If you're using Kerberos, you'll need to create a principal and keytab for Unravel daemons to use to access certain HDFS resources. Unravel needs access to the YARN resource manager's REST API (so that the principal can determine which resource manager is active). Unravel needs access to the JDBC access to the Hive metastore. Read-only access is sufficient. If you plan to use Unravel's move or kill auto actions, the Unravel username needs to be added to YARN's yarn.admin.acl property. Unravel needs read-only access to the Application Timeline Server (ATS). Network On the new node, open the following ports: Port(s) Direction Description 3000 Both Traffic to and from Unravel UI 3316 Both Database traffic 4020 Both Unravel APIs 4021 Both Host monitoring of JMX on localhost 4031 Both Database traffic 4043 In UDP and TCP ingest traffic from the entire cluster to Unravel Server(s) 4044-4049 In UDP and TCP ingest spares for unravel_lr* 4091-4099 Both Kafka brokers 4171-4174, 4176-4179 Both ElasticSearch; localhost communication between Unravel daemons or Unravel Servers in a multi-host deployment 4181-4189 Both Zookeeper daemons 4210 Both Cluster access service HDFS ports Both Traffic to\/from the cluster to Unravel Server(s) Hive metadata database port Out For YARN only. Traffic from Hive to Unravel Server(s) for partition reporting 8088 Out Traffic from Unravel Server(s) to the Resource Manager API 8188 Out Traffic from Unravel Server(s) to the ATS server(s) 11000 Out For Oozie only. Traffic from Unravel Server(s) to the Oozie server " }, 
{ "title" : "Part 1: Installing Unravel Server", 
"url" : "102061-install-hdp-part1.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Hortonworks Data Platform (HDP) \/ Part 1: Installing Unravel Server", 
"snippet" : "If you have not already done so, confirm your new node meets Unravel's hosting requirements . 3. Install Unravel Server on the new node Download the Unravel Server RPM. downloads Install the Unravel Server RPM. sudo rpm -Uvh unravel- version .rpm The installation creates the following items: \/usr\/lo...", 
"body" : "If you have not already done so, confirm your new node meets Unravel's hosting requirements . 3. Install Unravel Server on the new node Download the Unravel Server RPM. downloads Install the Unravel Server RPM. sudo rpm -Uvh unravel- version .rpm The installation creates the following items: \/usr\/local\/unravel\/ , which contains executables, scripts, properties file ( unravel.properties ), and logs. \/etc\/init.d\/unravel_* , which contains scripts for controlling services, such as unravel_all.sh for manually stopping, starting, and getting the status of all daemons in proper order. User unravel if it doesn't exist already. An HDFS directory for Hive Hook instrumentation, if your cluster is non-secure. If your cluster is secured you'll create this directory later. If the cluster is kerberized, check Kerberos settings with the kinit and klist commands: klist -kt \/etc\/security\/keytabs\/unravel.keytab\nKeytab name: FILE:\/etc\/security\/keytabs\/unravel.keytab\nKVNO Timestamp Principal\n---- ------------------- ------------------------------------------------------\n 3 03\/08\/2019 15:20:13 unravel\/congo52.unraveldata.com@lab.localdomain\n kinit -kt \/etc\/security\/keytabs\/unravel.keytab unravel\/congo52.unraveldata.com@lab.localdomain\n klist\nTicket cache: FILE:\/tmp\/krb5cc_0\nDefault principal: unravel\/congo52.unraveldata.com@lab.localdomain \nValid starting Expires Service principal\n03\/14\/2019 16:02:49 03\/15\/2019 16:02:49 krbtgt\/lab.localdomain@lab.localdomain \n# groups unravelunravel : unravel hadoop \n# kvno unravel\/congo52.unraveldata.com@lab.localdomainunravel\/congo52.unraveldata.com@lab.localdomain: kvno = 3 Check the unravel user's network access. If the cluster is kerberized, make sure Unravel can access http:\/\/timeline-host:8188\/ws\/v1\/timeline .  The curl command below requires a successful kinit command for the unravel user.  The -u option is a fake user and is ignored when relying on GSS-API. curl --negotiate -v -u :-X GET http:\/\/timeline-host:8188\/ws\/v1\/timeline\n...\n> Authorization: Negotiate token \n> User-Agent: curl\/7.29.0\n> Host: congo52.unraveldata.com:8188\n> Accept: *\/*\n>\n< HTTP\/1.1 200 OK\n...\n 6. Configure Unravel Server with basic options On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties as follows. Set general properties. Property\/Description Set by user Unit Default com.unraveldata.customer.organization Customer name. Used to identify your installation for reporting and notification purposes in Unravel UI. Optional string Not Set com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. Example: http:\/\/unravelserver.company.com:3000   string http:\/\/{host}:3000 com.com.unraveldata.hdfs.timezone Timezone of HDFS, for example, US\/Eastern, Etc\/GMT-4, America\/New_York. If the timezone is not set then an error message is logged and UTC timezone is used. Possible timezones can be obtained by calling TimeZone.getAvailableIDs() . string - com.unraveldata.tmpdir The base location for Unravel process control files where Unravel's temp files reside. string (path) \/srv\/unravel\/tmp com.unraveldata.history.maxSize.weeks Number of weeks retained for search results in Elastic Search. integer 5 com.unraveldata.retention.max.days Number of days to keep the heaviest data (such as error logs and drill-down details) in the SQL Database. integer 30 Specify the location of HDFS logs . If you're using Spark1 and Spark2 you must set com.unraveldata.spark.eventlog.location to two directories using a comma-separated list. com.unraveldata.job.collector.done.log.base=\/mr-history\/done\ncom.unraveldata.job.collector.log.aggregation.base=\/app-logs\/*\/logs*\ncom.unraveldata.spark.eventlog.location=hdfs:\/\/\/spark1-history\/,hdfs:\/\/\/spark2-history\/. If Kerberos is enabled, do the following: Create or identify a principal and keytab for Unravel to use to access the HDFS resources listed in the table below and the REST API. Resource Principal Access Description hdfs:\/\/\/user\/spark\/applicationHistory\/,hdfs:\/\/\/spark2-history\/,hdfs:\/\/\/spark-history\/ Your restricted principal for Unravel read+execute Spark and Spark2 event logs hdfs:\/\/mr-history\/done Your restricted principal for Unravel read+execute MapReduce logs hdfs:\/\/\/app-logs\/*\/logs* Your restricted principal for Unravel read+execute YARN log aggregation directory hdfs:\/\/apps\/hive\/warehouse (Default value of hive.metastore.warehouse.dir ) Your restricted principal for Unravel read+execute File and partition sizes in the Hive warehouse Hive metastore database GRANT hive read+execute Hive table information Set these properties: Property\/Description Set by user Unit Default com.unraveldata.kerberos.principal Name of the Kerberos principal for Unravel daemons to use, along with its host name, domain name, and realm. Example: unravel\/myhost.mydomain@MYREALM string - com.unraveldata.kerberos.keytab.path Path to keytab file, on Unravel Server, corresponding to the Kerberos principal for Unravel daemons to use. Example: \/usr\/local\/unravel\/etc\/unravel.keytab You can verify the principal in a keytab by using klist -kt KETYAB_FILE . The keytab file should have chmod bits 500 and be owned by unravel local user (default) or by the user you want to use, as explained in  Run Unravel Daemons with Custom User . string If Ranger is enabled, do the following: Check\/set Ranger permissions for the runtime user. Create or identify a principal and keytab for Unravel daemons to access the HDFS resources listed in the table below. Resource Principal Access Description hdfs:\/\/\/user\/spark\/applicationHistory\/,hdfs:\/\/\/spark2-history\/,hdfs:\/\/\/spark-history\/ Your restricted principal for Unravel read+execute Spark and Spark2 event logs hdfs:\/\/mr-history\/done Your restricted principal for Unravel read+execute MapReduce logs hdfs:\/\/\/app-logs\/*\/logs* Your restricted principal for Unravel read+execute YARN log aggregation directory hdfs:\/\/apps\/hive\/warehouse (Default value of hive.metastore.warehouse.dir ) Your restricted principal for Unravel read+execute File and partition sizes in the Hive warehouse Hive metastore database GRANT hive read+execute Hive table information Disable Unravel's Impala sensor. HDP doesn't officially support Impala, so set com.unraveldata.sensor.tasks.disabled to iw . com.unraveldata.sensor.tasks.disabled=iw In the Ambari dashboard, set YARN ACLs. Select YARN | Configs | Advanced . Set yarn.acl.enable to true . Add the Unravel user specified in com.unraveldata.kerberos.principal to yarn.admin.acl . Save your changes. 7. Modify Unravel Server for HDP On Unravel Server, stop all services and run the switch_to_hdp.sh script. sudo \/etc\/init.d\/unravel_all.sh stop\nsudo \/usr\/local\/unravel\/install_bin\/switch_to_hdp.sh The changes made by switch_to_hdp.sh are persistent through subsequent RPM upgrades; you don't need to re-run this when you upgrade the RPM. Open \/usr\/local\/unravel\/etc\/unravel.properties to see if you need to modify any properties which were added or modified by the script switch_to_hdp.sh : Log properties . If you're using Spark1 and Spark2 you must set com.unraveldata.spark.eventlog.location to two directories using a comma-separated list. com.unraveldata.job.collector.done.log.base=\/mr-history\/done\ncom.unraveldata.job.collector.log.aggregation.base=\/app-logs\/*\/logs*\ncom.unraveldata.spark.eventlog.location=hdfs:\/\/\/spark1-history\/,hdfs:\/\/\/spark2-history\/ Hive metastore properties . javax.jdo.option.ConnectionURL=jdbc:mysql:\/\/ unravel-host :3306\/ database-name \njavax.jdo.option.ConnectionDriverName=com.mysql.jdbc.Driver\njavax.jdo.option.ConnectionPassword= hive-metastore-password \njavax.jdo.option.ConnectionURL= JDBC connection string Log into Ambari Web UI and set the following properties to match the values in \/usr\/local\/unravel\/etc\/unravel.properties : Cluster Property Unravel Property mapreduce.jobhistory.done-dir com.unraveldata.job.collector.done.log.base yarn.nodemanager.remote-app-log-dir com.unraveldata.job.collector.done.log.aggregation.base jdbc:mysql:\/\/ database-host :3306\/ database-url javax.jdo.option.ConnectionURL JDBC Driver Class javax.jdo.option.ConnectionDriverName Database Username javax.jdo.option.ConnectionUserName 9. Start Unravel services Run the following command to start all Unravel services: sudo \/etc\/init.d\/unravel_all.sh start\n This completes the basic\/core configuration. Sample working configuration The sample contents of \/usr\/local\/unravel\/etc\/unravel.properties shown below include the Hive metastore connection details. Note that in this sample, the system user hive is being configured, which is not a requirement; you can use any user that has read access to Hive metastore database. # Ensure that you keep DB and Unravel specific information that come with installation. \n# Ensure that you do not overwrite properties by copying and pasting properties below  \ncom.unraveldata.advertised.url=http:\/\/ unravel-host :3000\ncom.unraveldata.history.maxSize.weeks=3\ncom.unraveldata.job.collector.done.log.base=\/mr-history\/done\ncom.unraveldata.job.collector.log.aggregation.base=\/app-logs\/*\/logs*\ncom.unraveldata.login.admins=admin\ncom.unraveldata.s3.batch.monitoring.interval.sec=300\ncom.unraveldata.spark.eventlog.location=hdfs:\/\/\/spark2-history\/,hdfs:\/\/\/spark-history\nyarn.resourcemanager.webapp.address=http:\/\/ rm-host :8088\noozie.server.url=http:\/\/ oozie-host :11000\/oozie\ncom.unraveldata.sensor.tasks.disabled=iw\n\njavax.jdo.option.ConnectionURL=jdbc:mysql:\/\/ hive-metastore-db-host \/hive\njavax.jdo.option.ConnectionDriverName=com.mysql.jdbc.Driver\njavax.jdo.option.ConnectionUserName=hive\njavax.jdo.option.ConnectionPassword=hadoop\n\ncom.unraveldata.yarn.timeline-service.webapp.address=http:\/\/ time-line-host \ncom.unraveldata.yarn.timeline-service.port=8188\ncom.unraveldata.job.collector.done.log.aggregation.base=\/app-logs \n\n# Kerberos\ncom.unraveldata.kerberos.principal=unravel\/congo52.unraveldata.com@lab.localdomain\ncom.unraveldata.kerberos.keytab.path=\/etc\/security\/keytabs\/unravel.keytab \n\n# Kafka\ncom.unraveldata.ext.kafka.clusters=c1\ncom.unraveldata.ext.kafka.c1.bootstrap_servers=congo52.unraveldata.com:6667\ncom.unraveldata.ext.kafka.c1.jmx_servers=kafka-test1\ncom.unraveldata.ext.kafka.c1.jmx.kafka-test1.host=congo52.unraveldata.com\ncom.unraveldata.ext.kafka.c1.jmx.kafka-test1.port=9393\ncom.unraveldata.ext.kafka.c1.consumer.config=\/usr\/local\/unravel\/etc\/consumerConfig.properties \n\n# LDAP\ncom.unraveldata.login.mode=ldap\ncom.unraveldata.ldap.url=ldap:\/\/ariel.unraveldata.com\ncom.unraveldata.ldap.Domain=unraveldata.com\ncom.unraveldata.ldap.baseDN=DC=unraveldata,DC=com \n\n# LDAP groups\ncom.unraveldata.ldap.groupFilter=seth-test-group,seth-test-admingroup\ncom.unraveldata.login.admins.ldap.groups=seth-test-admingroup\ncom.unraveldata.ldap.use_jndi=truecom.unraveldata.ldap.verbose=true 10. Log into Unravel UI Using a supported web browser , navigate to http:\/\/ unravel-host :3000 and log in with username admin with password unraveldata . compmatrix-platform Unravel UI displays collected data . 11. Enable additional instrumentation " }, 
{ "title" : "1. Create a new node on your cluster", 
"url" : "102061-install-hdp-part1.html#UUID-62844051-afda-a820-6efd-63ea6523656c_section-5d70348ca52e8-idm45107324684992", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Hortonworks Data Platform (HDP) \/ Part 1: Installing Unravel Server \/ 1. Create a new node on your cluster", 
"snippet" : "Using Ambari, add a new node (host) to your cluster. This new node will be Unravel Server's host machine. Verify that Ambari installed the following clients on the new host: Atlas Metadata HCat HDFS Hive Kerberos MapReduce2 Oozie Pig Slider Spark Spark2 Tez YARN ZooKeeper...", 
"body" : "Using Ambari, add a new node (host) to your cluster. This new node will be Unravel Server's host machine. Verify that Ambari installed the following clients on the new host: Atlas Metadata HCat HDFS Hive Kerberos MapReduce2 Oozie Pig Slider Spark Spark2 Tez YARN ZooKeeper " }, 
{ "title" : "2. Install MySQL", 
"url" : "102061-install-hdp-part1.html#UUID-62844051-afda-a820-6efd-63ea6523656c_section-5cd09b081436d-idm45444639878864", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Hortonworks Data Platform (HDP) \/ Part 1: Installing Unravel Server \/ 2. Install MySQL", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "4. Configure MySQL", 
"url" : "102061-install-hdp-part1.html#UUID-62844051-afda-a820-6efd-63ea6523656c_section-5cd09bd696d67-idm45444640181408", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Hortonworks Data Platform (HDP) \/ Part 1: Installing Unravel Server \/ 4. Configure MySQL", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "5. (Optional) Enable additional daemons for high-volume workloads", 
"url" : "102061-install-hdp-part1.html#UUID-62844051-afda-a820-6efd-63ea6523656c_section-5d69c38569388-idm46495049322304", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Hortonworks Data Platform (HDP) \/ Part 1: Installing Unravel Server \/ 5. (Optional) Enable additional daemons for high-volume workloads", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "8. Change the run-as user and group for Unravel daemons", 
"url" : "102061-install-hdp-part1.html#UUID-62844051-afda-a820-6efd-63ea6523656c_section-5d154c51b117f-idm45764268358704", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Hortonworks Data Platform (HDP) \/ Part 1: Installing Unravel Server \/ 8. Change the run-as user and group for Unravel daemons", 
"snippet" : "Unravel daemons run under the local user unravel by default. However, if you have Kerberos or Ranger\/Sentry enabled, or a non-Kerberos cluster with simple Unix security, or a different username for the Unravel user, or a non-local user such as an LDAP user, run switch_to_user.sh script to change the...", 
"body" : "Unravel daemons run under the local user unravel by default. However, if you have Kerberos or Ranger\/Sentry enabled, or a non-Kerberos cluster with simple Unix security, or a different username for the Unravel user, or a non-local user such as an LDAP user, run switch_to_user.sh script to change the Unix owner and group of the Unravel daemons. " }, 
{ "title" : "Part 2: Enabling additional instrumentation", 
"url" : "102062-install-hdp-part2.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Hortonworks Data Platform (HDP) \/ Part 2: Enabling additional instrumentation", 
"snippet" : "This topic explains how to configure Unravel to retrieve additional data from Hive, Tez, Spark and Oozie, such as Hive queries, application timelines, Spark jobs, YARN resource management data, and logs. You'll do this by generating Unravel's JARs and distributing them to every node that runs querie...", 
"body" : "This topic explains how to configure Unravel to retrieve additional data from Hive, Tez, Spark and Oozie, such as Hive queries, application timelines, Spark jobs, YARN resource management data, and logs. You'll do this by generating Unravel's JARs and distributing them to every node that runs queries in the cluster. Later, after JARs are distributed to the nodes, you'll integrate Hive, Tez, and Spark data with Unravel. 1. Generate and distribute Unravel's Hive Hook and Spark Sensor JARs On Unravel Server, log in as root and confirm that wget is installed. yum install -y wget Run the following commands: mkdir \/usr\/local\/unravel-jars\nchmod 775 -R \/usr\/local\/unravel-jars\/\nchown root:hadoop \/usr\/local\/unravel-jars\/ \n\nchmod +x \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/unravel_hdp_setup.py\n\ncd \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/\n\nsudo python2 unravel_hdp_setup.py --sensor-only --unravel-server unravel-host :3000 --spark-version spark-version --hive-version hive-version --ambari-server ambari-host \n\ncp \/usr\/local\/unravel_client\/unravel-hive- hive-version -hook.jar \/usr\/local\/unravel-jars\/\n\ncp -pr \/usr\/local\/unravel-agent\/jars\/* \/usr\/local\/unravel-jars\/\n For unravel-host , specify the protocol (http or https) and use the fully qualified domain name (FQDN) or IP address of Unravel Server. For example, https:\/\/playground3.unraveldata.com:3000 . For spark-version , use a Spark version that is compatible with this version of Unravel . For example, compmatrix-platform spark-2.0 for Spark 2.0.x spark-2.1 for Spark 2.1.x spark-2.2 for Spark 2.2.x spark-2.3 for Spark 2.3.x For hive-version , use a Hive version that is compatible with this version of Unravel . For example, compmatrix-platform 1.2.0 for Hive 1.2.0 or 1.2.1 0.13.0 for Hive 0.13.0 Files are installed locally in these two directories: \/usr\/local\/unravel_client (Hive Hook JAR) \/usr\/local\/unravel-agent\/jars\/ (Resource metrics sensor JARs) Make a note of these directories because you'll need to specify them in Ambari. Distribute \/usr\/local\/unravel_client and \/usr\/local\/unravel-agent\/jars\/ to all worker, edge, and master nodes which run queries. For example, scp -r \/usr\/local\/unravel_client\/ root@ hostname :\/usr\/local\/unravel-jars\n\nscp -r \/usr\/local\/unravel-agent\/ root@ hostname :\/usr\/local\/unravel-jars On each instrumented node, do the following: Make sure the node can reach port 4043 of Unravel Server. In Ambari, find the hive-env template , and add this line to the bottom of the file: export AUX_CLASSPATH=${AUX_CLASSPATH}:\/usr\/local\/unravel-jars\/unravel-hive-1.2.0-hook.jar \n For example, In Ambari, find the hadoop-env template , and add this line to the bottom of the file: export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:\/usr\/local\/unravel-jars\/unravel-hive-1.2.0-hook.jar For example, 2. For Oozie, copy the Hive Hook and BTrace JARs to the HDFS shared library path Copy the Hive Hook JAR, \/usr\/local\/unravel_client\/unravel-hive-1.2.0-hook.jar and the Btrace JAR, \/usr\/local\/unravel-agent\/jars\/btrace-agent.jar to the HDFS shared library path specified by oozie.libpath . If you don't do this, jobs controlled by Oozie 2.3+ will fail. If you are launching Spark actions: Copy the jar for the Spark version you are using, for example, spark-2.3. If you copy multiple Spark jars, Oozie won't be to launch actions. Ensure that the Spark event log location is configured the same as the local Spark jobs event logs' directory. In other words, Oozie must be able to locate the event log directory to store its event history logs. 4. Connect Unravel Server to Hive Completion of this step requires a restart of all affected Hive services in Ambari Web UI. In Ambari's general properties, append ,com.unraveldata.dataflow.hive.hook.UnravelHiveHook , to the following properties: Be sure to append a comma with no space followed by the property name. hive.exec.failure.hooks hive.exec.post.hooks hive.exec.pre.hooks For example, com.unraveldata.dataflow.hive.hook.UnravelHiveHook hive.exec.pre.hooks= existing-value ,com.unraveldata.dataflow.hive.hook.UnravelHiveHook\nhive.exec.post.hooks= existing-value ,com.unraveldata.dataflow.hive.hook.UnravelHiveHook\nhive.exec.failure.hooks= existing-value ,com.unraveldata.dataflow.hive.hook.UnravelHiveHook\n In Ambari's custom hive-site editor, do the following: Set hive.exec.driver.run.hooks to com.unraveldata.dataflow.hive.hook.UnravelHiveHook. If hive.exec.driver.run.hooks isn't there already, add it. Set com.unraveldata.host : to unravel-gateway-internal-IP-hostname Set com.unraveldata.hive.hook.tcp to true If you installed an Unravel version older than 4.5.1.0, set com.unraveldata.hive.hdfs.dir to \/user\/unravel\/HOOK_RESULT_DIR . For example, If LLAP is enabled, do the following: Edit hive-site.xml manually, not through Ambari Web UI. Copy the settings in Custom hive-interactive-site and paste them into \/etc\/hive\/conf\/hive-site.xml . Copy the settings in Advanced hive-interactive-env and paste them into \/etc\/hive\/conf\/hive-site.xml . If you have an Unravel version older than 4.5.1.0, create HDFS Hive Hook directories for Unravel: hdfs dfs -mkdir -p \/user\/unravel\/HOOK_RESULT_DIR\nhdfs dfs -chown unravel:hadoop \/user\/unravel\/HOOK_RESULT_DIR\nhdfs dfs -chmod -R 777 \/user\/unravel\/HOOK_RESULT_DIR 6. Connect Unravel Server to Tez If you're not running Tez, skip this step. Add these properties to \/usr\/local\/unravel\/etc\/unravel.properties : Property\/Description Set by user Unit Default com.unraveldata.yarn.timeline-service.webapp.address The http address of the Timeline service web application. Optional string (URL) - com.unraveldata.yarn.timeline-service.port Timeline service port. number 8188 Confirm that hive.execution.engine is set to tez . set hive.execution.engine=tez; Using the Ambari, configure the Btrace agent for Tez: Append the Java options below to tez.am.launch.cmd-opts and tez.task.launch.cmd-opts : -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr,config=tez -Dunravel.server.hostport= unravel-host :4043 For example, Restart the affected component(s). The screenshot below illustrates this change. In a Kerberos environment you need to modify tez.am.view-acls property with the \"run as\" user or *. Confirm that Unravel UI shows Tez data. Run \/usr\/local\/unravel\/install_bin\/hive_test_simple.sh on the HDP cluster or on any cloud environment where hive.execution.engine=tez . Check Unravel UI for Tez data. Unravel UI may take a few seconds to load Tez data. In Ambari, restart all affected Hive services. 7. (Optional) Connect Unravel Server to Spark-on-YARN Completion of this step requires a restart of all affected Spark services in Ambari UI. For unravel-host , use Unravel Server's fully qualified domain name (FQDN) or IP address. For spark-version , use a Spark version that is compatible with this version of Unravel . For example, compmatrix-platform spark-2.0 for Spark 2.0.x spark-2.1 for Spark 2.1.x spark-2.2 for Spark 2.2.x spark-2.3 for Spark 2.3.x Tell Ambari where the Spark JARs are: In Ambari Web UI, on the left side, click Spark | Configs | Custom spark-defaults | Add Property and use Bulk property add mode , or edit spark-defaults.conf as follows: If your cluster only has one Spark 1.X version, spark-defaults.conf is in \/usr\/hdp\/current\/spark-client\/conf . If your cluster is running Spark 2.X, spark-defaults.conf is in \/usr\/hdp\/current\/spark2-client\/conf . The example below uses default locations for Spark JARs. Your environment may vary. spark.unravel.server.hostport= unravel-host :4043\nspark.driver.extraJavaOptions=-javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=config=driver,libs= spark-version \nspark.executor.extraJavaOptions=-javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=config=executor,libs= spark-version \nspark.eventLog.enabled=true Enable Spark streaming. Spark streaming is disabled by default. To enable it, edit spark-defaults.conf as follows. Search for spark.driver.extraJavaOptions and set it to the following value. Be sure to substitute the correct version of Spark for spark-version . spark.driver.extraJavaOptions=-javaagent:${ unravel-sensor-path }\/btrace-agent.jar=script=DriverProbe.class:SQLProbe.class:StreamingProbe.class,libs= spark-version The value of spark.driver.extraJavaOptions changes based on whether Spark streaming is enabled or disabled. Unravel supports the Spark streaming feature for Spark 1.6.x, 2.0.x, 2.1.x and 2.2.x only. Unravel has limited support for Spark apps using the Structured Streaming API introduced in Spark2. " }, 
{ "title" : "3. Tell Ambari where Unravel's JARs are", 
"url" : "102062-install-hdp-part2.html#UUID-3f7f2010-3fa9-6c3f-147a-96568ff08530_section-5d1a8c0462155-idm45839550839360", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Hortonworks Data Platform (HDP) \/ Part 2: Enabling additional instrumentation \/ 3. Tell Ambari where Unravel's JARs are", 
"snippet" : "Adjust Hive and Hadoop configurations in Ambari to allow these JARs to be picked up by Hive Hook and Hadoop. Add AUX_CLASSPATH to Ambari's hive-env template: In Ambari, click Hive | Configs | Advanced | Advanced hive-env . Inside the Advanced hive-env template, towards the end of line, add: export A...", 
"body" : "Adjust Hive and Hadoop configurations in Ambari to allow these JARs to be picked up by Hive Hook and Hadoop. Add AUX_CLASSPATH to Ambari's hive-env template: In Ambari, click Hive | Configs | Advanced | Advanced hive-env . Inside the Advanced hive-env template, towards the end of line, add: export AUX_CLASSPATH=${AUX_CLASSPATH}:\/usr\/local\/unravel-jars\/unravel-hive-1.2.0-hook.jar Add HADOOP_CLASSPATH to Ambari's hadoop-env template: In Ambari, click HDFS | Configs | Advanced | Advanced hadoop-env . Inside the hadoop-env template, look for export HADOOP_CLASSPATH and append Unravel's JAR path: export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:\/usr\/local\/unravel-jars\/unravel-hive-1.2.0-hook.jar " }, 
{ "title" : "5. Configure the Application Timeline Server (ATS)", 
"url" : "102062-install-hdp-part2.html#UUID-3f7f2010-3fa9-6c3f-147a-96568ff08530_section-5d44e1ef9efab-idm45315531795936", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Hortonworks Data Platform (HDP) \/ Part 2: Enabling additional instrumentation \/ 5. Configure the Application Timeline Server (ATS)", 
"snippet" : "If you're not running Tez, skip this step. If ATS requires authentication, set these properties iIn \/usr\/local\/unravel\/etc\/unravel.properties : Property\/Description Set by user Unit Default yarn.ats.webapp.username Username required for authentication to the Application Timeline Server (if authentic...", 
"body" : "If you're not running Tez, skip this step. If ATS requires authentication, set these properties iIn \/usr\/local\/unravel\/etc\/unravel.properties : Property\/Description Set by user Unit Default yarn.ats.webapp.username Username required for authentication to the Application Timeline Server (if authentication is required). Optional string - yarn.ats.webapp.password Password required for authentication to the Application Timeline Server (if authentication is required). Optional string - In Ambari, add these settings: In yarn-site.xml : yarn.timeline-service.enabled: true\nyarn.timeline-service.entity-group-fs-store.group-id-plugin-classes: org.apache.tez.dag.history.logging.ats.TimelineCachePluginImpl If yarn.acl.enable is true , add unravel to yarn.admin.acl . In hive-site.xml , append these values to the following parameters: hive.exec.failure.hooks: org.apache.hadoop.hive.ql.hooks.ATSHook\nhive.exec.post.hooks: org.apache.hadoop.hive.ql.hooks.ATSHook\nhive.exec.pre.hooks: org.apache.hadoop.hive.ql.hooks.ATSHook In hive-env.sh , add: Use ATS Logging: true In tez-site.xml , add: tez.history.logging.service.class: org.apache.tez.dag.history.logging.ats.ATSV15HistoryLoggingService\ntez.am.view-acls: unravel-\"run-as\"-user or * In Ambari, restart all services whose components you changed. " }, 
{ "title" : "8. Add more configuration and instrumentation options", 
"url" : "102062-install-hdp-part2.html#UUID-3f7f2010-3fa9-6c3f-147a-96568ff08530_section-5cb9fb32960bb-idm46756704322432", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Hortonworks Data Platform (HDP) \/ Part 2: Enabling additional instrumentation \/ 8. Add more configuration and instrumentation options", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Part 3: Next steps", 
"url" : "102063-install-hdp-part3.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Hortonworks Data Platform (HDP) \/ Part 3: Next steps", 
"snippet" : "Add more configuration and instrumentation options ....", 
"body" : "Add more configuration and instrumentation options . " }, 
{ "title" : "Adding a new node in an existing HDP cluster monitored by Unravel", 
"url" : "102064-hdp-new-node.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Hortonworks Data Platform (HDP) \/ Adding a new node in an existing HDP cluster monitored by Unravel", 
"snippet" : "1. Generate and distribute Unravel's Hive Hook and Spark Sensor JARs On Unravel Server, log in as root and confirm that wget is installed. yum install -y wget Run the following commands: mkdir \/usr\/local\/unravel-jars chmod 775 -R \/usr\/local\/unravel-jars\/ chown root:hadoop \/usr\/local\/unravel-jars\/  c...", 
"body" : "1. Generate and distribute Unravel's Hive Hook and Spark Sensor JARs On Unravel Server, log in as root and confirm that wget is installed. yum install -y wget Run the following commands: mkdir \/usr\/local\/unravel-jars\nchmod 775 -R \/usr\/local\/unravel-jars\/\nchown root:hadoop \/usr\/local\/unravel-jars\/ \n\nchmod +x \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/unravel_hdp_setup.py\n\ncd \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/\n\nsudo python2 unravel_hdp_setup.py --sensor-only --unravel-server unravel-host :3000 --spark-version spark-version --hive-version hive-version --ambari-server ambari-host \n\ncp \/usr\/local\/unravel_client\/unravel-hive- hive-version -hook.jar \/usr\/local\/unravel-jars\/\n\ncp -pr \/usr\/local\/unravel-agent\/jars\/* \/usr\/local\/unravel-jars\/\n For unravel-host , specify the protocol (http or https) and use the fully qualified domain name (FQDN) or IP address of Unravel Server. For example, https:\/\/playground3.unraveldata.com:3000 . For spark-version , use a Spark version that is compatible with this version of Unravel . For example, compmatrix-platform spark-2.0 for Spark 2.0.x spark-2.1 for Spark 2.1.x spark-2.2 for Spark 2.2.x spark-2.3 for Spark 2.3.x For hive-version , use a Hive version that is compatible with this version of Unravel . For example, compmatrix-platform 1.2.0 for Hive 1.2.0 or 1.2.1 0.13.0 for Hive 0.13.0 Files are installed locally in these two directories: \/usr\/local\/unravel_client (Hive Hook JAR) \/usr\/local\/unravel-agent\/jars\/ (Resource metrics sensor JARs) Make a note of these directories because you'll need to specify them in Ambari. Distribute \/usr\/local\/unravel_client and \/usr\/local\/unravel-agent\/jars\/ to all worker, edge, and master nodes which run queries. For example, scp -r \/usr\/local\/unravel_client\/ root@ hostname :\/usr\/local\/unravel-jars\n\nscp -r \/usr\/local\/unravel-agent\/ root@ hostname :\/usr\/local\/unravel-jars On each instrumented node, do the following: Make sure the node can reach port 4043 of Unravel Server. In Ambari, find the hive-env template , and add this line to the bottom of the file: export AUX_CLASSPATH=${AUX_CLASSPATH}:\/usr\/local\/unravel-jars\/unravel-hive-1.2.0-hook.jar \n For example, In Ambari, find the hadoop-env template , and add this line to the bottom of the file: export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:\/usr\/local\/unravel-jars\/unravel-hive-1.2.0-hook.jar For example, 2. For Oozie, copy the Hive Hook and BTrace JARs to the HDFS shared library path Copy the Hive Hook JAR, \/usr\/local\/unravel_client\/unravel-hive-1.2.0-hook.jar and the Btrace JAR, \/usr\/local\/unravel-agent\/jars\/btrace-agent.jar to the HDFS shared library path specified by oozie.libpath . If you don't do this, jobs controlled by Oozie 2.3+ will fail. If you are launching Spark actions: Copy the jar for the Spark version you are using, for example, spark-2.3. If you copy multiple Spark jars, Oozie won't be to launch actions. Ensure that the Spark event log location is configured the same as the local Spark jobs event logs' directory. In other words, Oozie must be able to locate the event log directory to store its event history logs. " }, 
{ "title" : "3. If you have changed your Kerberos tokens or principal you must perform the following steps:", 
"url" : "102064-hdp-new-node.html#UUID-0db9fe0c-30e4-a33c-b63b-7d5630936b42_section-idm4550216482209631512898697301", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Hortonworks Data Platform (HDP) \/ Adding a new node in an existing HDP cluster monitored by Unravel \/ 3. If you have changed your Kerberos tokens or principal you must perform the following steps:", 
"snippet" : "Update the following properties to ensure the latest Kerberos keytab file for Unravel is available on Unravel servers. com.unraveldata.kerberos.principal= new principal com.unraveldata.kerberos.keytab.path= new path Make sure the new file's ownership\/permission is restored to the original setup. Res...", 
"body" : "Update the following properties to ensure the latest Kerberos keytab file for Unravel is available on Unravel servers. com.unraveldata.kerberos.principal= new principal \ncom.unraveldata.kerberos.keytab.path= new path Make sure the new file's ownership\/permission is restored to the original setup. Restart all services. sudo \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "MapR", 
"url" : "102065-install-mapr.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ MapR", 
"snippet" : "This topic explains how to deploy Unravel Server and Sensors on the MapR converged data platform. This section shows you how to: Do a pre-installation check. Perform the MySQL pre-installation steps. Install the Unravel RPM. Perform the MySQL post-installation steps. Configure Unravel Server. Distri...", 
"body" : "This topic explains how to deploy Unravel Server and Sensors on the MapR converged data platform. This section shows you how to: Do a pre-installation check. Perform the MySQL pre-installation steps. Install the Unravel RPM. Perform the MySQL post-installation steps. Configure Unravel Server. Distribute Unravel's sensor JARs to desired nodes in the cluster. Adjust your cluster's configuration to integrate with Unravel. Restart needed components on your cluster. Configure additional integrations like Kafka and LDAP. " }, 
{ "title" : "Prerequisites", 
"url" : "102066-mapr-pre.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ MapR \/ Prerequisites", 
"snippet" : "Ensure that your environment meets these requirements. Platform Each version of Unravel has specific platform requirements. Confirm that your MapR meets the requirements for the version of Unravel that you're installing . compmatrix-platform Sizing You must have separate nodes for the Unravel server...", 
"body" : "Ensure that your environment meets these requirements. Platform Each version of Unravel has specific platform requirements. Confirm that your MapR meets the requirements for the version of Unravel that you're installing . compmatrix-platform Sizing You must have separate nodes for the Unravel server and the external MySQL database . Unravel Server Architecture: x86_64 vm.max_map_count is set to 262144 Minimum requirements for cores, RAM, and disks: The table below lists the minimum requirements for cores, RAM, and disks for a typical environment with default data retention and lookback settings . \/usr\/local\/unravel is the storage location for Unravel binaries. \/srv\/unravel is used for Elasticsearch (ES) and the bundled database. In production environments, put \/usr\/local\/unravel and \/srv\/unravel on separate disks. Putting \/srv\/unravel on a separate high spin HDD with its own SATAIII (or equivalent) bus significantly increases IO bandwidth. If \/usr\/local\/unravel or \/srv\/unravel doesn't have the minimum free space shown in the table below, create symbolic links for them to another disk. To check the space on a volume use the df command. For example, df -h \/srv Jobs per day Cores RAM \/usr\/local\/unravel \/srv\/unravel Less than 50,000 8 96 GB 8 GB free 500 GB free 50,000 to 100,000 to 8 128GB 8 GB free 500 GB free Over 100,000 Contact Unravel Support All volumes are mounted. \/tmp is mounted with executable permissions. To re-mount \/tmp with executable permissions use the following command: mount -o remount,exec \/tmp MySQL Server Minimum requirements for cores, RAM, and disk. Jobs per day Data retention length Cores RAM Disk Less than 50,000 30 days 4 32 GB 1 TB 60 days 4 32 GB 2 TB 50,000 to 100,000 to 30 days 8 64 GB 2 TB 60 days 8 64 GB 4 TB Over 100,000 Contact Unravel Support Software If you're running Red Hat Enterprise Linux (RHEL) 6.x, boostrap.system_call_filter is set to false in elasticsearch.yml : boostrap.system_call_filter: false libaio.x86_64 is installed. For Unravel version 4.5.0.0, SELINUX is set to permissive or disabled in \/etc\/sysconfig\/selinuxg . For Unravel versions 4.5.0.1+, SELINUX can be set to enabled . PATH includes the path to the HDFS+Hive+YARN+Spark client\/gateway, Hadoop commands, and Hive commands. Zookeeper is not installed on the same host as the Unravel host. Permissions You must have root access or \"sudo root\" permission in order to install the Unravel Server RPM. If you're using Kerberos, we'll explain how to create a principal and keytab for Unravel daemons to use to access these HDFS resources: MapReduce logs YARN's log aggregation directory Spark and Spark2 event logs File and partition sizes in the Hive warehouse directory Unravel needs access to the YARN Resource Manager's REST API (so that the principal can determine which resource manager is active). Unravel needs access to the JDBC access to the Hive Metastore. Read-only access is sufficient. Network On the new node, open the following ports: Port(s) Direction Description 3000 Both Traffic to and from Unravel UI 3316 Both Database traffic 4020 Both Unravel APIs 4021 Both Host monitoring of JMX on localhost 4031 Both Database traffic 4043 In UDP and TCP ingest traffic from the entire cluster to Unravel Server(s) 4044-4049 In UDP and TCP ingest spares for unravel_lr* 4091-4099 Both Kafka brokers 4171-4174, 4176-4179 Both ElasticSearch; localhost communication between Unravel daemons or Unravel Servers in a multi-host deployment 4181-4189 Both Zookeeper daemons 4210 Both Cluster access service HDFS ports Both Traffic to\/from the cluster to Unravel Server(s) Hive metadata database port Out For YARN only. Traffic from Hive to Unravel Server(s) for partition reporting 8088 Out Traffic from Unravel Server(s) to the Resource Manager API 8188 Out Traffic from Unravel Server(s) to the ATS server(s) 11000 Out For Oozie only. Traffic from Unravel Server(s) to the Oozie server " }, 
{ "title" : "Part 1: Installing Unravel Server on MapR", 
"url" : "102067-mapr-part1.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ MapR \/ Part 1: Installing Unravel Server on MapR", 
"snippet" : "This topic explains how to deploy Unravel Server on the MapR converged data platform. If you have not already done so, confirm your cluster meets Unravel's hosting requirements . 1. Configure the host Allocate a cluster gateway\/edge\/client host with HDFS access. Enable the hadoop fs command, Hive, a...", 
"body" : "This topic explains how to deploy Unravel Server on the MapR converged data platform. If you have not already done so, confirm your cluster meets Unravel's hosting requirements . 1. Configure the host Allocate a cluster gateway\/edge\/client host with HDFS access. Enable the hadoop fs command, Hive, and Spark. Although Unravel Server doesn't launch Hive or Spark jobs, it's convenient to have Hive and Spark installed on this gateway\/edge\/client host. For more information about the MapR client configuration, see https:\/\/maprdocs.mapr.com\/52\/ReferenceGuide\/configure.sh.html Run the following commands this gateway\/edge\/client host as root , substituting your site-specific values for name , cldb-list , and history-server . sudo yum install mapr-client.x86_64\nsudo \/opt\/mapr\/server\/configure.sh -N name -c -C cldb-list -HS history-server \nsudo yum install mapr-hive.noarch\nsudo yum install mapr-spark.noarch Check\/add\/modify these MapR settings: Run the following commands on Unravel Server as root : sudo useradd -g mapr unravel\nhadoop fs -mkdir \/user\/unravel\nhadoop fs -chown unravel:mapr \/user\/unravel If MapR tickets are enabled, make sure you have tickets for users unravel and mapr on the target host. You might need to export ticket environment variables (such as MAPR_TICKETFILE_LOCATION ) in \/srv\/unravel\/unravel_ctl first. Check\/adjust available RAM on the Unravel gateway\/client host: free -g For instructions on adjusting RAM allocated to MapR-FS (mfs), see https:\/\/community.mapr.com\/docs\/DOC-1209 . For example, edit \/opt\/mapr\/conf\/warden.conf as follows: service.command.mfs.heapsize.maxpercent=10 Restart MapR-FS (mfs). 3. Install Unravel Server on the host Download the Unravel Server RPM. downloads Ensure that the host machine's local disks have the minimum space required . Unravel Server uses two separate disks: one for binaries ( \/usr\/local\/unravel ) and one for data ( \/srv\/unravel ). The separate disk \/srv\/unravel is beneficial for performance. If either disk doesn't have the minimum space required, create symbolic links for them to another disk drive. To check the space on a volume use the df command. For example, df -h \/srv Install the Unravel Server RPM. sudo rpm -Uvh unravel- version .rpm The installation creates the following items: \/usr\/local\/unravel\/ , which contains executables, scripts, properties file ( unravel.properties ), and logs. \/etc\/init.d\/unravel_* , which contains scripts for controlling services, such as unravel_all.sh for manually stopping, starting, and getting the status of all daemons in proper order. User unravel if it doesn't exist already. 5. Modify Unravel Server for MapR Run the following commands on Unravel Server. sudo \/etc\/init.d\/unravel_all.sh stop\nsudo \/usr\/local\/unravel\/install_bin\/switch_to_mapr.sh This change is persistent; you don't have to do this when you upgrade the RPM. 6. Configure Unravel Server with basic options (Optional) Enable additional daemons for high-volume workloads . In \/usr\/local\/unravel\/etc\/unravel.properties , set general properties for Unravel Server. Property\/Description Set by user Unit Default com.unraveldata.customer.organization Customer name. Used to identify your installation for reporting and notification purposes in Unravel UI. Optional string Not Set com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. Example: http:\/\/unravelserver.company.com:3000   string http:\/\/{host}:3000 com.com.unraveldata.hdfs.timezone Timezone of HDFS, for example, US\/Eastern, Etc\/GMT-4, America\/New_York. If the timezone is not set then an error message is logged and UTC timezone is used. Possible timezones can be obtained by calling TimeZone.getAvailableIDs() . string - com.unraveldata.tmpdir The base location for Unravel process control files where Unravel's temp files reside. string (path) \/srv\/unravel\/tmp com.unraveldata.history.maxSize.weeks Number of weeks retained for search results in Elastic Search. integer 5 com.unraveldata.retention.max.days Number of days to keep the heaviest data (such as error logs and drill-down details) in the SQL Database. integer 30 Point Unravel Server to logs on HDFS. Unravel collects HDFS logs for analysis. To point Unravel Server to these logs, set the following properties in \/usr\/local\/unravel\/etc\/unravel.properties : Property\/Description Set by user Unit Default com.unraveldata.job.collector.done.log.base HDFS path to \"done\" directory of MR logs. Don't include the hdfs:\/\/ prefix For HDP set this to: \/mr-history\/done . string \/user\/history\/done com.unraveldata.job.collector.log.aggregation.base HDFS path to the aggregated container logs (logs to process). Don't include the hdfs:\/\/ prefix. The log format defaults to TFile. You can specify multiple logs and log formats (TFile or IndexedFormat). Example: TFile:\/tmp\/logs\/*\/logs\/,IndexedFormat:\/tmp\/logs\/*\/logs-ifile\/. For HDP set this to: IndexedFormat:\/app-logs\/*\/logs\/ . CSL \/tmp\/logs\/*\/logs\/ com.unraveldata.spark.eventlog.location Comma-separated list of HDFS paths to the Spark event logs. Each path must include the hdfs:\/\/\/ prefix. For HDP set this to: hdfs:\/\/\/spark1-history\/,hdfs:\/\/\/spark2-history\/ . CSL hdfs:\/\/\/user\/spark\/applicationHistory\/ If the Application Timeline Server (ATS) requires user authentication, set the following properties: Property\/Description Set by user Unit Default yarn.ats.webapp.username Username required for authentication to the Application Timeline Server (if authentication is required). Optional string - yarn.ats.webapp.password Password required for authentication to the Application Timeline Server (if authentication is required). Optional string - Connect to the Oozie server by setting oozie.server.url . Enable https access to Resource Manager by setting https.protocol . Define the monitoring frequency. Property\/Description Set by user Unit Default com.unraveldata.s3.batch.monitoring.interval.sec Defines the monitoring frequency. Set this property to 60 for lower latency. s 180 If Kerberos is enabled, add authentication for HDFS: Create or identify a principal and keytab for Unravel daemons to access HDFS and REST when Kerberos is enabled. Find and verify the principal keytab by running this command: klist -kt KEYTAB_FILE Set the Linux file permissions of the keytab file to 500 ( chmod 500 ) and set its owner set to unravel or to your chosen user, as explained in Run Unravel Daemons with Custom User . In \/usr\/local\/unravel\/etc\/unravel.properties , add\/set these properties for Kerberos: com.unraveldata.kerberos.principal=unravel\/ my-host . my-domain @ my-realm \ncom.unraveldata.kerberos.keytab.path=\/usr\/local\/unravel\/etc\/unravel.keytab If Sentry is enabled, add these permissions: Define your own alt principal with narrow privileges and the access permissions shown in the table below. The alt principal can be unravel (default) or one of your choosing. The corresponding kerberos principal does not need to be the same as the local user. Verify that the user running the Unravel daemon \/etc\/unravel_ctl has the access permissions shown in the table below. Resource Principal Access Purpose hdfs:\/\/user\/spark\/applicationHistory mapr or alternate read+execute Spark event log hdfs:\/\/usr\/history\/done mapr or alternate read+execute MapReduce logs hdfs:\/\/tmp\/logs mapr or alternate read+execute YARN aggregation folder hdfs:\/\/user\/hive\/warehouse mapr or alternate read+execute Obtain table partition sizes Hive Metastore access hive read+execute Hive table information 9. Start Unravel services Run the following command to start all Unravel services: sudo \/etc\/init.d\/unravel_all.sh start\nsleep 60 This completes the basic\/core configuration. 10. Log into Unravel UI Find the hostname of Unravel Server. echo \"http:\/\/$(hostname -f):3000\/\" If you're using an SSH tunnel or HTTP proxy, you might need to make adjustments. Using a supported web browser , navigate to http:\/\/ unravel-host :3000 and log in with username admin with password unraveldata . compmatrix-platform Unravel UI displays collected data . 11. Enable additional instrumentation " }, 
{ "title" : "2. Install MySQL", 
"url" : "102067-mapr-part1.html#UUID-1fcf7a93-169b-8a08-0550-fe789d647c2d_section-5cd09b081436d-idm45444639878864", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ MapR \/ Part 1: Installing Unravel Server on MapR \/ 2. Install MySQL", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "4. Configure MySQL", 
"url" : "102067-mapr-part1.html#UUID-1fcf7a93-169b-8a08-0550-fe789d647c2d_section-5cd09bd696d67-idm45444640181408", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ MapR \/ Part 1: Installing Unravel Server on MapR \/ 4. Configure MySQL", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "7. Change the run-as user and group for Unravel daemons", 
"url" : "102067-mapr-part1.html#UUID-1fcf7a93-169b-8a08-0550-fe789d647c2d_section-5d1550ecf24b4-idm45315556571872", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ MapR \/ Part 1: Installing Unravel Server on MapR \/ 7. Change the run-as user and group for Unravel daemons", 
"snippet" : "Unravel daemons run under the local user unravel by default. However, if you have Kerberos or Ranger\/Sentry enabled, or a non-Kerberos cluster with simple Unix security, or a different username for the Unravel user, or a non-local user such as an LDAP user, run switch_to_user.sh script to change the...", 
"body" : "Unravel daemons run under the local user unravel by default. However, if you have Kerberos or Ranger\/Sentry enabled, or a non-Kerberos cluster with simple Unix security, or a different username for the Unravel user, or a non-local user such as an LDAP user, run switch_to_user.sh script to change the Unix owner and group of the Unravel daemons. " }, 
{ "title" : "8. Connect to the Hive metastore", 
"url" : "102067-mapr-part1.html#UUID-1fcf7a93-169b-8a08-0550-fe789d647c2d_section-5d15511cad5f0-idm45764268642800", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ MapR \/ Part 1: Installing Unravel Server on MapR \/ 8. Connect to the Hive metastore", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Part 2: Enabling additional instrumentation", 
"url" : "102068-mapr-part2.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ MapR \/ Part 2: Enabling additional instrumentation", 
"snippet" : "This topic explains how to enable additional instrumentation on your gateway\/edge\/client nodes. 1. Run the setup script On the Unravel host, run unravel_mapr_setup.py with the following values: For unravel-host , use Unravel Server's fully qualified domain name (FQDN) or IP address. For spark-versio...", 
"body" : "This topic explains how to enable additional instrumentation on your gateway\/edge\/client nodes. 1. Run the setup script On the Unravel host, run unravel_mapr_setup.py with the following values: For unravel-host , use Unravel Server's fully qualified domain name (FQDN) or IP address. For spark-version , use a Spark version that is compatible with this version of Unravel . For example, compmatrix-platform spark-2.0.0 for Spark 2.0.x spark-2.1.0 for Spark 2.1.x spark-2.2.0 for Spark 2.2.x spark-2.3.0 for Spark 2.3.x For hive-version , use a Hive version that is compatible with this version of Unravel . For example, compmatrix-platform 1.2.0 for Hive 1.2.0 or 1.2.1 0.13.0 for Hive 0.13.0 For tez-version , use a Tez version that is compatible with this version of Unravel . For example, compmatrix-platform 0.8 for Tez 0.8 For sensor installation... sudo python \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/unravel_mapr_setup.py --unravel-server unravel-host --spark-version spark-version --hive-version hive-version \nsudo python \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/unravel_mapr_setup.py --unravel-server unravel-host --spark-version spark-version --hive-version hive-version --sensor-only For sensor upgrade... sudo python \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/unravel_mapr_setup.py --unravel-server unravel-host --spark-version spark-version --hive-version hive-version --sensor-only For dry runs (to test instrumentation)... This doesn't change any configuration file. sudo python \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/unravel_mapr_setup.py --unravel-server unravel-host --spark-version spark-version --hive-version hive-version --dry-run The setup script, unravel_mapr_setup.py , does the following: It puts the Hive Hook JAR in \/usr\/local\/unravel_client\/ It puts the resource metrics JAR in \/usr\/local\/unravel-agent\/ For MapR 5.2 or MapR 6.0, it changes the contents of these configuration files: \/opt\/mapr\/spark\/spark- spark-version \/conf\/spark-defaults.conf \/opt\/mapr\/hive\/hive- hive-version \/conf\/hive-site.xml \/opt\/mapr\/hive\/hive- hive-version \/conf\/hive-env.sh \/opt\/mapr\/hadoop\/hadoop- hadoop-version \/etc\/hadoop\/yarn-site.xml \/opt\/mapr\/hadoop\/hadoop- hadoop-version \/etc\/hadoop\/mapred-site.xml \/usr\/local\/unravel\/etc\/unravel.properties It saves a copy of each original configuration file in the same directory. The copies are is named *.preunravel . For example, \/opt\/mapr\/hive\/hive-1.2\/conf\/hive-site.xml.preunravel . Once the files are present on the Unravel host, you can compress them with the tar command and distribute them to other hosts, if that is more convenient than running the script. All instrumented nodes must be able to open port 4043 of Unravel Server (host2 if multi-host Unravel install). 2. For Oozie, copy Unravel Hive Hook and BTrace JARs to the HDFS shared library path Copy the Hive Hook JAR in \/usr\/local\/unravel_client\/ and the metrics JARs in \/usr\/local\/unravel-agent\/ to the shared lib path specified by oozie.libpath . If you don't do this, jobs controlled by Oozie 2.3+ will fail. 3. Confirm that Unravel Web UI shows additional data Run a Hive job using a test script provided by Unravel Server. This is where you can see the effects of the instrumentation setup. Best practice is to run this test script on Unravel Server rather than on a gateway\/edge\/client node. That way you can verify that instrumentation is working first, and then enable instrumentation on other gateway\/edge\/client nodes. username must be a user that can create tables in the default database. If you need to use a different database, copy the script and edit it to change the target database. This script creates a uniquely named table in the default database, adds some data, runs a Hive query on it, and then deletes the table. It runs the query twice using different workflow tags so you can clearly see the two different runs of the same workflow in Unravel UI. sudo -u username \/usr\/local\/unravel\/install_bin\/hive_test_simple.sh 4. Confirm and adjust the settings in yarn-site.xml Check specific properties in \/opt\/mapr\/hadoop\/hadoop-2.7.0\/etc\/hadoop\/yarn-site.xml to be sure that these settings are present: yarn.resourcemanager.webapp.address <property> \n<name>yarn.resourcemanager.webapp.address<\/name> \n<value> your-resource-manager-webapp-ip-address :8088<\/value>\n<source>yarn-site.xml<\/source>\n<\/property> yarn.log-aggregation-enable <property>\n<name>yarn.log-aggregation-enable<\/name> \n<value>true<\/value>\n<description>For log aggregations<\/description>\n<\/property> 5. Enable additional instrumentation on other hosts in the cluster Run the shell script unravel_mapr_setup.sh on each node of the cluster, just like you ran it on Unravel Server, above. Copy the newly edited yarn-site.xml , to all nodes. Do a rolling restart of HiveServer2. To instrument more servers, you can use the setup script we provide or see the effect it has and replicate that effect using your own automated provisioning system. If you already have a way to customize and deploy hive-site.xml , yarn-site.xml , and user defined function JARs, you can add the changes and JAR from Unravel to your existing mechanism. 6. Enable instrumentation manually Enable instrumentation manually by updating hive-site.xml , hive-env.sh , spark-defaults.conf , hadoop-env.sh , mapred-site.xml , and tez-site.xml , as explained below. Once the files are updated on the Unravel host, you can use the scp command to copy them to other hosts. Back up your original files in case you need to roll back changes. In all cases, instrumented nodes must be able to open port 4043 of Unravel Server (host2 if multi-host Unravel install). Update hive-site.xml . Append the contents of \/usr\/local\/unravel\/hive-hook\/hive-site.xml.snip to \/opt\/mapr\/hive\/hive- hive-version \/conf\/hive-site.xml right before <\/configuration> . <property>\n<name>com.unraveldata.host<\/name>\n<value> unravel-host <\/value>\n<description>Unravel hive-hook processing host<\/description>\n<\/property>\n<property>\n<name>com.unraveldata.hive.hook.tcp<\/name>\n<value>true<\/value>\n<\/property>\n<property>\n<name>com.unraveldata.hive.hdfs.dir<\/name>\n<value>\/user\/unravel\/HOOK_RESULT_DIR<\/value>\n<description>destination for hive-hook, Unravel log processing<\/description>\n<\/property>\n<property>\n<name>hive.exec.driver.run.hooks<\/name>\n<value>com.unraveldata.dataflow.hive.hook.UnravelHiveHook<\/value>\n<description>for Unravel, from unraveldata.com<\/description>\n<\/property>\n<property>\n<name>hive.exec.pre.hooks<\/name>\n<value>com.unraveldata.dataflow.hive.hook.UnravelHiveHook<\/value>\n<description>for Unravel, from unraveldata.com<\/description>\n<\/property>\n<property>\n<name>hive.exec.post.hooks<\/name>\n<value>com.unraveldata.dataflow.hive.hook.UnravelHiveHook<\/value>\n<description>for Unravel, from unraveldata.com<\/description>\n<\/property>\n<property>\n<name>hive.exec.failure.hooks<\/name>\n<value>com.unraveldata.dataflow.hive.hook.UnravelHiveHook<\/value>\n<description>for Unravel, from unraveldata.com<\/description>\n<\/property>\n<\/configuration> Update hive-env.sh . In \/opt\/mapr\/hive\/hive- hive-version \/conf\/hive-env.sh , append these lines: export AUX_CLASSPATH=${AUX_CLASSPATH}:\/usr\/local\/unravel_client\/unravel-hive- hive-version -hook.jar \nexport HIVE_AUX_JARS_PATH=${HIVE_AUX_JARS_PATH}:\/usr\/local\/unravel_client Update spark-defaults.conf . In \/opt\/mapr\/spark\/spark- spark-version \/conf\/spark-defaults.conf , append these lines: spark.unravel.server.hostport unravel-host :4043 spark.eventLog.dir maprfs:\/\/\/apps\/spark \n\/\/ the following is one line\nspark.history.fs.logDirectory maprfs:\/\/\/apps\/spark spark.driver.extraJavaOptions -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=spark- spark-version ,config=driver \n\/\/ the following is one line\nspark.executor.extraJavaOptions -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=spark- spark-version ,config=executort Update hadoop-env.sh . In \/opt\/mapr\/hadoop\/hadoop-HADOOP_VERSION_X.Y.Z\/etc\/hadoop\/hadoop-env.sh , append these lines: export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:\/usr\/local\/unravel_client\/unravel-hive- hive-version .0-hook.jar Update mapred-site.xml . In \/opt\/mapr\/hadoop\/hadoop- hadoop-version \/etc\/hadoop\/mapred-site.xml , append these lines: <property>\n<name>mapreduce.task.profile<\/name>\n<value>true<\/value>\n<\/property>\n<property>\n<name>mapreduce.task.profile.maps<\/name>\n<value>0-5<\/value>\n<\/property>\n<property>\n<name>mapreduce.task.profile.reduces<\/name>\n<value>0-5<\/value>\n<\/property>\n<property>\n<name>mapreduce.task.profile.params<\/name>\n<value>-javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= unravel-host :4043<\/value>\n<\/property>\n<property>\n<name>yarn.app.mapreduce.am.command-opts<\/name>\n<value>-javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= unravel-host :4043<\/value>\n<\/property> Make sure the original value of yarn.app.mapreduce.am.command-opts is preserved, by appending the Java agent setup rather than replacing the original value. Update tez-site.xml . In \/opt\/mapr\/tez\/ tez-version \/conf\/tez-site.xml , append these lines: <property>\n <name>tez.task.launch.cmd-opts<\/name>\n <value>-javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr,config=tez -Dunravel.server.hostport= unravel-host :4043<\/value>\n <description \/>\n<\/property>\n\n<property>\n <name>tez.am.launch.cmd-opts<\/name>\n <value>-javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr,config=tez -Dunravel.server.hostport= unravel-host :4043<\/value>\n <description \/>\n<\/property> " }, 
{ "title" : "7. Add more configuration and instrumentation options", 
"url" : "102068-mapr-part2.html#UUID-fb0cc6f0-fbc7-965d-f264-e6a830d9d4a7_section-5cb9f66cc078d-idm45885589914016", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ MapR \/ Part 2: Enabling additional instrumentation \/ 7. Add more configuration and instrumentation options", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Part 3: Next steps", 
"url" : "102069-mapr-part3.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ MapR \/ Part 3: Next steps", 
"snippet" : "Add more configuration and instrumentation options ....", 
"body" : "Add more configuration and instrumentation options . " }, 
{ "title" : "Amazon Elastic MapReduce (EMR)", 
"url" : "102070-install-emr.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR)", 
"snippet" : "This topic explains how to deploy Unravel on Amazon Elastic MapReduce (EMR). Deploying Unravel takes less than an hour in most environments. In some environments, deployment takes longer due to the complexity of security\/VPC settings, various permissions' setup, and so on. Quickstart If you want to ...", 
"body" : "This topic explains how to deploy Unravel on Amazon Elastic MapReduce (EMR). Deploying Unravel takes less than an hour in most environments. In some environments, deployment takes longer due to the complexity of security\/VPC settings, various permissions' setup, and so on. Quickstart If you want to try Unravel quickly (for development or test environments), deploy Unravel Azure Databricks Marketplace . Supported clusters Cluster with respect to Unravel setup Supported New cluster without auto-scaling ✓ New cluster with auto-scaling ✓ Existing cluster without auto-scaling ✓ Existing cluster with auto-scaling ✖️ " }, 
{ "title" : "Prerequisites", 
"url" : "102071-emr-pre.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Prerequisites", 
"snippet" : "Platform Each version of Unravel has specific platform requirements. Check Unravel's compatibility matrix to confirm your Amazon EMR platform meets the requirements for the Unravel version you are installing. EMR Hardware EC2 instance type: Minimum: r4.2xlarge (61 GiB RAM) Maximum: r4.8xlarge (244 G...", 
"body" : "Platform Each version of Unravel has specific platform requirements. Check Unravel's compatibility matrix to confirm your Amazon EMR platform meets the requirements for the Unravel version you are installing. EMR Hardware EC2 instance type: Minimum: r4.2xlarge (61 GiB RAM) Maximum: r4.8xlarge (244 GiB RAM) Recommended: r4.4xlarge (122 GiB RAM) Virtualization type: HVM Root device type: EBS EBS volume specifications: Minimum: 100GiB. In a PoC or evaluation, the minimum root disk space should be sufficient. When monitoring more EMR clusters or lots of jobs, we recommend a 300-500GB Provisioned IOPS SSD (io1) volume with 3000 IOPS. For production use, we recommend a 200GB Provisioned IOPS EBS and RDS volume. The Baseline IOPS (3 IOPS per GiB with a minimum of 100 IOPS, burstable to 3000 IOPS) is sufficient for Unravel (Optional) RDS specifications: DB instance class: db.r3.xlarge (4 vCPU, 30.5 GiB RAM) Storage type: Provisioned IOPS (SSD) Allocated storage: 200 GiB or above Provisioned IOPS: 1000 Unravel Server doesn't require a heavy resources, but it's best to check your AWS Service Limits as you proceed. For example, if you provision an Unravel EC2 instance from our CloudFormation template, check Virtual Private Cloud (Amazon VPC) Limits . Sizing You must have separate nodes for the Unravel server and the external MySQL database . Unravel Server The minimum requirements for cores, RAM, and directories for a typical environment with default data retention and lookback settings. \/usr\/local\/unravel is the storage location for Unravel binaries. \/srv\/unravel is used for Elasticsearch (ES) and the bundled database. Root device type recommended: EBS - Provisioned IOPS SSD (io1). Jobs per day Cores RAM \/usr\/local\/unravel \/srv\/unravel Less than 50,000 8 96 GB 8 GB free 500 GB free 50,000 to 100,000 to 8 128GB 8 GB free 500 GB free Over 100,000 Contact Unravel Support MySQL Server The minimum requirements for cores, RAM, and disk. Jobs per day Data retention length Cores RAM Disk Less than 50,000 30 days 4 32 GB 1 TB 60 days 4 32 GB 2 TB 50,000 to 100,000 to 30 days 8 64 GB 2 TB 60 days 8 64 GB 4 TB Over 100,000 Contact Unravel Support Access permissions The Unravel EC2 instance must have read permission on the S3 bucket used by EMR clusters. You need an AWS account. You must be able to connect to AWS for the deployment process. Create an S3 ReadAccess only IAM role and assign it to Unravel Server to READ the archive logs on the S3 bucket configured for the EMR cluster. In other words, create an IAM role that contains the policy that can only READ the specific S3 bucket used on the EMR cluster; then, create an EC2 instance profile and add the IAM role to it. AWS Permissions and Access: You must have permission to: Create EC2 instances Connect to EC2 instances Install software on EC2 instances (you must have root access or \"sudo root\" permission in order to install the Unravel Server RPM) Create security groups and IAM roles Update IAM roles for the EMR cluster and the corresponding S3 storage If you want to deploy Unravel for a new EMR cluster, you also need AWS permissions to create an EMR cluster and necessary S3 buckets, create and configure VPCs, etc. Network The following ports must be open on the Unravel EC2 instance. In addition, the Unravel EC2 instance must be able to access all ports on the EMR cluster. Settings related to IAM roles and security groups In order to manage, monitor, and optimize the modern data applications running on your EMR cluster, Unravel needs data from the cluster as well as from apps running on the cluster. This data includes metrics, configuration information, and logs. Parts of this data is pushed to Unravel, and part of it is pulled by the daemons running on Unravel Server. In order for all data to be accessible, there must be both inbound and outbound access between Unravel Server (on the EC2 instance) and the EMR cluster. The Unravel Server must be in the same region as the target EMR cluster(s) it will be monitoring. There are two possible scenarios: Both the EMR cluster and the Unravel Server are created on the same VPC, same subnet; and the security group allows all traffic from the same subnet. The EMR cluster is located on a different VPC than Unravel Server. In this case you must configure VPC peering, route table creation, and update the security policy . The Unravel Server needs a TCP and UDP connection to the EMR master node. To implement this, do either of the following: Create a security group that allows port 3000 and port 4043 from the EMR cluster node's IP address. Configure the security group on Unravel Server to allow TCP traffic on ports 3000 for EMR cluster nodes. Put the member of security group used on the EMR cluster in this rule. The Unravel Server and EMR cluster(s) must allow all outbound traffic. EMR cluster nodes must allow all traffic from Unravel Server. If you can't allow Unravel Server to access all traffic, you must minimally allow Unravel Server to access cluster nodes' TCP port 8020, 50010, and 50020. Port(s) Direction Description 3000 Both Non- HTTPS traffic to and from Unravel UI 4043 In UDP and TCP ingest traffic from the entire cluster to Unravel Server(s) Skill set These instructions assume you're proficient in: Provisioning EC2 instances and RDS instances Creating and configuring the required IAM roles, security groups, and so on Understanding AWS networking concepts such as virtual private clouds (VPCs), subnets, and so on Running Ansible scripts, basic Unix commands, and AWS CLI commands You don't need to create any scripts or be familiar with any specific programming\/scripting language. These instructions are self-contained, and require only basic knowledge of AWS. Expert-level knowledge of AWS is not required. " }, 
{ "title" : "Architecture", 
"url" : "102072-emr-arch.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Architecture", 
"snippet" : "In order to manage, monitor, and optimize the modern data applications running on your EMR cluster, Unravel Server needs data corresponding to the EMR cluster as well as about the modern data apps running on the cluster. This information includes metrics, configuration information, and logs. Some of...", 
"body" : "In order to manage, monitor, and optimize the modern data applications running on your EMR cluster, Unravel Server needs data corresponding to the EMR cluster as well as about the modern data apps running on the cluster. This information includes metrics, configuration information, and logs. Some of this data is pushed to Unravel, and some is pulled by the daemons in Unravel Server. In order for this to work, you must allow both inbound and outbound traffic between Unravel Server (on the EC2 instance) and the EMR cluster. For details, see . Backing up Unravel amounts to RDS backup (if that is the chosen database) and backing up the state of the Unravel Server. For more information, see . " }, 
{ "title" : "Planning guidance", 
"url" : "102073-emr-ec2-plan.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Planning guidance", 
"snippet" : "This section explains what settings you need to specify or give access to, and why. Security aspects related to Unravel Server Unravel UI and API are Unravel's user-facing components. For instructions on enabling TLS (SSL) for the Unravel UI, see Enabling TLS . Risk audit mechanism Unravel logs its ...", 
"body" : "This section explains what settings you need to specify or give access to, and why. Security aspects related to Unravel Server Unravel UI and API are Unravel's user-facing components. For instructions on enabling TLS (SSL) for the Unravel UI, see Enabling TLS . Risk audit mechanism Unravel logs its actions on the same EC2 instance that hosts Unravel Server, in \/usr\/local\/unravel\/logs\/unravel_ngui.log . Costs There are two components of the cost to the user in using Unravel: Cost of AWS components: Cost of the running Amazon EC2 instance that has the Unravel Server deployed on it + Cost of Amazon EBS storage + Cost of RDS (if applicable) References: Amazon EC2 pricing Amazon EBS pricing Amazon RDS pricing Cost of Unravel license: Unravel comes with a 30-day trial license. This means you can deploy and use Unravel for 30 days without any additional licenses. To obtain a license after the trial, contact us to discuss your needs and Unravel's terms and conditions. AWS cost allocation tags can be used to track your AWS costs on a detailed level. It is recommended to enable and set both types of cost allocation tags: AWS generated tags as well as user-defined tags . For details, see Using cost allocation tags . " }, 
{ "title" : "Installing Unravel Server on an EC2 instance", 
"url" : "102074-emr-ec2-part1.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Installing Unravel Server on an EC2 instance", 
"snippet" : "This topic explains how to create a new EC2 instance, set up RDS (optionally), install and configure Unravel Server on the new EC2 instance, and connect it to the EMR cluster you want to monitor. If you have not already done so, confirm your cluster meets Unravel's hosting requirements . 1. Create a...", 
"body" : "This topic explains how to create a new EC2 instance, set up RDS (optionally), install and configure Unravel Server on the new EC2 instance, and connect it to the EMR cluster you want to monitor. If you have not already done so, confirm your cluster meets Unravel's hosting requirements . 1. Create an EC2 instance On your AWS console ( https:\/\/console.aws.amazon.com\/ ), go to the EC2 dashboard and click Launch Instance . Select the following options based on Unravel's instance requirements : Base OS Instance type and size EC2 instance's security group \/ IAM role Best practice is to create an IAM role that contains the policy that only reads the specific S3 bucket used on EMR cluster, and then create and instance profile and add the IAM role to it. Ports Networking The EC2 instance must be in same region with the target EMR clusters which Unravel EC2 node will be monitoring. Security groups or policies Create an S3 ReadAccess only IAM role and assign it to Unravel EC2 node to read the archive logs on the S3 bucket configured for the EMR cluster. Create TCP and UDP connections from the EMR master node to Unravel EC2 node. Create a security group that allows port 3000 and port 4043 from EMR cluster nodes' IP address, and put the member of the security group used on EMR cluster in this rule. Sample inbound rule Type Protocol Port range Source All traffic All All Security group ID of this group or subnet IP block. For example, 10.10.0.0\/16 SSH TCP 22 0.0.0.0\/0 or trusted public IP for SSH access Custom TCP Rule TCP 3000 Security group ID used on the EMR cluster or subnet IP block (if IP block belongs to a different VPC). Required for VPC peering connection. Custom TCP Rule TCP 4043 Security group ID used on the EMR cluster or subnet IP block (if IP block belongs to a different VPC). Required for VPC peering connection. Sample outbound rule Type Protocol Port range Source All traffic All All 0.0.0.0\/0 The Unravel EC2 node should have all TCP access to the EMR cluster (server\/parent or worker) nodes. You can grant access by inserting a security policy into both security groups of the EMR server\/parent and worker with all TCP, all port range. The source is the security group ID of the Unravel VM. For an example, see the screenshot below. If it isn't possible to allow the Unravel EC2 access to all traffic to EMR cluster, you must minimally allow it to access cluster nodes' TCP ports 8020, 50010 and 50020. 2. Configure the EC2 instance Disable selinux . sudo setenforce Permissive Edit \/etc\/selinux\/config to make sure the setting persists after reboot and make sure SELINUX=permissive . sudo vi \/etc\/selinux\/config Install libaio.x86_64 , lzop.x86_64 , and ntp.x86_64 . sudo yum install -y libaio.x86_64\nsudo yum install -y lzop.x86_64\nsudo yum install -y ntp.x86_64 Start ntpd and check the system time. sudo service ntpd start\nsudo ntpq -p Create a new user named hadoop . sudo useradd hadoop 4. Install the Unravel RPM on the EC2 instance Download the Unravel Server RPM. downloads Install the Unravel Server RPM. The precise filename varies depending on the version, how it was fetched, or copied. sudo rpm -Uvh unravel- version .rpm Run the await_fixups.sh script to ensure background processing is finished before proceeding. In a routine upgrade, it is okay to start all Unravel daemons, but do not stop or restart them until the await_fixups.sh prints Done . This may take a few minutes. \/usr\/local\/unravel\/install_bin\/await_fixups.sh\nsudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh hadoop hadoop Append the following line to \/usr\/local\/unravel\/etc\/unravel.properties : com.unraveldata.onprem=false For monitoring EMR Spark service, add the following properties to unravel.properties : com.unraveldata.spark.live.pipeline.enabled=true\ncom.unraveldata.spark.hadoopFsMulti.useFilteredFiles=true\ncom.unraveldata.spark.events.enableCaching=true The installation creates the following items: Virtualization type: HVM User unravel (if it doesn't exist already). \/etc\/init.d\/unravel_* scripts for controlling services, and \/etc\/init.d\/unravel_all.sh which you can use to manually stop, start, and get status of all daemons in proper order. 6. Log into Unravel UI Start Unravel daemons. sudo \/etc\/init.d\/unravel_all.sh start Create an SSH tunnel from your workstation to the Unravel EC2 instance. ssh -i ssh_key.pem centos@ unravel-host-ip -L 3000:127.0.0.1:3000 Using a supported web browser , navigate to http:\/\/127.0.0.1:3000 and log in with username admin with password unraveldata . compmatrix-platform Unravel UI displays collected data . " }, 
{ "title" : "3. Install MySQL", 
"url" : "102074-emr-ec2-part1.html#UUID-339d491e-7718-f5b1-7969-fdfa8eb1fd49_section-5cd09b081436d-idm45444639878864", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Installing Unravel Server on an EC2 instance \/ 3. Install MySQL", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "5. Configure MySQL", 
"url" : "102074-emr-ec2-part1.html#UUID-339d491e-7718-f5b1-7969-fdfa8eb1fd49_section-5cd09bd696d67-idm45444640181408", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Installing Unravel Server on an EC2 instance \/ 5. Configure MySQL", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "7. Connect to your existing or new EMR cluster", 
"url" : "102074-emr-ec2-part1.html#UUID-339d491e-7718-f5b1-7969-fdfa8eb1fd49_section-5d5d92b1ea459-idm45705481488384", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Installing Unravel Server on an EC2 instance \/ 7. Connect to your existing or new EMR cluster", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Connecting Unravel Server to a new or existing EMR cluster", 
"url" : "102075-emr-ec2-part2.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Connecting Unravel Server to a new or existing EMR cluster", 
"snippet" : "This topic explains how to set up and configure your EMR cluster so that Unravel can begin monitoring jobs running on the cluster. Assumptions The EC2 instance for Unravel Server has been created. Unravel services are running. The security group on the Unravel EC2 instance allows traffic to\/from EMR...", 
"body" : "This topic explains how to set up and configure your EMR cluster so that Unravel can begin monitoring jobs running on the cluster. Assumptions The EC2 instance for Unravel Server has been created. Unravel services are running. The security group on the Unravel EC2 instance allows traffic to\/from EMR cluster nodes on TCP port 3000. The Unravel EC2 instance and EMR clustersallow all outbound traffic. The nodes in the EMR cluster allow all traffic from the Unravel EC2 instance. This implies either of the following configurations: The EMR cluster and Unravel EC2 instance are on the same VPC and same subnet, and their security group allows all traffic from the same subnet. The EMR cluster is on a different VPC, and you've configured VPC peering, route table creation, and updated your security policy . Network ACL on VPC allows all traffic. Connect to a new EMR cluster Follow the steps below to run Unravel's bootstrap script, unravel_emr_bootstrap.py , on all nodes in the cluster. The bootstrap script makes the following changes: On the master node: On Hive clusters, it updates \/etc\/hive\/conf\/hive-site.xml . On Spark clusters, it updates \/etc\/spark\/conf\/spark-defaults.conf . It updates \/etc\/hadoop\/conf\/mapred-site.xml . It updates \/etc\/hadoop\/conf\/yarn-site.xml . If Tez is installed, it updates \/etc\/tez\/conf\/tez-site.xml . It installs and starts the unravel_es daemon in \/usr\/local\/unravel_es . It installs the Spark and MapReduce sensors in \/usr\/local\/unravel-agent . It installs the Hive Hook sensor in \/usr\/lib\/hive\/lib\/ . On all other nodes: It installs the Spark and MapReduce sensors in \/usr\/local\/unravel-agent . It installs Hive sensors in \/usr\/lib\/hive\/lib . Download Unravel's bootstrap script, unravel_emr_bootstrap.py . curl https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel_emr_bootstrap.py -o \/tmp\/unravel_emr_bootstrap.py Upload the bootstrap script to an S3 bucket. Permissions needed You need write access to the S3 bucket that you want to upload the bootstrap script to. In addition, the AWS account you use to create the EMR cluster must have read access to the bootstrap script in order to execute its directives. To upload the bootstrap script to the default EMR logging bucket, s3:\/\/aws-logs- account_number - region \/elasticmapreduce , execute the following command: aws s3 cp unravel_emr_bootstrap.py s3:\/\/aws-logs- account_number - region \/elasticmapreduce In the AWS console, select the EMR service and click Create cluster . In the Create Cluster - Quick Options screen, click Go to advanced options . In Release , select emr-5.14.0 , and select all the apps you need. For transient EMR clusters, add a step to run the Unravel JAR s3:\/\/unraveldatarepo\/unravel-bootstrap-step.jar . During cluster creation, this JAR waits for the unravel_es daemon to be up and running. It logs its actions in \/tmp\/unravel\/unravel_step.log on the master node. In the Add steps (optional) section, select step type Custom JAR , and click Configure . The Add step dialog appears. In JAR location , enter s3:\/\/unraveldatarepo\/unravel-bootstrap-step.jar In Arguments , enter these two parameters: Parameter 1: Maximum number of minutes for which the step will wait for unravel_es to come up.  Default: 4. Parameter 2: Length of each interval in seconds after which the step will check for the status of unravel_es . Default: 2. Click Add . Click Next . In Step 2: Hardware , specify the following settings: Set Network and EC2 Subnet to the cluster's VPC and subnet. The security group of the subnet you specify must have access to the Unravel EC2 node. If you created the Unravel EC2 node from our CloudFormation template, then a new VPC was generated, named Unravel_VPC. This VPC comes with one configured subnet, and by default has a CIDR \/ network address block of 10.10.0.0\/16 (but you might have changed this during stack creation). If you created the Unravel EC2 node from our Amazon Machine Image (AMI), you must create the EMR cluster on the same VPC and same subnet as the Unravel EC2 node. Modify the instance type and enter the desired instance count for core (worker) nodes. Click Next . In Step 3: General Cluster Settings , specify the following settings: For more guidance on creating a new EMR cluster, see Amazon documentation . Cluster name : Enter the name of your cluster. S3 bucket : Specify the location for your log files. In Add bootstrap action , select Custom action . In the Add Bootstrap Action dialog, specify the following settings: If you create multiple bootstrap actions, make sure you specify that Unravel's bootstrap action runs last, after all other bootstrap actions have completed. Setting Values Name Custom action Script location Enter the S3 bucket that you uploaded Unravel's bootstrap script to . For example, s3:\/\/aws-logs- account_number - region \/elasticmapreduce Optional arguments You must specify these settings: --unravel-server : IP address of the Unravel EC2 instance --bootstrap : Indicates that this action must be run at cluster creation time For example, --unravel-server unravel-ec2-ip-address --bootstrap You can also specify these settings: --metrics-factor interval : Specifies the interval at which Unravel sensors push data from the EMR cluster nodes to Unravel Server. interval is in units of 5 seconds. In other words, a value of 1 means 5 seconds, 2 means 10 seconds, and so on. Default: 1 --all : Enables all sensors, including the MapReduce sensor. --disable-aa : Disables the auto action feature. --enable-am-polling : Enables \"application master\" metrics polling for auto actions. --hive-id-cache num-jobs : Maximum number of jobs you expect to have on the cluster. Default: 1000. --init : Change the initial wait time when running in bootstrap mode. Default: 300. --sensor-url : Download sensor and daemon files from external URL. (Note: The sensor is uploaded to cluster dfs \/tmp\/unravel-sensors\/ the first time being installed. This can be configured with --sensor-dfs-path parameter.) If your cluster is kerberized, the default security settings should work, but you can change them . Click Add . Click Configure and add . Select Step 4: Security and edit the hardware configuration for the cluster as follows: Choose the EC2 key pair . Select the EC2 security groups . AWS EMR service automatically applies additional rules that are required for EMR nodes. In this example, the security group picked for both Master and Core & Task nodes have rules allowing all traffic access from the Unravel EC2 node. You must choose the security group that includes the Unravel EC2 instance, otherwise bootstrapping will fail. Click Create cluster . If everything was entered correctly, your new EMR cluster should finish the bootstrap process and be in the Waiting state. Once your new EMR cluster is up and running, you can run some jobs and log into the Unravel EC2 node's web UI to see the metrics collected by the Unravel node. Connect to an existing EMR cluster To connect the Unravel EC2 instance to an existing EMR cluster, follow the steps below to run the Unravel EMR Ansible playbook either on the EMR master node or on your Mac\/Linux workstation. The following process is for existing clusters created without Unravel bootstrap. At this time, only those clusters of this type that do not have auto-scaling enabled are supported. Whenever you upgrade Unravel Server, repeat the steps below to upgrade Unravel Sensors as well. Option 1: Run our Ansible playbook on the EMR master node Before you begin Save the private key to access all the EMR nodes somewhere in the master node and change the key's permissions to read-only ( chmod 400 key ). Download unravel-emr-ansible.zip : curl https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel-emr-ansible.zip --output unravel-emr-ansible.zip\n % Total % Received % Xferd Average Speed Time Time Time Current\n Dload Upload Total Spent Left Speed\n100 11708 100 11708 0 0 66541 0 --:--:-- --:--:-- --:--:-- 66902 Unzip unravel-emr-ansible.zip : unzip unravel-emr-ansible.zip \nArchive: unravel-emr-ansible.zip\n inflating: unravel-emr-ansible\/README.md \n inflating: unravel-emr-ansible\/emr_ansible_inventory \n inflating: unravel-emr-ansible\/emr_ansible_playbook.yaml \n inflating: unravel-emr-ansible\/prepare_inventory.py \n inflating: unravel-emr-ansible\/unravel_emr_bootstrap.py Run prepare_inventory.py : Enter the following values either interactively at the prompts or through their command line options: --ssh-key path : The full pathname of the SSH private key --unravel-host hostname : The Unravel EC2 host's internal IP address. --cluster-name displayname : The EMR cluster name as you want it to display in Unravel UI. For example, python prepare_inventory.py \nPlease Enter Unravel host IP: 172.31.62.27\nPlease Enter ssh key path: \/home\/hadoop\/id_rsa\n\nAnsible Inventory updated Install Ansible on the EMR master node: sudo pip install ansible (Optional) Determine what directory Ansible was installed in, and add that directory to the $PATH variable in ~\/.bashrc , if it isn't there already. which ansible\n\/usr\/local\/bin\/ansible In ~\/.bashrc , update this line: export PATH=\/usr\/local\/bin\/:$PATH Run the Unravel Ansible playbook: $ cd unravel-emr-ansible\n$ ANSIBLE_HOST_KEY_CHECKING=false\n$ ansible-playbook -i emr_ansible_inventory emr_ansible_playbook.yaml\n \nPLAY [nodes] *******************************************************************\n \nTASK [Gathering Facts] *********************************************************\nok: [172.31.109.7]\nok: [172.31.109.251]\nok: [172.31.97.203]\n \nTASK [Run emr bootstrap script] ************************************************\nchanged: [172.31.109.7]\nchanged: [172.31.109.251]\nchanged: [172.31.97.203]\n \nTASK [Check Unravel sensor version] ********************************************\nchanged: [172.31.109.7]\nchanged: [172.31.109.251]\nchanged: [172.31.97.203]\n \nTASK [Print sensor version] ****************************************************\nok: [172.31.109.7] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\nok: [172.31.109.251] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\nok: [172.31.97.203] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\n \nPLAY RECAP *********************************************************************\n172.31.109.251 : ok=4 changed=2 unreachable=0 failed=0 \n172.31.109.7 : ok=4 changed=2 unreachable=0 failed=0 \n172.31.97.203 : ok=4 changed=2 unreachable=0 failed=0 Option 2: Run our Ansible playbook on your personal workstation (Mac or Linux only) Before you begin Save the private key to access all the EMR nodes somewhere in the master node and change the key's permissions to read-only ( chmod 400 key ). Set up AWS CLI . Make sure AWS CLI has permission to list EMR clusters: aws emr list-instances --cluster-id cluster id Download unravel-emr-ansible.zip : wget https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel-emr-ansible.zip Unzip unravel-emr-ansible.zip : unzip unravel-emr-ansible.zip \nArchive: unravel-emr-ansible.zip\n inflating: unravel-emr-ansible\/README.md \n inflating: unravel-emr-ansible\/emr_ansible_inventory \n inflating: unravel-emr-ansible\/emr_ansible_playbook.yaml \n inflating: unravel-emr-ansible\/prepare_inventory.py \n inflating: unravel-emr-ansible\/unravel_emr_bootstrap.py Run prepare_inventory.py : Enter the following values either interactively at the prompts or through their command line options: --cluster-id string : Cluster ID. Optional on EMR master, required if run outside of EMR cluster. --region string : AWS region. Optional on EMR master, required if run outside of EMR master node --inventory path : Directory containing the Ansible inventory file, emr_ansible_inventory. Default is same directory as the playbook. --ssh-key path : The full pathname of the SSH private key --ssh-user string : SSH username with sudo privilege; default is hadoop --unravel-host hostname : The Unravel EC2 host's internal IP address. --cluster-name displayname : The EMR cluster name as you want it to display in Unravel UI. --use-public : Use public IP address instead of private IP address in Ansible inventory. Include this option if you need to connect to the EMR cluster solely through its public IP address. For example, python prepare_inventory.py --use-public\nPlease Enter Unravel host IP: 172.31.62.27\nPlease Enter ssh key path: \/home\/hadoop\/id_rsa\n\nAnsible Inventory updated Install Ansible: sudo pip install ansible Run the Unravel Ansible playbook: $ cd unravel-emr-ansible\n$ ANSIBLE_HOST_KEY_CHECKING=false\n$ ansible-playbook -i emr_ansible_inventory emr_ansible_playbook.yaml\n \nPLAY [nodes] *******************************************************************\n \nTASK [Gathering Facts] *********************************************************\nok: [18.61.10.17]\nok: [18.61.10.21]\nok: [18.61.10.20]\n \nTASK [Run emr bootstrap script] ************************************************\nchanged: [18.61.10.17]\nchanged: [18.61.10.21]\nchanged: [18.61.10.20]\n \nTASK [Check Unravel sensor version] ********************************************\nchanged: [18.61.10.17]\nchanged: [18.61.10.21]\nchanged: [18.61.10.20]\n \nTASK [Print sensor version] ****************************************************\nok: [18.61.10.17] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\nok: [18.61.10.21] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\nok: [18.61.10.20] => {\n \"msg\": \"Unravel Version: 4.4.3.0b0005\"\n}\n \nPLAY RECAP *********************************************************************\n18.61.10.21 : ok=4 changed=2 unreachable=0 failed=0 \n18.61.10.17 : ok=4 changed=2 unreachable=0 failed=0 \n18.61.10.20 : ok=4 changed=2 unreachable=0 failed=0 Sanity check After you connect the Unravel EC2 instance to your EMR cluster, run some jobs on the EMR cluster and monitor the information displayed in Unravel UI ( http:\/\/unravel_ec2_node_public_IP:3000 ). Next steps For additional configuration and instrumentation options, see Next Steps. " }, 
{ "title" : "Setting up Amazon RDS (optional)", 
"url" : "102076-emr-ec2-rds.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Setting up Amazon RDS (optional)", 
"snippet" : "Unravel's default installation uses a bundled database for part of its storage. For better performance and ease of management, we recommend replacing the bundled database with an Amazon RDS instance. 2. Create an Amazon RDS instance In the Amazon RDS portal, click Create database . Select MySQL and ...", 
"body" : "Unravel's default installation uses a bundled database for part of its storage. For better performance and ease of management, we recommend replacing the bundled database with an Amazon RDS instance. 2. Create an Amazon RDS instance In the Amazon RDS portal, click Create database . Select MySQL and click Next . Select Production - MySQL . Change the following properties, and leave all others with default values. License model : generic-public-license DB engine version : 5.5.46 DB instance class : db.r3.xlarge (vCPU, 30.5 GiB RAM) Multi-AZ deployment : Create replica in different zone Storage type : Provisioned IOPS (SSD) Allocated storage : 500GB (or more depending on number of jobs and clusters the unravel node will monitor) Provisioned IOPS : 1000 Specify the database instance ID, username, and password. DB Instance identifier : unravel_mysql_prod Master username : unravel Master password : Change_Password Click Next . In the Advanced Settings page change the following settings. You can leave all other settings with default values or specify values suitable to your requirements. Network & Security Settings Virtual Private Cloud : Select the VPC that contains minimally two subsets and on the same region that you plan to deploy Unravel and the EMR cluster. Subnet group : Select the new database subnet group you created , named unravel . Public accessibility : No Availability zone : No Preference VPC security group : Select the new VPC security group you created . Click Create database . You should see the following message. 3. Connect Unravel to the RDS instance By default, the security group created for the unravel RDS has no network access granted on port 3306 on the subnet connected. You must modify the security group applied on Unravel RDS. In the Amazon RDS dashboard, locate the MySQL database endpoint. Look for the security group used for the Unravel RDS instance. Modify the inbound rule of the security group by adding a new rule to allow connections from either: The Unravel EC2 instance's security group. The subnet IP block in which the Unravel EC2 instance located. Either the security group or IP block works, provided the RDS instance is located on the same region as the VPC. On Unravel Server, verify the MySQL connection. For example, \/usr\/local\/unravel\/mysql\/bin\/mysql -h unravelmysqlprod.csfw1hkmlpgh.us-east-1.rds.amazonaws.com -u unravel -p Verify that the database unravel_mysql_prod has been created; if not create it. CREATE DATABASE IF NOT EXISTS unravel_mysql_prod; 4. Create a schema for the RDS instance Stop Unravel Server. sudo \/etc\/init.d\/unravel_all.sh stop Set or add the following properties in \/usr\/local\/unravel\/etc\/unravel.properties so that Unravel Server knows about the database. If the property isn't found, add it. Use the actual values you set in the steps above. You can use a hostname; but to avoid DNS lookups use an IP address. The database password can be encrypted . These properties define the connection to Unravel's MySQL database. Property\/Description Set by user Unit Default unravel.jdbc.username Unravel database user. Required string - unravel.jdbc.password Password for unravel.jdbc.username . Required string - unravel.jdbc.url URL for jdbc, determined by your database. Example: jdbc:mysql:\/\/127.0.0.1:3306\/unravel_mysql_prod Required string (path) - unravel.jdbc.username=unravel\nunravel.jdbc.password= unraveldata \nunravel.jdbc.url=jdbc:mysql:\/\/unravelmysqlprod.csfw1hkmlpgh.us-east-1.rds.amazonaws.com:3306\/unravel_mysql_prod Ensure the schema is up to date using the schema upgrade utility provided by Unravel Server. The script step connects to the database and applies schema deltas, in order, until the schema is up to date. The success or failure of the update is noted. sudo \/usr\/local\/unravel\/dbin\/db_schema_upgrade.sh If table creation privilege is not granted because an internal DBA support group provides the external database, request that they apply the schemas in \/usr\/local\/unravel\/sql\/mysql\/ in numerical order. The schema deltas assume the database name is already picked with a 'use' statement. The schema_migrations table keeps track of what schemas have been applied. Create the default user admin with the SQL statement emitted by \/usr\/local\/unravel\/install_bin\/db_initial_inserts.sh | \/usr\/local\/unravel\/install_bin\/db_access.sh 5. Start Unravel Server Disable the bundled database on Unravel Server. Only one of these commands is needed, depending on your exact version of Unravel. The unnecessary command produces an error which you can ignore. sudo chkconfig unravel_db off\nsudo chkconfig unravel_pg off Start Unravel services. sudo \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "1. Create an RDS security group, subnet group, and parameter group", 
"url" : "102076-emr-ec2-rds.html#UUID-8e4b187d-cea5-77cc-735b-7412edccdc4c_N1557432872793", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Setting up Amazon RDS (optional) \/ 1. Create an RDS security group, subnet group, and parameter group", 
"snippet" : "Create an RDS security group on the VPC of Unravel Server and allow access from Unravel Server security group. Create a new database subnet group. A new database subnet group is required for \"multiple availability zone\" (multi-AZ) deployment. The VPC should at least contains two subnets in at least ...", 
"body" : "Create an RDS security group on the VPC of Unravel Server and allow access from Unravel Server security group. Create a new database subnet group. A new database subnet group is required for \"multiple availability zone\" (multi-AZ) deployment. The VPC should at least contains two subnets in at least two availability zones in a given region, in the same VPC. For more information, see AWS documentation. Create a new database parameter group , based on mysql 5.5, with custom settings. Custom database parameters key_buffer_size = 268435456 \n max_allowed_packet = 33554432 \n table_open_cache = 256 \n read_buffer_size = 262144 \n read_rnd_buffer_size = 4194304 \n max_connect_errors=2000000000 \n net-read-timeout = 300 \n net-write-timeout = 600 \n open_files_limit=9000 \n innodb_open_files=9000 \n character_set_server=utf8 \n collation_server = utf8_unicode_ci \n innodb_autoextend_increment=100 \n innodb_additional_mem_pool_size = 20971520 \n innodb_log_file_size = 134217728 \n innodb_log_buffer_size = 33554432 \n innodb_flush_log_at_trx_commit = 2 \n innodb_lock_wait_timeout = 50 Required database parameters Database name: unravel_mysql_prod \nPort: 3306 \nDB parameter group: unravel " }, 
{ "title" : "Setting up VPC peering (optional)", 
"url" : "102077-emr-vpc.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Setting up VPC peering (optional)", 
"snippet" : "Follow these steps only if you have an Amazon EMR cluster located in a different VPC than the Unravel EC2 instance. This topic explains how to resolve connectivity issues when you have an Amazon EMR cluster located in a different VPC than your Unravel EC2 instance or your Unravel EC2 instance connec...", 
"body" : "Follow these steps only if you have an Amazon EMR cluster located in a different VPC than the Unravel EC2 instance. This topic explains how to resolve connectivity issues when you have an Amazon EMR cluster located in a different VPC than your Unravel EC2 instance or your Unravel EC2 instance connects to an RDS instance created on a different VPC in the same region. Assumptions The VPC where Unravel EC2 located is in the same region as the EMR cluster. The subnet used by Unravel EC2 does not overlap the IP block range of the subnet used in the EMR cluster. Network ACLs on both the VPC for Unravel EC2 and the EMR cluster are the default and allow all traffic. The security group is the only security enforcement on network access. The examples below show both the Unravel EC2 instance and the EMR cluster in the us-east-1 region but configured with different VPC and subnet. There is no network access allowed between Unravel EC2 and EMR cluster by default. Resources Internal IP address Subnet ID Subnet IP block VPC ID (name) IP block in VPC Security group ID (name) Unravel EC2 node 10.10.0.7 subnet-03b82c56b2c26dbd1 10.10.0.0\/24 vpc-0b0e17b01c4a3b54a (Unravel_VPC) 10.10.0.0\/16 sg-0e0a03084398287c9 (Unravel-EC2_SG) EMR cluster master node 10.11.0.53 subnet-0294cc17a42a9acfd 10.11.0.0\/24 vpc-c3d079a4 (VPC_for_VPC Peering) 10.11.0.0\/16 sg-0a73c3aea9340ae49 (EMR_VPC_SG) EMR cluster core nodes 10.11.0.76 10.11.0.130 subnet-0294cc17a42a9acfd 10.11.0.0\/24 vpc-c3d079a4 (VPC_for_VPC Peering) 10.11.0.0\/16 sg-0a73c3aea9340ae49 (EMR_VPC_SG) 1. Create VPC peering in VPC dashboard From the AWS console | VPC services | Peering Connections , click Create Peering Connection . Enter the name tag. For example, EMR_to_Unravel . In the VPC (Requester) field, select the VPC of the EMR cluster. In the VPC (Accepter) field, select the VPC of Unravel Server. Click Create Peering Connection . A success message should appear in the screen. Click OK . 2. Accept the VPC peering request In the VPC Dashboard , the new VPC peering connection has the status Pending Acceptance . Select this connection, click Action , and select Accept Request . Click Yes Accept in the prompt screen. You will see a message regarding \"Modify my route tables\". Click Close . 3. Create routes between peered VPC To create the routes between peered VPCs (Unravel Server on Unravel_VPC and the EMR cluster on Test_EMR_VPC): Go to VPC Dashboard | Route Tables . After locating each route table, click Edit | Add another route . Find the Unravel_VPC route table. In the Destination field, enter the IP block of the EMR VPC. For example, 10.11.0.0\/16 In the Target field , select the VPC peer connection ID. For example, pcx-0a57a978ef9a525e2 . Click Save . Find the Test_EMR_VPC route table. Set the Destination to the IP block of Unravel_VPC. For example, 10.10.0.0\/16 . In the Target field, select the VPC peer connection ID. For example, pcx-0a57a978ef9a525e2 . Click Save . In the Target field, select the connection ID. For example, pcx-0a57a978ef9a525e2 . Click Save . 4. Update security groups Go to VPC Dashboard | Security Group . After locating each security group: Click Add another rule . Set Type to inbound ALL traffic and Protocol to ALL . Locate the security group used on Unravel EC2 node. Enter the EMR VPC IP block in the Source field. For example, 10.11.0.0\/16 . Click Save . Locate the security group used on EMR cluster node and enter the Unravel VPC IP block. For example, 10.10.0.0\/16 . Click Save . 5. Verify the connection between Unravel and the EMR master node Open SSH sessions to both Unravel EC2 nodeand EMR master node. Since the above example allows all traffic from both VPC IP blocks, you should be able to ping the IP address of EMR master node from Unravel Server. On the Unravel EC2 instance, open a telnet session to the EMR master node port 8082 (the namenode port). On the EMR master node, open telnet sessions to the Unravel EC2 instnace, ports 3000 and 4043. If telnet port tests are positive, the VPC peering connection is setup correctly. If not, troubleshoot the configuration on network ACL, security groups, and route tables used on both VPCs . " }, 
{ "title" : "Adding instrumentation for a new node", 
"url" : "102078-emr-ec2-new-node.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Adding instrumentation for a new node", 
"snippet" : "When a new node is added to your EMR cluster and that node was bootstrapped during creation, its bootstrap script runs on it automatically. To make sure that Unravel can communicate with the new node, do the following: Download the Unravel EMR bootstrap script from https:\/\/s3.amazonaws.com\/unravelda...", 
"body" : "When a new node is added to your EMR cluster and that node was bootstrapped during creation, its bootstrap script runs on it automatically. To make sure that Unravel can communicate with the new node, do the following: Download the Unravel EMR bootstrap script from https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel_emr_bootstrap.py to the new node. sudo curl https:\/\/s3.amazonaws.com\/unraveldatarepo\/unravel_emr_bootstrap.py -o \/tmp\/unravel_emr_bootstrap.py Manually run the bootstrap script on the new EMR node by providing the Unravel Server's private IP address and the EMR cluster ID. sudo python \/tmp\/unravel_emr_bootstrap.py --unravel-server unravel-host-private-ip --cluster-id emr-cluster-id " }, 
{ "title" : "Testing and troubleshooting", 
"url" : "102079-emr-ec2-test-troubleshoot.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Testing and troubleshooting", 
"snippet" : "This is an excerpt of the User Guide . For more details, see the full document. Testing the deployment Connect to the Unravel UI through an SSH tunnel. Create sn SSH tunnel to port 3000 on the Unravel EC2 instance. For example, ssh -i ssh_key.pem centos@ unravel-ec2-ip -L 3000:127.0.0.1:3000 Start y...", 
"body" : "This is an excerpt of the User Guide . For more details, see the full document. Testing the deployment Connect to the Unravel UI through an SSH tunnel. Create sn SSH tunnel to port 3000 on the Unravel EC2 instance. For example, ssh -i ssh_key.pem centos@ unravel-ec2-ip -L 3000:127.0.0.1:3000 Start your browser from your workstation and navigate to http:\/\/127.0.0.1:3000 . Log in with username admin and password unraveldata The OPERATIONS tab appears. Trial versions include a message in the top menu bar about the trial license and the number of days remaining until it expires. To extend your trial period or find out more about our licensing, contact us . Run sample jobs from the EMR master node. The EMR master node has sample MapReduce and Spark jobs on it. Run these jobs to verify that the Unravel EC2 node is collecting data from the EMR cluster. Your usage may vary depending on what applications you installed on your cluster. Sample MapReduce job Connect to the EMR master node via SSH: ssh -i ssh_key.pem ec2-user@ EMR-master-public-IP Run this MapReduce \"Pi\" job: sudo -u hdfs hadoop-mapreduce-examples pi 100 100 When the MapReduce job finishes, check Unravel UI. You should see one successful application labeled MR on the dashboard. To see details about the MapReduce job, click the APPLICATIONS tab, and expand the MR job. The job's details are displayed. Click the orange bar that notifies you that Unravel has recommendation(s) for tuning this job. Explore other metrics about this job by clicking the tabs within the job's details. Sample Spark job Connect to the EMR master node via SSH: ssh -i ssh_key.pem ec2-user@EMR_master_public_IP Run this Spark \"Pi\" job: sudo -u hdfs spark-submit --class org.apache.spark.examples.SparkPi --master yarn --num-executors 1 --driver-memory 512m --executor-memory 512m --executor-cores 1 \/usr\/lib\/spark\/examples\/jars\/spark-examples.jar 1000 When the Spark job finishes, check Unravel UI: You should see one successful application labeled SPARK on the dashboard. To see details about the Spark job, click the APPLICATIONS tab, and expand the Spark job. The job's details are displayed. Click the orange bar that notifies you that there Unravel has recommendation(s) for tuning this job. Explore other metrics about this job by clicking the tabs within the job details screen. Sample Tez job Copy \/usr\/local\/unravel\/install_bin\/hive_test_simple.sh from the Unravel host to the EMR master node. Run \/usr\/local\/unravel\/install_bin\/hive_test_simple.sh (where hive.execution.engine=tez ) Check Unravel UI for Tez data. For instructions, see Tez Application Manager. Sending diagnostics to Unravel Support In the upper right corner of Unravel UI, click the pull-down menu, and select Manage . Wait for the page to fully load. Select the Diagnostics tab. Click Send Diagnostics to Unravel Support . This sends an email message with a diagnostics report to Unravel Support and also to the users listed in the com.unraveldata.login.admins property. If you don't have access to push the bundle through Unravel UI: On the Unravel host, bundle the diagnostic information. \/usr\/local\/unravel\/install_bin\/diag_dump.sh Log into Unravel Support and upload the bundle. Reconnecting to your EMR cluster If you used our CloudFormation template to create your Unravel EC2 instance, it's protected by ASG, which sets the target\/maximum number of instances at 1. In the rare scenario of your EC2 instance failing, ASG will recreate it with the same configuration, and restore its prior history from a backup saved in an S3 bucket. In this case, your existing EMR clusters just need to be reconnected to the newly created Unravel EC2 instance as described in . Diagnosing Oozie errors You may see this common error: org.apache.oozie.action.ActionExecutorException: JA010: Property [fs.default.name] not allowed in action [job-xml] configuration This might indicate that you have an older version of a configuration file which contains some deprecated properties. The workaround is to comment-out the <job-xml> element in workflow.xml . Adjusting the historical data in Unravel UI Settings are needed to adjust the time horizon. In this example, it is set to 2 years with recent data showing the max amount minus 2 weeks: In Unravel UI, navigate to Manage | Core , and scroll down to the Retention section. Under TIME SERIES RETENTION DAYS , adjust the number of days to retain data. This corresponds to com.unraveldata.retention.max.days in \/usr\/local\/unravel\/etc\/unravel.properties . In \/usr\/local\/unravel\/etc\/unravel.properties , set com.unraveldata.history.maxSize.weeks=104 Restart all services. sudo \/etc\/init.d\/unravel_all.sh restart Checking Ansible playbook logs Check Ansible playbook logs in \/tmp\/unravel\/unravel_sensor_ansible.log . If the EMR cluster is created in a different VPC, configure VPC peering . " }, 
{ "title" : "Operational guidance", 
"url" : "102080-emr-ec2-ops-guidance.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Operational guidance", 
"snippet" : "Health check You can monitor Unravel Server through CloudWatch by creating alerts for the specific EC2 instance that hosts Unravel Server. You can see the status of various Unravel components in the Unravel UI: Navigate to the drop-down in the top right corner of Unravel UI and select Manage | Daemo...", 
"body" : "Health check You can monitor Unravel Server through CloudWatch by creating alerts for the specific EC2 instance that hosts Unravel Server. You can see the status of various Unravel components in the Unravel UI: Navigate to the drop-down in the top right corner of Unravel UI and select Manage | Daemons . This displays details similar to the screenshots below. If you've set up Unravel to use RDS as its database, and you want to monitor RDS storage capacity, see for details on how to set up a CloudWatch alarm for this. Your Unravel EC2 instance must be in the same region as the target EMR clusters it will be monitoring so that, region disruption does not apply to Unravel. Backup and recovery Best practice is to prepare in advance for disaster like instance failure by backing up and restoring your deployment. The Unravel EC2 instance must be in the same region as the target EMR clusters it's monitoring. So, no region recovery is needed. Unravel is designed to maintain business continuity but doesn't support high availability (HA). Routine maintenance There are multiple means by which Unravel announces and documents details of availability of new versions: In the . newsletters Unravel Solution Engineers and Account Management Team members engage with customers directly to tell them about the availability of upgrades and patches Blogs published on the Unravel website Through the Unravel Newsletter (sign up on the Unravel website ) Emergency maintenance In the event of fault conditions, such as a transient failure of an AWS Service such that the availability of EC2 in a particular availability zone (AZ) is degraded, or a more permanent failure of an AWS service such that EC2 instance has faulted, or an EC2 Scheduled Maintenance Event is received, the best course of action to take in such situations is to have already made a backup of the system. Unravel is designed to maintain business continuity but doesn't support High Availability (HA). Support To contact Unravel Support, visit Unravel Support . For details on support tiers, SLA, and so on, see Unravel's support policy . Support costs Currently, there are no additional costs for obtaining support. " }, 
{ "title" : "Deleting the Unravel EC2 instance", 
"url" : "102081-emr-ec2-delete.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Deleting the Unravel EC2 instance", 
"snippet" : "From your EC2 console , select the Unravel EC2 instance. In the Actions pull-down menu, select Delete Stack . Click Yes, Delete . Monitor the Status column to make sure the deletion is complete. If deletion fails, use the EC2 console (menu on left) to disable auto scaling, otherwise Amazon will keep...", 
"body" : "From your EC2 console , select the Unravel EC2 instance. In the Actions pull-down menu, select Delete Stack . Click Yes, Delete . Monitor the Status column to make sure the deletion is complete. If deletion fails, use the EC2 console (menu on left) to disable auto scaling, otherwise Amazon will keep re-spawning the EC2 instance. " }, 
{ "title" : "Deploying Unravel from the AWS Marketplace", 
"url" : "102082-emr-marketplace.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Deploying Unravel from the AWS Marketplace", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Overview", 
"url" : "102082-emr-marketplace.html#UUID-6b30e33a-a4cc-53b2-8df9-0045f4cadb30_UUID-16b8b228-d92d-d89d-46fb-801e3cea894d", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Deploying Unravel from the AWS Marketplace \/ Overview", 
"snippet" : "This page describes how to launch an Unravel Server from the AWS Marketplace and connect EMR clusters to it. Use these instructions for test or development environments only; for production environments, see . Unravel for Amazon EMR is the only solution to provide full-stack monitoring, tuning, trou...", 
"body" : "This page describes how to launch an Unravel Server from the AWS Marketplace and connect EMR clusters to it. Use these instructions for test or development environments only; for production environments, see . Unravel for Amazon EMR is the only solution to provide full-stack monitoring, tuning, troubleshooting and resource optimization for big data running on Amazon EMR. Unravel goes beyond passive monitoring to highly automated and intelligent management and optimization of data pipelines and applications. This topic helps you set up the below configuration. We take you through the following steps, to get Unravel for Amazon EMR up and running via the AWS Marketplace. " }, 
{ "title" : "Background", 
"url" : "102082-emr-marketplace.html#UUID-6b30e33a-a4cc-53b2-8df9-0045f4cadb30_UUID-c8a9e8e2-701a-a683-8a41-b506bce3531d", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Deploying Unravel from the AWS Marketplace \/ Background", 
"snippet" : "A few things to note before getting started... For Unravel Server to be able to monitor your EMR cluster(s), you need to: Allow Unravel Server to access EMR cluster(s) nodes' TCP ports 8020, 50010, 50020. Open Unravel Server's port 4043 for receiving traffic from the EMR cluster(s). To access Unrave...", 
"body" : "A few things to note before getting started... For Unravel Server to be able to monitor your EMR cluster(s), you need to: Allow Unravel Server to access EMR cluster(s) nodes' TCP ports 8020, 50010, 50020. Open Unravel Server's port 4043 for receiving traffic from the EMR cluster(s). To access Unravel UI, you need to open port 3000 on Unravel Server for HTTP access. No action is needed right now. We'll walk you through these actions later. " }, 
{ "title" : "Step 0: Meet the network prerequisites", 
"url" : "102082-emr-marketplace.html#UUID-6b30e33a-a4cc-53b2-8df9-0045f4cadb30_UUID-07d65881-01db-81fd-3155-f68f396467e5", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Deploying Unravel from the AWS Marketplace \/ Step 0: Meet the network prerequisites", 
"snippet" : "You can choose your own VPC\/subnet settings\/security groups as long as you can achieve the requirements mentioned in . In this case, you would also have appropriate VPN connections or NAT Gateways available for this VPC do you can connect to the Unravel Server via the internal\/external IP address as...", 
"body" : "You can choose your own VPC\/subnet settings\/security groups as long as you can achieve the requirements mentioned in . In this case, you would also have appropriate VPN connections or NAT Gateways available for this VPC do you can connect to the Unravel Server via the internal\/external IP address as applicable. OR You can create a new one to quickly check out Unravel for EMR as described below. For ease of use, we are creating a VPC with a Single Public Subnet . Log into the AWS VPC Dashboard . Click Launch VPC Wizard . Select the first option, Select a VPC Configuration . Add the names for the VPC and subnet and click Create VPC to create the VPC and related subcomponents. On the AWS VPC Dashboard , click Subnets on the left-hand menu and select the subnet just created. Click the Actions | Modify auto-assign IP settings . Select the Enable auto-assign public IPv4 address checkbox, and click Save . " }, 
{ "title" : "Step 1: Launch and set up Unravel Server", 
"url" : "102082-emr-marketplace.html#UUID-6b30e33a-a4cc-53b2-8df9-0045f4cadb30_UUID-3f651f1a-0175-53ee-d69b-fb78d4d0af82", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Deploying Unravel from the AWS Marketplace \/ Step 1: Launch and set up Unravel Server", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Step 1 (a): Find Unravel for Amazon EMR from the Marketplace", 
"url" : "102082-emr-marketplace.html#UUID-6b30e33a-a4cc-53b2-8df9-0045f4cadb30_UUID-ff95fa6b-9a0a-6fda-e700-82db05cc9719", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Deploying Unravel from the AWS Marketplace \/ Step 1: Launch and set up Unravel Server \/ Step 1 (a): Find Unravel for Amazon EMR from the Marketplace", 
"snippet" : "Search for Unravel for Amazon EMR in the AWS Marketplace . On the listing page, Click Continue to Subscribe . On the subscription page, click Continue to Configuration . Choose a version and location: On the Configure this software page, select the latest software version and the same region where y...", 
"body" : "Search for Unravel for Amazon EMR in the AWS Marketplace . On the listing page, Click Continue to Subscribe . On the subscription page, click Continue to Configuration . Choose a version and location: On the Configure this software page, select the latest software version and the same region where you plan to create EMR clusters (Unravel Server can reside in a different region, but then you'll need to set up VPC peering . Click Continue to Launch to continue with the setup. Note that there are several steps to be done next before the instance is actually launched. Launch the software: On the Launch this software page, select Launch from Website from the dropdown and then click Launch . Note that some more steps remain before the instance is actually launched. " }, 
{ "title" : "Step 1 (b): Choose an instance type", 
"url" : "102082-emr-marketplace.html#UUID-6b30e33a-a4cc-53b2-8df9-0045f4cadb30_UUID-e10f1dba-c760-7999-66c3-461de8d02fd9", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Deploying Unravel from the AWS Marketplace \/ Step 1: Launch and set up Unravel Server \/ Step 1 (b): Choose an instance type", 
"snippet" : "The instance type you choose depends on the number of applications that will be monitored on a daily basis and other factors . You can leave the default value as is....", 
"body" : "The instance type you choose depends on the number of applications that will be monitored on a daily basis and other factors . You can leave the default value as is. " }, 
{ "title" : "Step 1 (c): Configure VPC and subnet settings", 
"url" : "102082-emr-marketplace.html#UUID-6b30e33a-a4cc-53b2-8df9-0045f4cadb30_UUID-1a944af1-44b9-fcfb-da00-20f4124713d6", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Deploying Unravel from the AWS Marketplace \/ Step 1: Launch and set up Unravel Server \/ Step 1 (c): Configure VPC and subnet settings", 
"snippet" : "Choose the VPC and Subnet that you created or a suitable one as discussed in ....", 
"body" : "Choose the VPC and Subnet that you created or a suitable one as discussed in . " }, 
{ "title" : "Step 1 (d): Configure security group settings", 
"url" : "102082-emr-marketplace.html#UUID-6b30e33a-a4cc-53b2-8df9-0045f4cadb30_UUID-16e8e282-7472-9fef-dbd6-248ea349469c", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Deploying Unravel from the AWS Marketplace \/ Step 1: Launch and set up Unravel Server \/ Step 1 (d): Configure security group settings", 
"snippet" : "Click Create New Based On Seller Settings . Name the security group anything you like and make a note of its name. Note: Unravel recommends limiting access to only known IP addresses. For example, Unravel for Amazon EMR-1 . Enter any description you like. Click Save . For enhanced security, you can ...", 
"body" : "Click Create New Based On Seller Settings . Name the security group anything you like and make a note of its name. Note: Unravel recommends limiting access to only known IP addresses. For example, Unravel for Amazon EMR-1 . Enter any description you like. Click Save . For enhanced security, you can change the rules above based on your setup and Unravel's requirements . " }, 
{ "title" : "Step 1 (e): Configure key pair settings", 
"url" : "102082-emr-marketplace.html#UUID-6b30e33a-a4cc-53b2-8df9-0045f4cadb30_UUID-af365288-5b96-260e-8c51-42e851f0fbdb", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Deploying Unravel from the AWS Marketplace \/ Step 1: Launch and set up Unravel Server \/ Step 1 (e): Configure key pair settings", 
"snippet" : "Choose an existing key pair that connects to this instance or create a new one, and then click Launch ....", 
"body" : "Choose an existing key pair that connects to this instance or create a new one, and then click Launch . " }, 
{ "title" : "Step 1 (f): Log into Unravel UI", 
"url" : "102082-emr-marketplace.html#UUID-6b30e33a-a4cc-53b2-8df9-0045f4cadb30_UUID-ae6aa636-4b8e-1e60-9740-b9cf780f5731", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Deploying Unravel from the AWS Marketplace \/ Step 1: Launch and set up Unravel Server \/ Step 1 (f): Log into Unravel UI", 
"snippet" : "Once the AMI has instantiated and Unravel is installed, which can take 10-15 minutes, log into Unravel UI. Get the hostname or IP of the instance from the AWS Console: Navigate to http:\/\/ hostname-or-ip-address-of-the-instance :3000 with a web browser. Log in with username admin and password unravel...", 
"body" : "Once the AMI has instantiated and Unravel is installed, which can take 10-15 minutes, log into Unravel UI. Get the hostname or IP of the instance from the AWS Console: Navigate to http:\/\/ hostname-or-ip-address-of-the-instance :3000 with a web browser. Log in with username admin and password unraveldata . Next, let’s connect a new EMR cluster to Unravel Server. " }, 
{ "title" : "Step 2: Connect a new EMR cluster to Unravel Server", 
"url" : "102082-emr-marketplace.html#UUID-6b30e33a-a4cc-53b2-8df9-0045f4cadb30_UUID-2f0dd017-9e06-2536-258a-15a989a85fc9", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Deploying Unravel from the AWS Marketplace \/ Step 2: Connect a new EMR cluster to Unravel Server", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Step 2 (a): Create a new EMR cluster and connect Unravel", 
"url" : "102082-emr-marketplace.html#UUID-6b30e33a-a4cc-53b2-8df9-0045f4cadb30_UUID-99c12480-8cc1-7a80-1a8a-51f4022dcd54", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Deploying Unravel from the AWS Marketplace \/ Step 2: Connect a new EMR cluster to Unravel Server \/ Step 2 (a): Create a new EMR cluster and connect Unravel", 
"snippet" : "Go to AWS EMR Dashboard and click Create Cluster . Click Go to advanced options . Select the release and the services you want to install. Click Next . Select the same VPC and subnet as the one chosen for Unravel Server (Unravel Server can reside in a different VPC, but then you would need to set up...", 
"body" : "Go to AWS EMR Dashboard and click Create Cluster . Click Go to advanced options . Select the release and the services you want to install. Click Next . Select the same VPC and subnet as the one chosen for Unravel Server (Unravel Server can reside in a different VPC, but then you would need to set up VPC peering ). Click Next . Add a bootstrap action. In this section, go the Bootstrap Actions section, select Custom Actions , and click Configure and Add . That brings up a dialog box like this: In the Script location text box, enter s3:\/\/unraveldatarepo\/unravel_emr_bootstrap.py In the Optional Arguments text box, do the following: Make a note of the private IP from your Unravel instance: Add --unravel-server unravel-ec2-private-ip-address --bootstrap Click Save in the dialog box, and then click Next . In the next screen, as shown below, select a key pair to be able to connect to the EC2 nodes and also make a note of the names of the security groups encircled below, as you will modify them. Click Create Cluster . " }, 
{ "title" : "Step 2 (b): Modify EMR cluster security groups so that Unravel Server has adequate access", 
"url" : "102082-emr-marketplace.html#UUID-6b30e33a-a4cc-53b2-8df9-0045f4cadb30_UUID-5cf973a1-61b7-6b4c-92df-5822ba7157af", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Deploying Unravel from the AWS Marketplace \/ Step 2: Connect a new EMR cluster to Unravel Server \/ Step 2 (b): Modify EMR cluster security groups so that Unravel Server has adequate access", 
"snippet" : "To find the security group ID of the security group named Unravel for Amazon EMR-1 , navigate to Security Groups from the left panel in your AWS console and search for Unravel for Amazon EMR-1 . Make a note of the group ID. For example, in the screenshot above, the group ID is sg-0564b1b8902ecf611 ....", 
"body" : "To find the security group ID of the security group named Unravel for Amazon EMR-1 , navigate to Security Groups from the left panel in your AWS console and search for Unravel for Amazon EMR-1 . Make a note of the group ID. For example, in the screenshot above, the group ID is sg-0564b1b8902ecf611 . From the EMR cluster’s screen (where it shows its status in the creation process), navigate to each of the two security groups highlighted in the screenshot below. You only need to change these security groups once. Click the group, select one, click the Inbound tab, and click Edit . Click Add Rule . Add three rules as follows and then Save (as shown in the screenshot below): Type = Custom TCP , Protocol = TCP , Port Range = 8020 , Source = security-group-ID-of-Unravel-Server’s-security-group Type = Custom TCP , Protocol = TCP , Port Range = 50010 , Source = security-group-ID-of-Unravel-Server’s-security-group Type = Custom TCP , Protocol = TCP , Port Range = 50020 , Source = security-group-ID-of-Unravel-Server’s-security-group Make the above changes to both the security groups corresponding to the EMR cluster. In order to connect to an existing EMR cluster (instead of a new one) and\/or for more advanced options, see . " }, 
{ "title" : "Step 3: Check out the Unravel UI with your EMR cluster connected", 
"url" : "102082-emr-marketplace.html#UUID-6b30e33a-a4cc-53b2-8df9-0045f4cadb30_UUID-44e53284-3302-2da3-0298-c3f6d2aee8f0", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Elastic MapReduce (EMR) \/ Deploying Unravel from the AWS Marketplace \/ Step 3: Check out the Unravel UI with your EMR cluster connected", 
"snippet" : "Once the cluster is created, and you go back to the Unravel UI ( http:\/\/ hostname-or-ip-address-of-the-instance :3000 ), within a few seconds, you should see the cluster ID in the main dashboard dropdown on the right and some data in various charts. You can now run applications in your cluster and u...", 
"body" : "Once the cluster is created, and you go back to the Unravel UI ( http:\/\/ hostname-or-ip-address-of-the-instance :3000 ), within a few seconds, you should see the cluster ID in the main dashboard dropdown on the right and some data in various charts. You can now run applications in your cluster and use Unravel to optimize them. Applications you run will show up in the Applications tab. Next Steps See Unravel Product Documentation – User Guide to learn more about how to use Unravel. Run Unravel Spark benchmark scripts (specifically Benchmarks 2.0.x) to see the value for Unravel Insights and Recommendations. Try the full Unravel product free for 30 days. To get help for setting up or using Unravel for AWS EMR please contact us as at Unravel AWS Markertpcee help . " }, 
{ "title" : "Amazon Athena (preview only)", 
"url" : "102085-install-athena.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Athena (preview only)", 
"snippet" : "Amazon Athena is a serverless query service that enables you to interact with data directly in place on AWS S3 using ANSI standard SQL. You pay only for the queries you run, based on how much data the queries scan. Failed queries cost $0. For cancelled or killed queries, you're charged only for the ...", 
"body" : "Amazon Athena is a serverless query service that enables you to interact with data directly in place on AWS S3 using ANSI standard SQL. You pay only for the queries you run, based on how much data the queries scan. Failed queries cost $0. For cancelled or killed queries, you're charged only for the data that was scanned before the queries were cancelled. For more information on Athena pricing, see Amazon's Athena pricing . Since you’re charged per scan per query, you can use Unravel to show you the cost per Athena query. This feature is in beta\/preview mode. Currently, Unravel UI doesn't display insights and recommendations on Athena queries. Preview features are in beta and are subject to change. The design and code is less mature than official GA features and is being provided as-is with no warranties. Preview features are not subject to the support SLA of official GA features. We do not recommend you deploy Preview features in a production environment. This feature is available only in releases that include updates to our Amazon EMR support , such as 4.5.0.5. compmatrix-platform Use cases Amazon Athena is well suited to structured data such as logs. You send Unravel information about your Athena queries through an AWS Lambda function which monitors your AWS CloudTrail trail for Athena events. Follow these steps to connect your Athena queries to Unravel through an AWS Lambda function. These steps assume you already have Athena queries set up. In summary, we'll walk you through how to: Create a trail in AWS CloudTrail for management read\/write events. Create a new AWS role to allow AWS Lambda functions to call AWS services on your behalf. Create an AWS Lambda function that sends data to Unravel whenever your trail has a new entry. View Athena queries in Unravel UI. For help with Athena, see https:\/\/aws.amazon.com\/athena\/ . " }, 
{ "title" : "1. Create a trail in AWS CloudTrail", 
"url" : "102085-install-athena.html#UUID-ca6d470f-8ef2-086f-ae72-ad20cd8be9f7_id_AmazonAthena-CreateaTrailinAWSCloudTrail", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Athena (preview only) \/ 1. Create a trail in AWS CloudTrail", 
"snippet" : "You can capture Athena activity by creating a specific CloudTrail trail for management read\/write events, and specifying a new or existing S3 bucket to store the trail. Your AWS account must have the following permissions for these steps: AWSCloudTrailReadOnlyAccess CloudtrailFullAccess Log into you...", 
"body" : "You can capture Athena activity by creating a specific CloudTrail trail for management read\/write events, and specifying a new or existing S3 bucket to store the trail. Your AWS account must have the following permissions for these steps: AWSCloudTrailReadOnlyAccess CloudtrailFullAccess Log into your AWS console at https:\/\/console.aws.amazon.com . In the AWS console, select CloudTrail . On the CloudTrail page, click Trails | Create trail . In the Trail name field, type Unravel In the Apply trail to all regions section, select Yes . In the Management events section, next to Read\/Write events , select All . In the Data events section, don’t make any changes. This trail doesn’t need to log any data events. In the Storage location section, specify where you want AWS to store your new trail. You can create a new S3 bucket or use an existing S3 bucket. If you create a new bucket: Set the S3 bucket name to unravel-cloudtrail Expand the Advanced section. Leave the Log file prefix field blank. For Encrypt log files with SSE-KMS , select No . For Enable log file validation , select Yes . For Send SNS notification for every log file delivery , select No . Click Create . Configure CloudWatch permissions on unravel-cloudtrail : Click your newly created trail, unravel-cloudtrail , and scroll down to CloudWatch Logs . Click Configure . In the New or existing log group field, type CloudTrail\/UnravelLogGroup Click Continue . On the next page, expand View Details , and specify the following: IAM Role : Create a new IAM Role . Role Name : unravel-cloudtrail-role Click Allow . The configuration summary for this trail appears, and in the upper right corner the logging status is displayed. " }, 
{ "title" : "2. Create a role for Unravel's AWS Lambda function", 
"url" : "102085-install-athena.html#UUID-ca6d470f-8ef2-086f-ae72-ad20cd8be9f7_id_AmazonAthena-CreateaRoleforUnravelsAWSLambdaFunction", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Athena (preview only) \/ 2. Create a role for Unravel's AWS Lambda function", 
"snippet" : "Unravel provides an AWS Lambda function to forward your CloudTrail trail to Unravel. To connect Unravel’s AWS Lambda function with your trail, you first need to create an AWS role for Unravel’s Lambda function to use, if you don’t have one already. For more information on AWS Lambda, see Using AWS L...", 
"body" : "Unravel provides an AWS Lambda function to forward your CloudTrail trail to Unravel. To connect Unravel’s AWS Lambda function with your trail, you first need to create an AWS role for Unravel’s Lambda function to use, if you don’t have one already. For more information on AWS Lambda, see Using AWS Lambda with AWS CloudTrail . Log into your AWS console at https:\/\/console.aws.amazon.com . In the AWS console, select IAM . On the IAM page, click Roles . Click Create role . In the Select type of trusted entity , choose AWS service . In the Choose the service that will use this role section, select Lambda . Click Next: Permissions . On the Attach permissions policies page, type each of the following policies into the search box and select the checkbox next to it: AmazonS3ReadOnlyAccess AWSLambdaVPCAccessExecutionRole Click Next: Tags . (Optional) If you want to add tags to this role, add them here. Click Next: Review . On the Review page, set Role name to unravel-athena-lambda-role Click Create role . The AWS console displays a message indicating that it created the role. Select the role in the list of roles. On the role summary page, select the Trust relationships tab to verify which trusted entities that can assume this role. " }, 
{ "title" : "3. Create Unravel's AWS Lambda function", 
"url" : "102085-install-athena.html#UUID-ca6d470f-8ef2-086f-ae72-ad20cd8be9f7_id_AmazonAthena-CreateUnravelsAWSLambdaFunction", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Athena (preview only) \/ 3. Create Unravel's AWS Lambda function", 
"snippet" : "This section explains how to create an AWS Lambda function that sends data to Unravel whenever your trail has a new entry. Your AWS account must have the following permission for these steps: AWSLambdaFullAccess Define basic settings for the Lambda function Log into your AWS console at https:\/\/conso...", 
"body" : "This section explains how to create an AWS Lambda function that sends data to Unravel whenever your trail has a new entry. Your AWS account must have the following permission for these steps: AWSLambdaFullAccess Define basic settings for the Lambda function Log into your AWS console at https:\/\/console.aws.amazon.com In the AWS console, select Lambda . On the Lambda page, click Create function . On the Create function page, enter the following: Function name : UnravelAthenaLambda Runtime : Python 2.7 Execution role : Use an existing role Existing role : unravel-athena-lambda-role Click Create function . AWS displays a banner indicating success, and displays your new Lambda function’s page. Add a trigger to the Lambda function From the list of triggers on the left, select S3 . In the Configure triggers section, enter the following: Bucket : unravel-cloudtrail Event type : All object create events Select the Enable trigger checkbox. Click Add . AWS shows the new S3 trigger at the bottom of the page. At the top of the page, click Save . Add code to Unravel’s AWS Lambda function Select the new Lambda function: AWS displays configurable settings for this function. In the Function code section, enter the following: Code entry type : Upload a file Amazon S3 Amazon S3 link URL: s3:\/\/unraveldatarepo\/share\/lambda\/UnravelAthenaLambda.zip Runtime : Python 2.7 In the Environment variables section, enter the following key-value pair: Key: unravel_lr_url Value: http:\/\/ private-IP-of-Unravel-Node : Port \/logs\/athena\/j-default\/athena\/athena Where: private-IP-of-Unravel-Node is the private IP address of your Unravel Server, and Port is 4043 unless 4043 is already in use (in which case, contact ). In the Execution role section, enter the following: Select Use an existing role . Existing role: unravel-athena-lambda-role In the Network section, specify your virtual private cloud (VPC) information: Don’t select No VPC . Select your VPC. Select at least two subnets from the pull-down list (hold CTRL to select multiple subnets). Select your private security group (SG). Review the inbound and outbound rules. At the top of the page, click Test . At the top of the page, click Save . AWS displays a banner indicating success. " }, 
{ "title" : "4. View Athena queries in Unravel UI", 
"url" : "102085-install-athena.html#UUID-ca6d470f-8ef2-086f-ae72-ad20cd8be9f7_id_AmazonAthena-ViewAthenaQueriesinUnravelUI", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Athena (preview only) \/ 4. View Athena queries in Unravel UI", 
"snippet" : "In Unravel UI, look at Athena | Apps ....", 
"body" : "In Unravel UI, look at Athena | Apps . " }, 
{ "title" : "Resources", 
"url" : "102085-install-athena.html#UUID-ca6d470f-8ef2-086f-ae72-ad20cd8be9f7_id_AmazonAthena-Resources", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Athena (preview only) \/ Resources", 
"snippet" : "Create a Lambda Function with the Console AWS Lambda Permissions Examine Athena requests using CloudTrail Logs...", 
"body" : "Create a Lambda Function with the Console AWS Lambda Permissions Examine Athena requests using CloudTrail Logs " }, 
{ "title" : "Creating private subnets for Unravel's Lambda function", 
"url" : "102086-athena-lambda-vpc.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Amazon Athena (preview only) \/ Creating private subnets for Unravel's Lambda function", 
"snippet" : "To ensure that Unravel's Lambda Function can access Unravel Node port 4043 and a specific S3 bucket, you might need to create private subnets by following these steps. Step 1: Create private subnets In the AWS VPC dashboard, click Create subnet . Create two subnets within the same VPC Unravel Server...", 
"body" : "To ensure that Unravel's Lambda Function can access Unravel Node port 4043 and a specific S3 bucket, you might need to create private subnets by following these steps. Step 1: Create private subnets In the AWS VPC dashboard, click Create subnet . Create two subnets within the same VPC Unravel Server is located. IPv4 CIDR block is the block of IP addresses that you're assigning to this subnet. This value can be different based on your environment. For example, 172.31.64.0\/24 means IP addresses between 172.31.64.0 and 172.31.64.255 are assigned to this subnet. Create a route table Create a route table for the two private subnets. In VPC field, specify the VPC ID that Unravel Server instance is using. Associate the two subnets with this route table. Create a NAT gateway On the VPC Dashboard , click Create NAT Gateway . Attach the NAT gateway to a public subnet with an elastic IP: Subnet is the public subnet within Unravel VPC. Elastic IP ID is the elastic IP (EIP). If there is no available EIP, click Create New EIP . Add the NAT gateway to the route tables Select Unravel Lambda Route Table . Select the Routes tab. Click Edit routes . Add the route gateway. Click Save routes . References " }, 
{ "title" : "Google Dataproc", 
"url" : "102087-install-dataproc.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Google Dataproc", 
"snippet" : "This topic explains how to deploy Unravel on Dataproc. Deploying Unravel takes less than an hour in most environments. In some environments, deployment takes longer due to the complexity of security\/VPC settings, various permissions' setup, and so on. Supported clusters Cluster with respect to Unrav...", 
"body" : "This topic explains how to deploy Unravel on Dataproc. Deploying Unravel takes less than an hour in most environments. In some environments, deployment takes longer due to the complexity of security\/VPC settings, various permissions' setup, and so on. Supported clusters Cluster with respect to Unravel setup Supported New cluster without auto-scaling ✓ New cluster with auto-scaling ✓ Existing cluster without auto-scaling ✓ Existing cluster with auto-scaling ✖️ " }, 
{ "title" : "Prerequisites", 
"url" : "102088-dataproc-pre.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Google Dataproc \/ Prerequisites", 
"snippet" : "Platform Each version of Unravel has specific platform requirements. Check Unravel's compatibility matrix to confirm your Dataproc platform meets the requirements for the Unravel version you're installing. EMR Hardware Compute Engine GCE type: General-purpose: Minimum: n2-standard-16 \/ n1-standard-1...", 
"body" : "Platform Each version of Unravel has specific platform requirements. Check Unravel's compatibility matrix to confirm your Dataproc platform meets the requirements for the Unravel version you're installing. EMR Hardware Compute Engine GCE type: General-purpose: Minimum: n2-standard-16 \/ n1-standard-16 (64 GiB RAM) Maximum: n2-standard-64 \/ n1-standard-64(256 GiB RAM) Recommended: n2-standard-32 \/ n1-standard-32 (128 GiB RAM) Virtualization type: HVM Root device type: Standard Persistent Disk \/ SSD persistent disks Volume specifications: Minimum: 100GiB. In a PoC or evaluation, the minimum root disk space should be sufficient. When monitoring more Dataproc clusters or lots of jobs, we recommend a 300-500GB SSD persistent disks that can handle high rates of IOPS For production use, we recommend a 200GB SSD persistent disks. The Baseline IOPS (3 IOPS per GiB with a minimum of 100 IOPS, burstable to 3000 IOPS) is sufficient for Unravel. Unravel Server doesn't require heavy resources, but it's best to check your Dataproc Quotas as you proceed. Sizing You must have separate nodes for the Unravel server and the external MySQL database . Unravel Server The minimum requirements for cores, RAM, and directories for a typical environment with default data retention and lookback settings. \/usr\/local\/unravel is the storage location for Unravel binaries. \/srv\/unravel is used for Elasticsearch (ES) and the bundled database. Root device type recommended: Standard Persistent Disk \/ SSD persistent disks Jobs per day Cores RAM \/usr\/local\/unravel \/srv\/unravel Less than 50,000 8 96 GB 8 GB free 500 GB free 50,000 to 100,000 to 8 128GB 8 GB free 500 GB free Over 100,000 Contact Unravel Support MySQL Server The minimum requirements for cores, RAM, and disk. Jobs per day Data retention length Cores RAM Disk Less than 50,000 30 days 4 32 GB 1 TB 60 days 4 32 GB 2 TB 50,000 to 100,000 to 30 days 8 64 GB 2 TB 60 days 8 64 GB 4 TB Over 100,000 Contact Unravel Support Software Operating system: RedHat\/CentOS 6.4 - 7.4 Network The following ports must be open on the Unravel GCE. In addition, the Unravel GCE must be able to access all ports on the Dataproc cluster. Settings related to IAM roles and firewall rules In order to manage, monitor, and optimize the modern data applications running on your Dataproc cluster, Unravel needs data from the cluster as well as from apps running on the cluster. This data includes metrics, configuration information, and logs. Parts of this data is pushed to Unravel, and part of it is pulled by the daemons running on Unravel Server. In order for all data to be accessible, there must be both inbound and outbound access between Unravel Server (on the GCE) and the Dataproc cluster. The Unravel Server must be in the same region as the target Dataproc clusters it is monitoring. There are two possible scenarios: Both the Dataproc cluster and the Unravel server are created on the same VPC, same subnet; and the security group allows all traffic from the same subnet. The Dataproc cluster is located on a different VPC than the Unravel server. In this case you must configure VPC peering, route table creation, and update the firewall policy. The Unravel Server needs a TCP and UDP connection to the Dataproc master node. To implement this, do either of the following: Create a firewall rule that allows port 3000 and port 4043 from the Dataproc cluster node's IP address. Configure the firewall rule on Unravel Server to allow TCP traffic on ports 3000 for Dataproc cluster nodes. Put the member of firewall rule used on the Dataproc cluster in this rule. The Unravel Server and Dataproc clusters must allow all outbound traffic. Dataproc cluster nodes must allow all traffic from Unravel Server. If you can't allow the Unravel server to access all traffic, you must minimally allow it to access the cluster nodes' TCP ports 9870, 9866, and 9867 Ports Direction Description 3000 Both Non-HTTPS traffic to and from Unravel UI. 4043 In UDP and TCP ingest traffic from the entire cluster to Unravel Servers. Skill set These instructions are self-contained, and require only basic knowledge of GCP. You don't need to create any scripts or be familiar with any specific programming or scripting language. These instructions assume you're proficient in: Provisioning GCEs. Creating and configuring the required IAM roles, firewall rules, etc. Understanding GCP networking concepts such as virtual private clouds (VPCs) and subnets. Running Ansible scripts, basic Unix commands, and AWS CLI commands. " }, 
{ "title" : "Architecture", 
"url" : "102089-dataproc-arch.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Google Dataproc \/ Architecture", 
"snippet" : "In order to manage, monitor, and optimize the modern data applications running on your Dataproc cluster, Unravel server needs data corresponding to the Dataproc cluster as well as about the modern data apps running on the cluster. This information includes metrics, configuration information, and log...", 
"body" : "In order to manage, monitor, and optimize the modern data applications running on your Dataproc cluster, Unravel server needs data corresponding to the Dataproc cluster as well as about the modern data apps running on the cluster. This information includes metrics, configuration information, and logs. Some of this data is pushed to Unravel, while some is pulled by the daemons in Unravel Server. " }, 
{ "title" : "Installing Unravel Server on an GCE VM", 
"url" : "102090-dataproc-gce-part1.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Google Dataproc \/ Installing Unravel Server on an GCE VM", 
"snippet" : "This topic explains how to create a new GCE instance, install and configure Unravel Server on the new GCE instance, and connect it to the GCE cluster you want to monitor. If you have not already done so, confirm your cluster meets Unravel's hosting requirements . 1. Create an GCE instance On your GC...", 
"body" : "This topic explains how to create a new GCE instance, install and configure Unravel Server on the new GCE instance, and connect it to the GCE cluster you want to monitor. If you have not already done so, confirm your cluster meets Unravel's hosting requirements . 1. Create an GCE instance On your GCP console , go to the GCEs dashboard and click Create Instance . Select the following options based on Unravel's instance requirements: Base OS Instance type and size GCE instance's Firewall Rules \/ IAM role Best practice is to create an IAM role that contains the policy that only reads the specific Cloud storage bucket used on Dataproc cluster. Then create an instance profile and add the IAM role to it. Ports Networking The GCE instance must be in same region with the target Dataproc clusters which Unravel compute node is be monitoring. Firewall rules or policies Create an Cloud storage ReadAccess only IAM role and assign it to Unravel GCE to read the archive logs on the Cloud storage bucket configured for the Dataproc cluster. Create TCP and UDP connections from the Dataproc master node to Unravel Compute node. Create a firewall rule that allows port 3000 and port 4043 from Dataproc cluster nodes' IP address, and put the member of the Firewall Rules used on Dataproc cluster in this rule. Sample inbound rule Type Protocol Port range Source All traffic All All For example, 10.10.0.0\/16 SSH TCP 22 0.0.0.0\/0 or trusted public IP for SSH access Custom TCP Rule TCP 3000 Custom TCP Rule TCP 4043 Sample outbound rule Type Protocol Port range Source All traffic All All 0.0.0.0\/0 The GCE instance should have all TCP access to the Dataproc cluster (server\/parent or worker) nodes. You can grant access by inserting adding firewall rules of the Dataproc server\/parent and worker with all TCP, all port range. If it isn't possible to allow the Unravel VM access to all traffic to Dataproc cluster, you must minimally allow it to access cluster nodes' TCP ports 9870, 9866 and 9867. While creating the GCE instance add the Firewall properties, Enable the HTTP and HTTPS traffic Go to Network tab and add Network tags . (This is the firewall rules that is already created.) 2. Configure the GCE instance Disable selinux . sudo setenforce Permissive Edit \/etc\/selinux\/config to make sure the setting persists after reboot and make sure SELINUX=permissive . sudo vi \/etc\/selinux\/config Install libaio.x86_64 , lzop.x86_64 , and ntp.x86_64 . sudo yum install -y libaio.x86_64\nsudo yum install -y lzop.x86_64\nsudo yum install -y ntp.x86_64 Start ntpd and check the system time. sudo service ntpd start\nsudo ntpq -p Create a new user named hadoop . sudo useradd hadoop 4. Install the Unravel RPM on the GCE instance Download the Unravel Server RPM. downloads Install the Unravel Server RPM. The precise filename can vary, depending on how it was fetched or copied. sudo rpm -U unravel-4.5.0.*-EMR-latest.rpm Switch User and User-group to hadoop hadoop . sudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh hadoop hadoop Add the following property to \/usr\/local\/unravel\/etc\/unravel.properties : com.unraveldata.onprem=false For monitoring Dataproc Spark service, add the following properties to \/usr\/local\/unravel\/etc\/unravel.properties : com.unraveldata.spark.live.pipeline.enabled=true\ncom.unraveldata.spark.hadoopFsMulti.useFilteredFiles=true\ncom.unraveldata.spark.events.enableCaching=true The installation creates the following items: Virtualization type: HVM User unravel (if it doesn't exist already). \/etc\/init.d\/unravel_* scripts for controlling services, and \/etc\/init.d\/unravel_all.sh which you can use to manually stop, start, and get status of all daemons in proper order. 6. Log into Unravel UI Start Unravel daemons. sudo \/etc\/init.d\/unravel_all.sh start Create an SSH tunnel from your workstation to the Unravel GCE instance. ssh -i ssh_key.pem centos@ unravel-host-ip -L 3000:127.0.0.1:3000 Using a supported web browser , navigate to http:\/\/127.0.0.1:3000 and log in with username admin with password unraveldata . GoogleDataproc See Unravel Product Documentation – User Guide to learn how to use Unravel. " }, 
{ "title" : "3. Install MySQL", 
"url" : "102090-dataproc-gce-part1.html#UUID-34013772-f1d8-bfe0-a93f-92cf73658e4f_section-5cd09b081436d-idm45444639878864", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Google Dataproc \/ Installing Unravel Server on an GCE VM \/ 3. Install MySQL", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "5. Configure MySQL", 
"url" : "102090-dataproc-gce-part1.html#UUID-34013772-f1d8-bfe0-a93f-92cf73658e4f_section-5cd09bd696d67-idm45444640181408", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Google Dataproc \/ Installing Unravel Server on an GCE VM \/ 5. Configure MySQL", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "7. Connect to your new GCE cluster.", 
"url" : "102090-dataproc-gce-part1.html#UUID-34013772-f1d8-bfe0-a93f-92cf73658e4f_section-5d5d92b1ea459-idm45705481488384", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Google Dataproc \/ Installing Unravel Server on an GCE VM \/ 7. Connect to your new GCE cluster.", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Connecting Unravel Server to a new Dataproc cluster", 
"url" : "102091-dataproc-gce-new-part2.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Google Dataproc \/ Connecting Unravel Server to a new Dataproc cluster", 
"snippet" : "This topic explains how to set up and configure your Dataproc cluster so that Unravel can begin monitoring jobs running on the cluster. Assumptions The GCE instance for Unravel Server has been created. Unravel services are running. The firewall rules on the GCE allows traffic to\/from Dataproc cluste...", 
"body" : "This topic explains how to set up and configure your Dataproc cluster so that Unravel can begin monitoring jobs running on the cluster. Assumptions The GCE instance for Unravel Server has been created. Unravel services are running. The firewall rules on the GCE allows traffic to\/from Dataproc cluster nodes on TCP port 3000. The Unravel GCE instance and Dataproc clusters allow all outbound traffic. The nodes in the Dataproc cluster allow all traffic from the Unravel GCE. This implies either one of the following configurations: The DataProc cluster is on a different VPC, and you've configured VPC peering, route table creation, and updated your Firewall policy. The DataProc cluster is on a different VPC, and you've configured VPC peering, route table creation, and updated your Firewall policy. Network ACL on VPC allows all traffic. Connect to a new DataProc cluster Follow the steps below to run Initialization actions, unravel_emr_dataproc_init.py , on all nodes in the cluster. The bootstrap script makes the following changes: On the server\/parent node: On Hive clusters, it updates \/etc\/hive\/conf\/hive-site.xml . On Spark clusters, it updates \/etc\/spark\/conf\/spark-defaults.conf . It updates \/etc\/hadoop\/conf\/mapred-site.xml . It updates \/etc\/hadoop\/conf\/yarn-site.xml . If Tez is installed, it updates \/etc\/tez\/conf\/tez-site.xml . It installs and starts the unravel_es daemon in \/usr\/local\/unravel_es . It installs the Spark and MapReduce sensors in \/usr\/local\/unravel-agent . It installs the Hive Hook sensor in \/usr\/lib\/hive\/lib\/ . On all other nodes: It installs the Spark and MapReduce sensors in \/usr\/local\/unravel-agent . It installs Hive sensors in \/usr\/lib\/hive\/lib . Be sure to subtitute your specific bucket location for my-bucket . Download Unravel's bootstrap script, unravel_emr_bootstrap.py using curl or gsutil . curl curl https:\/\/storage.cloud.google.com\/unraveldata.com\/unravel_dataproc_init.py -o \/tmp\/unravel_dataproc_bootstrap.py gsutil gsutil cp gs:\/\/unraveldata.com\/unravel_dataproc_init.py \/tmp\/unravel_dataproc_init.py Upload the bootstrap script to a Google Cloud Storage Bucket. Permissions needed You need write access to the Cloud Storage bucket that you want to upload the init actions script to. In addition, the GCP account you use to create the Dataproc cluster must have read access to the init action script to execute its directives. Use gsutil to upload the init action script to the default Dataproc logging bucket. gsutil cp unravel_dataproc_init.py gs:\/\/ my-bucket \/unravel_dataproc_init.py In the GCP console , select the Dataproc services and click Create cluster . In Hardware section: Set Network and VM Subnet to the cluster's VPC and subnet. The firewall rule of the subnet you specify must have access to the Unravel GCE node. Select the required configuration of the cluster, with cluster mode being Single node\/Standard . Modify the instance type and enter the desired instance count for worker and preemptible worker nodes Select the Image 1.4\/1.3 from Standard Cloud Dataproc image . Provide the Network Options such as the VPC, subnet as specified in the prerequisites . Add the Initialization actions and Metadata to connect your Dataproc cluster to the Unravel node You can specify script arguments in the Metadata section. Settings Value Script location Script location gs:\/\/ my-bucket \/unravel_dataproc_init.py Optional arguments metrics-factor interval: Specifies the interval at which Unravel sensors push data from the Dataproc cluster nodes to Unravel Server. interval is in units of 5 seconds. In other words, a value of 1 means 5 seconds, 2 means 10 seconds, and so on. Default: 1. all: Enables all sensors, including the MapReduce sensor. disable-aa: Disables the auto action feature. enable-am-polling: Enables \"application master\" metrics polling for auto actions. hive-id-cachenum-jobs: Maximum number of jobs you expect to have on the cluster. Default: 1000. Click Create Cluster . Sanity check After you connect the Unravel GCE to your Dataproc cluster, run some jobs on the Dataproc cluster and monitor the information displayed in Unravel UI (http:\/\/ unravel_VM_node_public_IP :3000). " }, 
{ "title" : "Setting up Google Cloud SQL (optional)", 
"url" : "102092-dataproc-mysql.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Google Dataproc \/ Setting up Google Cloud SQL (optional)", 
"snippet" : "Step 1: Create database on Google Dataproc. Log in to GCP. Navigate to Google Dataproc console . Select Choose MySQL . Complete the Instance Info including the Instance ID , Root Password , Location Region and Zone , and the Database version . Click Save . To ensure connectivity, the Instance must b...", 
"body" : "Step 1: Create database on Google Dataproc. Log in to GCP. Navigate to Google Dataproc console . Select Choose MySQL . Complete the Instance Info including the Instance ID , Root Password , Location Region and Zone , and the Database version . Click Save . To ensure connectivity, the Instance must be in the same region as Unravel. Under Connectivity select Public IP and the Instance type ( Machine Type ) to create the SQL Instance. After the database is created go to the overview of the instance. Make note of its Private IP address as you need this address to configure Unravel. If you are using Public IP navigate to Connections > Add network . Add the unravel private IP to the list to authorize the connection to the SQL Instance The Public IP changed upon rebooting the Unravel VM. Upon reboot you must add the new Public IP to the Authorized Network . Step 2: Log into the Unravel node and install the MySQL Client sudo yum install mysql Step 3: Log into the MySQL with the Public IP of the SQL Instance and verify the connection is valid. Exit once verified. mysql --host= sql_instance _IP --user=root -- password Step 4: Install the MySQL JDBC driver. Download the JDBC driver for MySQL to \/tmp . wget https:\/\/dev.mysql.com\/get\/Downloads\/Connector-J\/mysql-connector-java-5.1.47.tar.gz -O \/tmp\/mysql-connector-java-5.1.47.tar.gz Navigate to \/tmp and extract the driver. cd \/tmp\ntar xvzf \/tmp\/mysql-connector-java-5.1.47.tar.gz Copy the MySQL JDBC JAR to \/usr\/local\/unravel\/share\/java\/ . sudo mkdir -p \/usr\/local\/unravel\/share\/java\nsudo cp \/tmp\/mysql-connector-java-5.1.47\/mysql-connector-java-5.1.47.jar \/usr\/local\/unravel\/share\/java\nsudo cp \/tmp\/mysql-connector-java-5.1.47\/mysql-connector-java-5.1.47.jar \/usr\/local\/unravel\/dlib\/unravel Configure Unravel to connect to the MySQL server. Using mysql, create a database and user for Unravel. CREATE DATABASE unravel_mysql_prod;\nCREATE USER 'unravel'@'%' IDENTIFIED BY ' password ';\nGRANT ALL PRIVILEGES ON unravel_mysql_prod.* TO 'unravel'@'%'; In \/usr\/local\/unravel\/etc\/unravel.properties , update the following properties: unravel.jdbc.username (username will be unravel)\nunravel.jdbc.password (password thats given in above step)\nunravel.jdbc.url () Example: # username is always unravel\nunravel.jdbc.username=unravel\n\n# use the password used in Step 4\nunravel.jdbc.password=unraveldata\n\n# use public IP of SQL instance from Step 6\nunravel.jdbc.url=jdbc:mysql:\/\/127.0.0.1:3306\/unravel_mysql_prod Restart unravel daemons, \/etc\/init.d\/unravel_all.sh restart\t Step 5: Create a schema for the Unravel tables. sudo \/usr\/local\/unravel\/dbin\/db_schema_upgrade.sh Step 6: Create the default admin user for Unravel UI. sudo \/usr\/local\/unravel\/install_bin\/db_initial_inserts.sh | sudo \/usr\/local\/unravel\/install_bin\/db_access.sh " }, 
{ "title" : "Step 7: Run db_access.sh to verify the connectivity.", 
"url" : "102092-dataproc-mysql.html#UUID-3bbca53d-e8ff-4e92-e888-8db3438eb02c_section-5dc454fb6bfc2-idm44771258460768", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Google Dataproc \/ Setting up Google Cloud SQL (optional) \/ Step 7: Run db_access.sh to verify the connectivity.", 
"snippet" : "sudo \/usr\/local\/unravel\/install_bin\/db_access.sh...", 
"body" : " sudo \/usr\/local\/unravel\/install_bin\/db_access.sh " }, 
{ "title" : "Step 8: Log into the Unravel UI to verify the connectivity to the Unravel UI.", 
"url" : "102092-dataproc-mysql.html#UUID-3bbca53d-e8ff-4e92-e888-8db3438eb02c_section-5dc4895dd1fba-idm46022269793904", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Google Dataproc \/ Setting up Google Cloud SQL (optional) \/ Step 8: Log into the Unravel UI to verify the connectivity to the Unravel UI.", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Microsoft Azure HDInsight", 
"url" : "102093-install-azure-hdi.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure HDInsight", 
"snippet" : "This topic explains how to deploy Unravel on Microsoft Azure HDInsight. Quickstart If you want to try Unravel quickly (for development or test environments), deploy Unravel for Azure HDInsight from Azure Marketplace . Supported clusters Cluster with respect to Unravel setup Supported New cluster wit...", 
"body" : "This topic explains how to deploy Unravel on Microsoft Azure HDInsight. Quickstart If you want to try Unravel quickly (for development or test environments), deploy Unravel for Azure HDInsight from Azure Marketplace . Supported clusters Cluster with respect to Unravel setup Supported New cluster without auto-scaling ✓ New cluster with auto-scaling ✓ Existing cluster without auto-scaling ✓ Existing cluster with auto-scaling ✓ Additionally, Azure HDInsight has a concept of cluster type. HDInsight Cluster Type Supported Spark ✓ Hadoop ✓ HBase ✓ Kafka ✓ Storm ✖️ Interactive Query ✖️ ML Services ✖️ " }, 
{ "title" : "Prerequisites", 
"url" : "102094-azure-hdi-pre.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure HDInsight \/ Prerequisites", 
"snippet" : "Permissions You must already have an Azure account . You must already have a resource group assigned to a region in order to group your policies, VMs, and storage blobs\/lakes\/drives. A resource group is a container that holds related resources for an Azure solution. In Azure, you logically group rel...", 
"body" : "Permissions You must already have an Azure account . You must already have a resource group assigned to a region in order to group your policies, VMs, and storage blobs\/lakes\/drives. A resource group is a container that holds related resources for an Azure solution. In Azure, you logically group related resources such as storage accounts, virtual networks, and virtual machines (VMs) to deploy, manage, and maintain them as a single entity. You must have root privilege in order to perform some commands on the VM. You must already have created Azure storage . You must have an SSH key pair. " }, 
{ "title" : "Platform", 
"url" : "102094-azure-hdi-pre.html#UUID-1f328f17-5792-00b4-3161-ce6a502c74c0_section-5d2cdabd85312-idm45764258367888", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure HDInsight \/ Prerequisites \/ Platform", 
"snippet" : "Each version of Unravel has specific platform requirements. Check Unravel's compatibility matrix to confirm your HDInsight platform meets the requirements for the Unravel version you are installing. AzureHDI...", 
"body" : "Each version of Unravel has specific platform requirements. Check Unravel's compatibility matrix to confirm your HDInsight platform meets the requirements for the Unravel version you are installing. AzureHDI " }, 
{ "title" : "Sizing", 
"url" : "102094-azure-hdi-pre.html#UUID-1f328f17-5792-00b4-3161-ce6a502c74c0_section-5d2cdac76fd25-idm45824275743408", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure HDInsight \/ Prerequisites \/ Sizing", 
"snippet" : "Minimum VM type suggested: Medium memory optimized such as Standard_E8s_v3 You must have separate nodes for the Unravel server and the external MySQL database . Unravel Server Architecture: x86_64 vm.max_map_count is set to 262144 Minimum requirements for cores, RAM, and disks: The table below lists...", 
"body" : "Minimum VM type suggested: Medium memory optimized such as Standard_E8s_v3 You must have separate nodes for the Unravel server and the external MySQL database . Unravel Server Architecture: x86_64 vm.max_map_count is set to 262144 Minimum requirements for cores, RAM, and disks: The table below lists the minimum requirements for cores, RAM, and disks for a typical environment with default data retention and lookback settings . \/usr\/local\/unravel is the storage location for Unravel binaries. \/srv\/unravel is used for Elasticsearch (ES) and the bundled database. In production environments, put \/usr\/local\/unravel and \/srv\/unravel on separate disks. Putting \/srv\/unravel on a separate high spin HDD with its own SATAIII (or equivalent) bus significantly increases IO bandwidth. If \/usr\/local\/unravel or \/srv\/unravel doesn't have the minimum free space shown in the table below, create symbolic links for them to another disk. To check the space on a volume use the df command. For example, df -h \/srv Jobs per day Cores RAM \/usr\/local\/unravel \/srv\/unravel Less than 50,000 8 96 GB 8 GB free 500 GB free 50,000 to 100,000 to 8 128GB 8 GB free 500 GB free Over 100,000 Contact Unravel Support All volumes are mounted. \/tmp is mounted with executable permissions. To re-mount \/tmp with executable permissions use the following command: mount -o remount,exec \/tmp MySQL Server Minimum requirements for cores, RAM, and disk. Jobs per day Data retention length Cores RAM Disk Less than 50,000 30 days 4 32 GB 1 TB 60 days 4 32 GB 2 TB 50,000 to 100,000 to 30 days 8 64 GB 2 TB 60 days 8 64 GB 4 TB Over 100,000 Contact Unravel Support " }, 
{ "title" : "Software", 
"url" : "102094-azure-hdi-pre.html#UUID-1f328f17-5792-00b4-3161-ce6a502c74c0_section-5d2cdacfd5679-idm45824275235408", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure HDInsight \/ Prerequisites \/ Software", 
"snippet" : "Image (underlying operating system for the VM): RHEL 7 or CentOS 7.2 - 7.6. The actual HDInsight Kafka\/Spark cluster can run another OS....", 
"body" : "Image (underlying operating system for the VM): RHEL 7 or CentOS 7.2 - 7.6. The actual HDInsight Kafka\/Spark cluster can run another OS. " }, 
{ "title" : "Network", 
"url" : "102094-azure-hdi-pre.html#UUID-1f328f17-5792-00b4-3161-ce6a502c74c0_section-5d2cdae46b909-idm45824275312944", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure HDInsight \/ Prerequisites \/ Network", 
"snippet" : "You must already have a virtual network and network security group set up for your resource group. Your virtual network and subnet(s) must be big enough to be shared by the Unravel VM and the target HDInsight cluster(s). The Unravel VM must be located in the same VNET and VSNET as the HDInsight clus...", 
"body" : "You must already have a virtual network and network security group set up for your resource group. Your virtual network and subnet(s) must be big enough to be shared by the Unravel VM and the target HDInsight cluster(s). The Unravel VM must be located in the same VNET and VSNET as the HDInsight cluster. You must allow inbound SSH connections to the Unravel VM. You must allow outbound Internet access and all traffic within the subnet (VSNET). Port 443 is open on the cluster for Azure HDInsight to monitor applications. Port 3000 (or 4020) is open for Unravel Web UI access. UDP and TCP ports 4041-4043 are open from the cluster to Unravel Server. On the new node, open the following ports: Port(s) Direction Description 3000 Both Traffic to and from Unravel UI 3316 Both Database traffic 4020 Both Unravel APIs 4021 Both Host monitoring of JMX on localhost 4031 Both Database traffic 4043 In UDP and TCP ingest traffic from the entire cluster to Unravel Server(s) 4044-4049 In UDP and TCP ingest spares for unravel_lr* 4091-4099 Both Kafka brokers 4171-4174, 4176-4179 Both ElasticSearch; localhost communication between Unravel daemons or Unravel Servers in a multi-host deployment 4181-4189 Both Zookeeper daemons 4210 Both Cluster access service HDFS ports Both Traffic to\/from the cluster to Unravel Server(s) Hive metadata database port Out For YARN only. Traffic from Hive to Unravel Server(s) for partition reporting 8088 Out Traffic from Unravel Server(s) to the Resource Manager API 8188 Out Traffic from Unravel Server(s) to the ATS server(s) 11000 Out For Oozie only. Traffic from Unravel Server(s) to the Oozie server " }, 
{ "title" : "Creating Azure storage", 
"url" : "102095-azure-create-storage.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure HDInsight \/ Creating Azure storage", 
"snippet" : "In Azure, storage is separate from compute. An HDI or Databricks cluster can use one or more storage accounts. You create a storage account and specify its storage format. Unravel supports the following Azure storage formats: WASB ADLS Gen 1 ADLS Gen 2 or ABFS Description Windows Azure Storage Blob ...", 
"body" : "In Azure, storage is separate from compute. An HDI or Databricks cluster can use one or more storage accounts. You create a storage account and specify its storage format. Unravel supports the following Azure storage formats: WASB ADLS Gen 1 ADLS Gen 2 or ABFS Description Windows Azure Storage Blob (WASB or Blob) is a general-purpose storage format that uses a key-value store with a flat namespace. It has full support for: Analytics workloads; batch, interactive, streaming analytics Machine learning data such as log files, IoT data, click streams, large datasets Low-cost, tiered storage High availability\/disaster recovery Azure Data Lake Storage Generation 1 (ADLS Gen 1) is a hierarchical file system. It has full support for: Analytics workloads; batch, interactive, streaming analytics Machine learning data such as log files, IoT data, click streams, large datasets File system semantics File-level security Scalability Azure Data Lake Storage Generation 2 (ADLS Gen 2 or ABFS) combines the features of WASB and ADLS Gen 1. Does Unravel support this format? Yes Yes 4.5.0.5: No. Preview mode only. 4.5.2.0: Yes Does Unravel support encrypted access (SSL)? No Yes 4.5.0.5: No 4.5.2.0: Yes Does Unravel support multiple storage accounts on a single Unravel VM? Yes 4.5.0.5: No. Unravel supports a single ADLS Gen 1 account. 4.5.2.0: Yes 4.5.0.5: No 4.5.2.0: Yes This topic explains how to create Azure storage for your HDI or Databricks cluster. Later, you'll tell Unravel about your storage account(s) and their storage format so that Unravel Server knows how\/where to pull event logs and executor logs from the storage account (necessary for Spark on HDI; for other app types, logs are pushed to Unravel Server from its sensors). The steps below assume that: You have an Azure account. You already have a resource group assigned to a region in order to group your policies, VMs, and storage blobs\/lakes\/drives. A resource group is a container that holds related resources for an Azure solution. In Azure, you logically group related resources such as storage accounts, virtual networks, and virtual machines (VMs) to deploy, manage, and maintain them as a single entity. You already have a virtual network for your resource group. This virtual network will be shared by your cluster and the Unravel VM. Log into the Azure portal . Click Storage accounts | + Add . On the Basics tab, enter values for the following fields: Subscription : Select the subscription type. Resource Group : Select the resource group to associate with this storage instance. Storage Account Name : Enter a name, using lowercase letters and numbers. Location : Select a data center region. Performance : Select Standard or Premium : Standard storage uses magnetic disks and is cheaper. Premium storage uses SSDs, so it has higher performance and is recommended for Spark and Kafka clusters. Account kind : Select your storage format. Replication : Select your desired replication to either be local, or always available in the same zone, region, or replicated geographically. See more choices in the Advanced section. Locally redundant storage (LRS) : Only handles failures within the data-center. Durability guarantee is 11 9's. Zone-redundant storage (ZRS) : Handles failures in the data-center and zone, but not region. Durability guarantee is 12 9's. Only supported on ADLS Gen 2. Geo-redundant storage (GRS) : Handles failures in the data-center, zone, and region, but does not allow read-access in another region in a failure scenario. Durability guarantee is 16 9's. Read-access geo-redundant storage (RA-GRS) FIXLINK: Handles failures in the data-center, zone, region, and allows read-access in another region. Durability guarantee is 16 9's. Access Tier : Only available for WASB storage and ADLS Gen 2 . If you pick this option, select hot storage. Click the Advanced tab. Set Secure transfer required to Disabled or Enabled . Unravel doesn't support encryption (SSL) with WASB. For Virtual Networks, select whether to allow traffic from all networks or only from within the virtual network and subnet(s) you specify. Click Review + create . If your settings are correct, click Create . To edit your settings, click Previous . Resources Comparison of WASB and ADLS Gen 1 Azure - creating a storage account Difference between replication types " }, 
{ "title" : "Part 1: Installing Unravel on a Separate Azure VM", 
"url" : "102096-azure-hdi-part1-vm.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure HDInsight \/ Part 1: Installing Unravel on a Separate Azure VM", 
"snippet" : "Installing Unravel on a separate Azure VM allows you to connect to ephemeral Hadoop clusters on the same virtual network. This topic explains how to create a separate Azure VM, install the Unravel RPM, and configure it. If you have not already done so, confirm your cluster meets Unravel's hosting re...", 
"body" : "Installing Unravel on a separate Azure VM allows you to connect to ephemeral Hadoop clusters on the same virtual network. This topic explains how to create a separate Azure VM, install the Unravel RPM, and configure it. If you have not already done so, confirm your cluster meets Unravel's hosting requirements . 1. Provision an Azure VM for Unravel Server Log into the Azure portal . Select Virtual machines , and click + Add . On the Basics tab, enter values for the following fields: Subscription : Select the subscription type. Resource Group : Select the resource group to associate with this VM. The VM inherits configurations for lifecycle, permissions, and policies from this group. Virtual machine name : Enter a name, using only alphanumeric characters, hypens (\"-\"), and underscores (\"_\"). The VM name you specify here also becomes the VM's hostname. Region : Select a data center region for this VM. Some VM types are unavailable in some regions. Availability options : Select your redundancy (durability) settings. Image : Select a compatible underlying operating system for the VM. compmatrix-platform Size (required): Select a VM type that meets Unravel's requirements . Select your VM's Authentication type . Best practice is to authenticate using an SSH public key, which you can generate using ssh-keygen . Avoid any reserved names like \"admin\" for the username. Set Inbound Port Rules: If you plan to allow external access to Unravel UI, select Allow selected ports and then select HTTPS and SSH . Click Next: Disks . On the Disks tab, enter values for the following fields: OS disk type : For better performance in production, we recommend a Premium SSD because it tolerates higher IOPS. For a dev\/test cluster, we suggest a Standard SSD . Advanced : We recommend using managed disks that have better performance and reliability. Data disks : If you don't have a disk ready, click Create and attach new disk . In the dialog box, specify the disk name, size in GiB (must meet Unravel's minimum requirements ), and source type of \"empty disk\". Otherwise, click Attach an existing disk . Click Next: Networking . On the Networking tab, enter values for the following fields: For HDInsight, it's essential that the VM, the Azure storage, and the cluster(s) you plan to monitor are all on the same virtual network and subnet(s). Virtual network (required): Select the appropriate virtual network for your cluster(s). Subnet (required): Select a subnet with the appropriate address range based on the number of IPs you plan to have in your network . NIC network security group: Set this to Basic . For HDInsight, a TCP and UDP connection is needed from the \" head node \" of each HDInsight cluster to Unravel Server. Add an inbound security policy to allow SSH access and 443 access to the Unravel node. The default security policy should allow all access within the VNET. Default rules start with a priority of 65000. Click Review + create . Click Create . It takes about 2 minutes to create your VM. When Azure completes the creation of your VM, click Go to resource . Copy the VM's public IP address. Open an SSH session to your VM's public IP address to verify that your IP address is reachable: ssh -i ssh-key user @ ip-address Verify that eth0 on the new VM is bound to the private IP address shown in the Azure portal. ifconfig\neth0 Link encap:Ethernet HWaddr 00:0d:3a:1b:c2:48\n inet addr:10.10.1.96 2. Configure the VM Install ntpd , start it at boot time, and confirm that the time on the VM is accurate. This is necessary in order to synchronize your VM's clock. sudo su -\nyum install ntp\nntpd -u ntp:ntp Disable Security Enhanced Linux (SELinux) permanently. This is important because HDFS maintains replication in different nodes\/racks, so setting firewall rules in SELinux leads to performance degradation. sudo setenforce Permissive In \/etc\/selinux\/config , set SELINUX=permissive to make sure the settings persist after reboot: SELINUX=permissive Install libaio.x86_64 . Libaio has a huge performance benefit over the standard POSIX asynchronous I\/O facility because the operations are performed in the Linux kernel instead of as a separate user process. sudo yum -y install libaio.x86_64 Install lzop.x86_64 . Hadoop requires LZO compression libraries. sudo yum install lzop.x86_64 Disable the firewall and check your iptable rules. sudo systemctl disable firewalld\nsudo systemctl stop firewalld\nsudo iptables -F\nsudo iptables -L Prepare the second disk (for example, \/dev\/sdc ) with at least 500 GB that was configured previously on Azure portal. Use fdisk -l to check any 500GB disk without partition. This step requires root privilege. sudo su -\n \n \n# List all disks and partitions\n# You should see one called \"sdc\" if you attached a 500-1000 GB disk.\nfdisk -l\nfdisk \/dev\/sdc\n# p (list current partitions)\n# n (new partition)\n# p (primary)\n# Keep accepting rest of default configs.\n# w (save)\n \n# Format the disk\n\/usr\/sbin\/mkfs -t ext4 \/dev\/sdc\n \n \nmkdir -p \/srv\n \nDISKUUID=`\/usr\/sbin\/blkid |grep ext4 |grep sdc | awk '{ print $2}' |sed -e 's\/\"\/\/g'`\necho $DISKUUID\n \n# Mount the disk on \/srv\necho \"${DISKUUID} \/srv ext4 defaults 0 0\" >> \/etc\/fstab\nmount \/dev\/sdc1 \/srv\n \n# Verify the disk space\ndf -hT \/srv\n \nFilesystem Type Size Used Avail Use% Mounted on\n\/dev\/sdc1 ext4 197G 61M 187G 1% \/srv\n \n \n# Set permissions for Unravel and symlink Unravel's directories to the \/srv mount\nmkdir -p \/srv\/local\/unravel\nchmod -R 755 \/srv\/local\nln -s \/srv\/local\/unravel \/usr\/local\/unravel\nchmod 755 \/usr\/local\/unravel If you have HDInsight clusters, create the hdfs user and the hadoop group. If you have Databricks workspaces, skip this step. sudo useradd hdfs\nsudo groupadd hadoop\nsudo usermod -a -G hadoop hdfs 3. Install Unravel Server on the VM Download the Unravel Server RPM. downloads Install the Unravel Server RPM: sudo rpm -ivh unravel- version .rpm This installation creates the following directories, databases, and users: Directories : The installation creates \/usr\/local\/unravel\/ which contains the executables, scripts, and settings ( \/usr\/local\/unravel\/etc\/unravel.properties ). \/etc\/init.d\/unravel_* contains scripts for controlling the Unravel services \/etc\/init.d\/unravel_all.sh can be used to manually stop, start, restart, and get the status of all daemons in the proper order. Subsequent RPM upgrades don't change \/usr\/local\/unravel\/etc\/unravel.properties because your site-specific properties are put into this file. Users : User unravel is created if it does not already exist. Config : The master configuration file is \/usr\/local\/unravel\/etc\/unravel.properties . Logs : All logs are in \/usr\/local\/unravel\/logs\/ Grant access to Unravel Server: By default, a Public IP should be assigned to the Unravel VM. Create a security policy that allows SSH access on to the Unravel VM through port 443. It is recommended that you use an SSH key to access the Unravel node. 5. Configure Unravel Server with basic options Open an SSH session to the Unravel VM. ssh -i ssh-private-key ssh-user @ unravel-host Set correct permissions on the Unravel configuration directory. cd \/usr\/local\/unravel\/etc\nsudo chown unravel:unravel *.properties\nsudo chmod 644 *.properties Update unravel.ext.sh based your cluster's HDInsight version. hdp-select status | grep hadoop\nhadoop-client - 2.6.5.3005-27\n\n# Append this classpath based on the version you found\necho \"export CDH_CPATH=\/usr\/local\/unravel\/dlib\/hdp2.6.x\/*\" >> \/usr\/local\/unravel\/etc\/unravel.ext.sh Run the switch_to_user.sh script. \/usr\/local\/unravel\/install_bin\/switch_to_user.sh hdfs hadoop In \/usr\/local\/unravel\/etc\/unravel.properties , add\/modify the following properties: Set com.unraveldata.onprem to false . Property\/Description Set by user Unit Default com.unraveldata.onprem Specifies whether the deployment is on premise or on cloud. Important For Azure Databricks, EMR, and HDInsight set to False   boolean true com.unraveldata.supported.platforms Defines the cloud platform when com.unraveldata.onprem =false. Value = EMR | HDI set member - Set general properties: Property\/Description Set by user Unit Default com.unraveldata.customer.organization Customer name. Used to identify your installation for reporting and notification purposes in Unravel UI. Optional string Not Set com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. Example: http:\/\/unravelserver.company.com:3000   string http:\/\/{host}:3000 com.com.unraveldata.hdfs.timezone Timezone of HDFS, for example, US\/Eastern, Etc\/GMT-4, America\/New_York. If the timezone is not set then an error message is logged and UTC timezone is used. Possible timezones can be obtained by calling TimeZone.getAvailableIDs() . string - com.unraveldata.tmpdir The base location for Unravel process control files where Unravel's temp files reside. string (path) \/srv\/unravel\/tmp com.unraveldata.history.maxSize.weeks Number of weeks retained for search results in Elastic Search. integer 5 com.unraveldata.retention.max.days Number of days to keep the heaviest data (such as error logs and drill-down details) in the SQL Database. integer 30 Point Unravel to your Azure storage account(s) and their storage formats: Properties for WASB Storage (Unravel 4.5.0.5) Property\/Description Set by user Unit Default com.unraveldata.hdinsight.storage-account. X Storage account name that a HDInsight cluster uses. You must define this property for each storage account. X starts with 1 and then is incremented by 1 for each additional account. The account numbers must be consecutive. Optional string Azure storage account name. (See finding the storage name .) com.unraveldata.hdinsight.access-key. X Storage account key. For each storage-account. X you must define access-key. X If you have two access keys, pick one to use here. Optional string Azure storage account key. (See finding the access key .) Properties for WASB Storage (Unravel 4.5.2.x) Property\/Description Set by user Unit Default com.unraveldata.azure.storage.wasb.account-name. X Name of the WASB storage account that the HDInsight cluster uses. You must define this property for each WASB storage account. X . X=1 for the first storage account and the is incremented by one for each new account, that is, account numbers must be consecutive. Optional string Azure storage account name. (See finding the storage name .) com.unraveldata.azure.storage.wasb.access-key. X WASB storage account key. For each storage account defined you must define the storage access key. If you have two keys, pick one to use here. Optional string Azure storage account access key. (See finding the access key .) Properties for ADLS Gen 1 (Unravel 4.5.0.5) In Unravel 4.5.0.5, you can only specify a single ADLS Gen 1 account. Property\/Description Set by user Unit Default com.unraveldata.adl.accountFQDN The data lake's fully qualified domain name, for example, mydatalake.azuredatalakestore.net. Optional string Azure storage account name. (See finding the storage name .) com.unraveldata.adl.clientId An application ID. An application registration has to be created in the Azure Active Directory. Optional string Azure application id. (See finding the application Id .) com.unraveldata.adl.clientKey An application access key which can be created after registering an application. Optional string Azure storage access key. (See finding the storage access key .) com.unraveldata.adl.accessTokenEndpoint The OAUTH 2.0 Access Token Endpoint. It is obtained from the application registration tab on Azure portal. Optional string Azure OAUTH 2.0 token endpoint (See finding the OAUTH endpoint .) com.unraveldata.adl.clientRootPath The path in the Data lake store where the target cluster has been given access. Optional string URL Azure CONTAINER\/DIRECTORY path for storage account name. (See finding the container path .) Properties for ADLS Gen 1 (Unravel 4.5.2.x) Property\/Description Set by user Unit Default com.unraveldata.azure.storage.adl.account-name. X The Azure Data Lake Gen1 storage account. The name does not need to be fully qualified. For instance, you can use mydatalake or mydatalake.azuredatalakestore.net . You must define this property for each storage account. X starts with 1 and then is incremented by 1 for each additional account. The account numbers must be consecutive. Optional string Azure storage account name. (See finding the storage name .) com.unraveldata.azure.storage.adl.client-id. X An application ID. An application registration has to be created in the Azure Active Directory. Optional string Azure application id. (See finding the application Id .) com.unraveldata.azure.storage.adl.client-key. X An application's \"secret\" (key) described in the ADL Gen1 client-id field. Optional string Azure storage secret. (See finding the secret (access key) .) com.unraveldata.azure.storage.adl.access-token-endpoint. X The OAUTH 2.0 Access Token Endpoint. It is obtained from the application registration tab on Azure portal. Optional string Azure OAUTH 2.0 token endpoint (See finding the OAUTH endpoint .) Properties for ADLS Gen 2 (ABFS) (Unravel 4.5.2.x) Property\/Description Set by user Unit Default com.unraveldata.azure.storage.abfs.account-name. X Name of the ABFS storage account that the HDInsight cluster uses. You must define this property for each ABFS storage account. X . X=1 for the first storage account and then is incremented by one for each new account, that is, account numbers must be consecutive. Optional string Azure storage account name. (See finding the storage name .) com.unraveldata.azure.storage.abfs.access-key. X The access key for the corresponding ABFS storage account. Optional string Azure storage account name. (See finding the secret (access key) .) 7. Start Unravel services sudo \/etc\/init.d\/unravel_all.sh restart 8. Log into Unravel UI Run the echo command to find the URL for Unravel UI. If you're using an SSH tunnel or HTTP proxy, you might need to make adjustments to the host\/IP of the URL: echo \"http:\/\/(hostname -f):3000\/\" Create an SSH tunnel to access the Azure VM for Unravel's TCP port 3000. ssh -i ssh-private-key ssh-user @ unravel-host -L 3000:127.0.0.1:3000 Using a supported web browser , navigate to http:\/\/127.0.0.1:3000 and log in as user admin with password unraveldata . compmatrix-platform " }, 
{ "title" : "4. Set up your database using one of the following methods", 
"url" : "102096-azure-hdi-part1-vm.html#UUID-33b28a07-c2f5-f098-e5c1-69a60a98a26d_section-5dca35bf9cd75-idm44771253339552", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure HDInsight \/ Part 1: Installing Unravel on a Separate Azure VM \/ 4. Set up your database using one of the following methods", 
"snippet" : "a. Set up an Azure MySQL instance for Unravel. or b. Install and configure your own MySQL....", 
"body" : "a. Set up an Azure MySQL instance for Unravel. or b. Install and configure your own MySQL. " }, 
{ "title" : "9. Connect Unravel to the HDInsight Cluster", 
"url" : "102096-azure-hdi-part1-vm.html#UUID-33b28a07-c2f5-f098-e5c1-69a60a98a26d_section-5d5d8ffa0d96a-idm46042009379728", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure HDInsight \/ Part 1: Installing Unravel on a Separate Azure VM \/ 9. Connect Unravel to the HDInsight Cluster", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Part 2: Connecting Unravel to an HDInsight cluster", 
"url" : "102098-azure-hdi-part2.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure HDInsight \/ Part 2: Connecting Unravel to an HDInsight cluster", 
"snippet" : "This topic explains how to connect Unravel Server to an HDInsight cluster and deploy Unravel sensors on the cluster's nodes through an Azure \"script action\". Unravel different \"script actions\" for different cluster types. Cluster type Download path Apply to cluster node types Hadoop, HBase, or Spark...", 
"body" : "This topic explains how to connect Unravel Server to an HDInsight cluster and deploy Unravel sensors on the cluster's nodes through an Azure \"script action\". Unravel different \"script actions\" for different cluster types. Cluster type Download path Apply to cluster node types Hadoop, HBase, or Spark unravel_hdi_spark_bootstrap_4.5.sh Head node, Worker node, Edge node Kafka unravel_hdi_kafka_bootstrap.sh Head node If your cluster doesn't have access to the Internet, download the scripts, store them in an Azure blob storage account, and use the blob storage URI on the script action's Bash script URI field. Prerequisites Unravel Server must already be running, with the Unravel UI accessible on port 3000 . If you plan to create a cluster, you must have the following information ready: Virtual Network and subnet of the Unravel VM Your Azure Storage details. For storage setup, see Create Azure Storage . Read the latest documentation on the ports required by HDInsight: https:\/\/docs.microsoft.com\/en-us\/azure\/hdinsight\/hdinsight-hadoop-port-settings-for-services Ensure Unravel service is running on Unravel VM and ports 3000 and 4043 are reachable from the Azure HDInsight cluster master node before running the the Unravel \"script action\" script. Run the following checks: ssh -i ssh_key ssh_user @ unravel-host \nsudo su -\nnetstat -anp | grep 3000\ntcp 0 0 0.0.0.0:3000 0.0.0.0:* LISTEN 65072\/node\nhostname\n\n# On one of the cluster's head nodes:\nping unravel-host Troubleshooting tips From the Azure portal, you can check if a script action finished successfully by checking the Script Action History : If script action process fails, you can check the error messages from the HDInsight cluster's Ambari dashboard, which has a balloon next to the cluster name on the top menu bar with the recent operations. Click Ops and search for the most recent run_customscriptaction command and inspect the log messages. You may see multiple entries of run_customscriptaction which were created by previous runs. The Unravel script action cannot be rerun. If you need to redeploy the Unravel script action, you must submit a new \"script action\" script with a different name. " }, 
{ "title" : "Connect to a new cluster", 
"url" : "102098-azure-hdi-part2.html#UUID-3512d4df-e463-c4ba-cd05-c2e92f35d314_optionA", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure HDInsight \/ Part 2: Connecting Unravel to an HDInsight cluster \/ Connect to a new cluster", 
"snippet" : "Log into the Azure portal . Select HDInsight cluster . In the dialog box, enter the details for your desired cluster type, topology, OS, and so on. In the Security + networking tab, make sure to select the same virtual network and subnet that is used by the Unravel VM. In the Storage tab, select whe...", 
"body" : "Log into the Azure portal . Select HDInsight cluster . In the dialog box, enter the details for your desired cluster type, topology, OS, and so on. In the Security + networking tab, make sure to select the same virtual network and subnet that is used by the Unravel VM. In the Storage tab, select whether to use Azure Blob Storage or Azure Data Lake Storage, plus any secondary accounts. In the Cluster size tab, select your desired topology for number of workers and VM types. Optional : In the Script action tab, specify settings based on your cluster type. You can also do this step after the cluster has been created. For Hadoop, HBase, or Spark clusters... Option Values Script type Custom Name unravel-script-01 (or any name to identify this script action run) Bash script URI https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_4.5.sh Node types Select options Head , Worker , and Edge . Note: The Edge option is only available for an existing cluster. Parameters You must specify: --unravel-server unravel-host-private-ip :3000 : Private IP address and port of the Unravel VM For Hadoop and Spark you must also specify: --spark-version spark-version : Indicates the Spark version running on the cluster For example, for Hadoop or Spark: --unravel-server 10.10.1.10:3000 --spark-version 2.3.0 You can also specify these options: --metrics-factor interval : Specifies the interval at which Unravel obtains JVM metrics from the EMR cluster nodes to Unravel Server. interval is in units of 5 seconds. In other words, a value of 1 means 5 seconds, 2 means 10 seconds, and so on. Default: 1 For workloads dominated by long-running jobs, use a larger factor. For example, if a cluster only has one Spark job that takes hours, use a factor of 12, or 60 seconds. --all : Enables all sensors, including the MapReduce sensor. --disable-aa : Disables the auto action feature. --enable-am-polling : Enables \"application master\" metrics polling for auto actions. --hive-id-cache num-jobs : Maximum number of jobs you expect to have on the cluster. Default: 1000. --sensor-url : Download sensor and daemon files from external URL. (Note: The sensor is uploaded to cluster dfs \/tmp\/unravel-sensors\/ the first time being installed. This can be configured with --sensor-dfs-path parameter.) --uninstall : Undoes the actions of the bootstrap script. For example, for HBase: --unravel-server 10.10.1.10:3000 --uninstall If your cluster is kerberized, the default security settings should work, but you can change them . Persist this script action to rerun when new nodes are added to the cluster. Select this checkbox. Persistence only applies to new head and worker nodes. For Kafka clusters... Option Values Script type Custom Name unravel-script-01 (or any name to identify this script action run) Bash script URI https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-kafka-script-action\/unravel_hdi_kafka_bootstrap.sh Node types Head Parameters You must specify these options: --unravel-server unravel-host-private-ip :3000 : Private IP address and port of the Unravel VM For example, --unravel-server 10.10.1.10:3000 You can also specify these options: --metrics-factor interval : Specifies the interval at which Unravel obtains JVM metrics from the EMR cluster nodes to Unravel Server. interval is in units of 5 seconds. In other words, a value of 1 means 5 seconds, 2 means 10 seconds, and so on. Default: 1 For workloads dominated by long-running jobs, use a larger factor. For example, if a cluster only has one Spark job that takes hours, use a factor of 12, or 60 seconds. --all : Enables all sensors, including the MapReduce sensor. --disable-aa : Disables the auto action feature. --enable-am-polling : Enables \"application master\" metrics polling for auto actions. If sensor autoscaling is enabled, Unravel's BTrace sensor JARs will be located in \/mnt\/yarn\/unravel-agent instead of \/usr\/local\/unravel-agent . --hive-id-cache num-jobs : Maximum number of jobs you expect to have on the cluster. Default: 1000. --sensor-url : Download sensor and daemon files from external URL.(note: sensor will be uploaded to cluster dfs \/tmp\/unravel-sensors\/ the first time being installed, can be configured with --sensor-dfs-path parameter) If your cluster is kerberized, the default security settings should work, but you can change them . Persist this script action Checked. Note that persistence only applies to new Head nodes In the Summary - Confirm configurations tab, review your cluster and click Create . It takes 5-15 minutes to create your cluster, depending on its size and parameters. " }, 
{ "title" : "Connect to an existing Hadoop, HBase, or Spark cluster", 
"url" : "102098-azure-hdi-part2.html#UUID-3512d4df-e463-c4ba-cd05-c2e92f35d314_UUID-1888cec7-ed7a-1d32-2c30-2f1fad4cbc7c", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure HDInsight \/ Part 2: Connecting Unravel to an HDInsight cluster \/ Connect to an existing Hadoop, HBase, or Spark cluster", 
"snippet" : "Log into the Azure portal . Select HDInsight cluster . Select the Hadoop, HBase, or Spark cluster that you want to apply the Unravel \"script action\" script to. Click Script actions on the vertical menu, and click Submit new . Specify the following settings for the bootstrap script: Option Values Scr...", 
"body" : "Log into the Azure portal . Select HDInsight cluster . Select the Hadoop, HBase, or Spark cluster that you want to apply the Unravel \"script action\" script to. Click Script actions on the vertical menu, and click Submit new . Specify the following settings for the bootstrap script: Option Values Script type Custom Name unravel-script-01 (or any name to identify this script action run) Bash script URI https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_4.5.sh Node types Select options Head , Worker , and Edge . Note: The Edge option is only available for an existing cluster. Parameters You must specify: --unravel-server unravel-host-private-ip :3000 : Private IP address and port of the Unravel VM For Hadoop and Spark you must also specify: --spark-version spark-version : Indicates the Spark version running on the cluster For example, for Hadoop or Spark: --unravel-server 10.10.1.10:3000 --spark-version 2.3.0 You can also specify these options: --metrics-factor interval : Specifies the interval at which Unravel obtains JVM metrics from the EMR cluster nodes to Unravel Server. interval is in units of 5 seconds. In other words, a value of 1 means 5 seconds, 2 means 10 seconds, and so on. Default: 1 For workloads dominated by long-running jobs, use a larger factor. For example, if a cluster only has one Spark job that takes hours, use a factor of 12, or 60 seconds. --all : Enables all sensors, including the MapReduce sensor. --disable-aa : Disables the auto action feature. --enable-am-polling : Enables \"application master\" metrics polling for auto actions. --hive-id-cache num-jobs : Maximum number of jobs you expect to have on the cluster. Default: 1000. --sensor-url : Download sensor and daemon files from external URL. (Note: The sensor is uploaded to cluster dfs \/tmp\/unravel-sensors\/ the first time being installed. This can be configured with --sensor-dfs-path parameter.) --uninstall : Undoes the actions of the bootstrap script. For example, for HBase: --unravel-server 10.10.1.10:3000 --uninstall If your cluster is kerberized, the default security settings should work, but you can change them . Persist this script action to rerun when new nodes are added to the cluster. Select this checkbox. Persistence only applies to new head and worker nodes. Click Create . " }, 
{ "title" : "Connect to an existing Kafka cluster", 
"url" : "102098-azure-hdi-part2.html#UUID-3512d4df-e463-c4ba-cd05-c2e92f35d314_UUID-f7499c12-b483-9081-77bd-9ec45dc579a2", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure HDInsight \/ Part 2: Connecting Unravel to an HDInsight cluster \/ Connect to an existing Kafka cluster", 
"snippet" : "Log into the Azure portal . Select HDInsight cluster . Select the Kafka cluster that you want to apply the Unravel \"script action\" script to. If the Kafka cluster has no Internet access, download the HDInsightUtilities-v01.sh script and copy it to the Kafka head node's \/tmp folder. For example, wget...", 
"body" : "Log into the Azure portal . Select HDInsight cluster . Select the Kafka cluster that you want to apply the Unravel \"script action\" script to. If the Kafka cluster has no Internet access, download the HDInsightUtilities-v01.sh script and copy it to the Kafka head node's \/tmp folder. For example, wget -O \/tmp\/HDInsightUtilities-v01.sh -q https:\/\/hdiconfigactions.blob.core.windows.net\/linuxconfigactionmodulev01\/HDInsightUtilities-v01.sh Click Script actions | Submit new . Specify the following settings for the bootstrap script: Option Values Script type Custom Name unravel-script-01 (or any name to identify this script action run) Bash script URI https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-kafka-script-action\/unravel_hdi_kafka_bootstrap.sh Node types Head Parameters You must specify these options: --unravel-server unravel-host-private-ip :3000 : Private IP address and port of the Unravel VM For example, --unravel-server 10.10.1.10:3000 You can also specify these options: --metrics-factor interval : Specifies the interval at which Unravel obtains JVM metrics from the EMR cluster nodes to Unravel Server. interval is in units of 5 seconds. In other words, a value of 1 means 5 seconds, 2 means 10 seconds, and so on. Default: 1 For workloads dominated by long-running jobs, use a larger factor. For example, if a cluster only has one Spark job that takes hours, use a factor of 12, or 60 seconds. --all : Enables all sensors, including the MapReduce sensor. --disable-aa : Disables the auto action feature. --enable-am-polling : Enables \"application master\" metrics polling for auto actions. If sensor autoscaling is enabled, Unravel's BTrace sensor JARs will be located in \/mnt\/yarn\/unravel-agent instead of \/usr\/local\/unravel-agent . --hive-id-cache num-jobs : Maximum number of jobs you expect to have on the cluster. Default: 1000. --sensor-url : Download sensor and daemon files from external URL.(note: sensor will be uploaded to cluster dfs \/tmp\/unravel-sensors\/ the first time being installed, can be configured with --sensor-dfs-path parameter) If your cluster is kerberized, the default security settings should work, but you can change them . Persist this script action Checked. Note that persistence only applies to new Head nodes Click Create . After the Kafka script action script completed successfully, open an SSH session to the Kafka cluster's \"head node\" and append the contents of \/tmp\/unravel\/unravel.ext.properties to \/usr\/local\/unravel\/etc\/unravel.properties on your Unravel VM. In a multi-cluster deployment, com.unraveldata.ext.kafka.clusters is a comma-separated list of clusters and the set of properties prefixed with com.unraveldata.ext.kafka. cluster_name are repeated for each cluster. This step also applies to new clusters. For example, for two Kafka clusters, \/tmp\/unravel\/unravel.ext.properties looks like this: com.unraveldata.ext.kafka.clusters= cluster_name1 , cluster_name2 \ncom.unraveldata.ext.kafka. cluster_name1 .bootstrap_servers=wn0- cluster_name1 :9092,wn1- cluster_name1 :9092\ncom.unraveldata.ext.kafka. cluster_name1 .jmx_servers=broker1,broker2\ncom.unraveldata.ext.kafka. cluster_name1 .jmx.broker1.host=wn0- cluster_name1 \ncom.unraveldata.ext.kafka. cluster_name1 .jmx.broker1.port=9999\ncom.unraveldata.ext.kafka. cluster_name1 .jmx.broker2.host=wn1- cluster_name1 \ncom.unraveldata.ext.kafka. cluster_name1 .jmx.broker2.port=9999\n\ncom.unraveldata.ext.kafka. cluster_name2 .bootstrap_servers=wn0- cluster_name2 :9092,wn1- cluster_name2 :9092\ncom.unraveldata.ext.kafka. cluster_name2 .jmx_servers=broker1,broker2\ncom.unraveldata.ext.kafka. cluster_name2 .jmx.broker1.host=wn0- cluster_name2 \ncom.unraveldata.ext.kafka. cluster_name2 .jmx.broker1.port=9999\ncom.unraveldata.ext.kafka. cluster_name2 .jmx.broker2.host=wn1- cluster_name2 \ncom.unraveldata.ext.kafka. cluster_name2 .jmx.broker2.port=9999 Unravel VM must have access to the Kafka worker nodes' broker port 9092 and Kafka JMX port 9999 Property\/Definition Set by user Unit Default com.unraveldata.ext.kafka.clusters Cluster list. These user-defined names are used to clearly identify the Kafka cluster in the Unravel UI. Use a comma separated list for multiple clusters. Required CSL - com.unraveldata.ext.kafka. cluster . bootstrap_servers List of brokers that is used to retrieve initial information about the kafka cluster. For each cluster in com.unraveldata.ext.kafka.clusters you must define the associated brokers. Use a comma separated list for multiple brokers. For example, com.unraveldata.ext.kafka.East.bootstrap_servers=localhost:9092,localhost:9093. Required CSL - com.unraveldata.ext.kafka. cluster . jmx_servers Aliases for each kafka nodes in the clusters with JMX ports exposed. You must assign aliases the cluster nodes. Use a comma separated list for multiple nodes. For example, unraveldata.ext.kafka.East.jmx_servers=kNode-1, kNode-2\/. Required CSL - com.unraveldata.ext.kafka. cluster . jmx. kNode-1 . host The host for a node in the cluster. You must define a host for each node in each cluster. For example, com.unraveldata.ext.kafka.East.kNode1=localhost com.unraveldata.ext.kafka.East.kNode2=localhost. Required - com.unraveldata.ext.kafka. cluster . jmx . kNode-1 . port For each node in each cluster you must assign a port. For example, com.unraveldata.ext.kafka.East.jmx.kNode1.port=5005. Required number - To locate Kafka and JMX ports: Cloudera Manager . Navigate to: Clusters → Kafka → Configuration → Ports and Addresses. Alternatively, you may lookup up the information in the broker nodes of Zookeeper CLI. HDP : For Protocol and broker port navigate to: Kafka → Configs → Kafka Broker. JMX port navigate to: Kafka → Configs → Advanced kafka-env → kafka-env template. After updating the Kafka properties, restart Unravel Server. sudo \/etc\/init.d\/unravel_all.sh restart " }, 
{ "title" : "Next steps", 
"url" : "102098-azure-hdi-part2.html#UUID-3512d4df-e463-c4ba-cd05-c2e92f35d314_section-5cc4e7c46e93e-idm45390852690944", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure HDInsight \/ Part 2: Connecting Unravel to an HDInsight cluster \/ Next steps", 
"snippet" : "For additional configuration and instrumentation options, see Next Steps ....", 
"body" : "For additional configuration and instrumentation options, see Next Steps . " }, 
{ "title" : "Finding properties in Azure", 
"url" : "102099-azure-finding-properties.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure HDInsight \/ Finding properties in Azure", 
"snippet" : "Storage Account Name In the Azure portal , click HDInsight clusters , select the cluster you want, and click Storage accounts . In the screenshot, the name of the storage account is unravelstorage01 . WASB Storage account access key In the Azure portal , click Storage accounts , and storage account ...", 
"body" : "Storage Account Name In the Azure portal , click HDInsight clusters , select the cluster you want, and click Storage accounts . In the screenshot, the name of the storage account is unravelstorage01 . WASB Storage account access key In the Azure portal , click Storage accounts , and storage account name. Select the storage acccount's Access keys . ADLS Application Id In the Azure portal , click Azure Active Directory | App registrations , and click the All applications tab. Select your application from the list of registered applications. If your application isn't listed, that means it isn't registered and you need to register it . Application access key (secret) In the Azure portal , click Azure Active Directory | App registrations and select your application. Click Certificates & secrets . Click New client secret and follow the instructions. To generate the key, click Save . Copy and save the key value immediately because when you leave this page it will no longer be visible. OAUTH 2.0 token endpoint The OAUTH 2.0 token endpoint is displayed in the application registration tab. In the Azure portal , click Azure Active Directory | App registrations and select your application. Click Overview , and select the Endpoints tab. Find the value for CONTAINER\/DIRECTORY path In the Azure portal , click HDInsight clusters , select the cluster you want, and click Storage accounts . Register a new application Select + New registration . Fill in the application information, and click Register . Grant access to the new application... Select Data Lake Storage Gen1 | your-store-name | Data explorer . Click Access | +Add . Select your app, grant the appropriate permissions to your app, and click Ok . Your app is now listed under Assigned permissions . " }, 
{ "title" : "Using Azure HDInsight APIs", 
"url" : "102100-azure-hdi-api.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure HDInsight \/ Using Azure HDInsight APIs", 
"snippet" : "This section explains how to use the Azure CLI for common actions. Best practice is to install the Azure CLI on a docker container. Submit a script action Log into the Azure CLI. az login To sign in, use a web browser to open the page https:\/\/microsoft.com\/devicelogin and enter the code ######### to...", 
"body" : "This section explains how to use the Azure CLI for common actions. Best practice is to install the Azure CLI on a docker container. Submit a script action Log into the Azure CLI. az login To sign in, use a web browser to open the page https:\/\/microsoft.com\/devicelogin and enter the code ######### to authenticate. Run the script action. First, refer to the script you want to run and ensure you have its proper parameters. You might need to remove any \" -- \" from the parameters. azure hdinsight script-action create $CLUSTER -g $RESOURCEGROUP -n $SCRIPTNAME -u $SHELLSCRIPT -p 'unravel-server $PRIVATEIP:3000 spark-version $MAJOR.$MINOR.$PATCH' -t \"headnode;workernode;edgenode\" Where: -g is the Resource Group name -n is the name of this script action task -u is the script path -p is a list of input paramaters for the script -t is a semicolon-separated list of node types For example, azure hdinsight script-action create DEVCLUSTER -g UNRAVEL01 -n unravel-script-action -u https:\/\/raw.githubusercontent.com\/unravel-data\/public\/master\/hdi\/hdinsight-unravel-spark-script-action\/unravel_hdi_spark_bootstrap_3.0.sh -p 'unravel-server 128.164.0.1:3000 --spark-version 2.3.0' -t \"headnode;workernode;edgenode\" Create an edge node An edge node is a VM on an HDInsight cluster that only has the Hadoop client installed instead of any servers or daemons. Determine which ARM template and parameter file to download to the workstation that contains Azure CLI: Option A: An edge node that also runs the Unravel script (recommended) ARM template: Parameter file: Option B: An edge node that only runs a simple script, emptynode-setup.sh ARM template: Parameter file: Download the ARM template and JSON parameter files into your configured Azure CLI workstation. curl filename -o name .json Modify the VM type, parameters, Kafka\/Spark version, and so on. For example, In the ARM template, edit these fields as appropriate. \"vmSize\": \"Standard_D3_v2\"\n\"parameters\": \"unravel-server $PRIVATEIP:3000 spark-version $MAJOR.$MINOR.$PATCH\"\n\"applicationName1\": \"$NEW_EDGE_NODE_HOSTNAME\" In the parameter file, modify the cluster name. \"clusterName\": {\n \"value\": \"$MY_CLUSTER_NAME\"\n} Validate template before deployment. az group deployment validate --resource-group \"$RESOURCEGROUP\" --template- file azuredeploy.json --parameters azuredeploy.parameters.json { \"error\": null, ... \"provisioningState\": \"Succeeded\", ... } Create the edge node. This should take 10-15 minutes to run since it has to provision a VM and install the Hadoop binaries. az group deployment create --name deploymentname --resource-group \"$RESOURCEGROUP\" --template- file azuredeploy.json --parameters azuredeploy.parameters.json Verify that the changes have been added to Ambari. Auto-scale the cluster HDInsight allows you to resize your cluster up\/down to meet your current demands. From the Azure portal, navigate to HDInsight Clusters | your-cluster | Cluster Size . Enter your desired number of workers and validate that you have enough resources for your resource group and region (based on any quotas). Click Save . HDInsight takes the appropriate action: For downsizing, HDInsight runs the Decommission command some number of workers on the DataNode, NodeManager, and HBase RegionServer processes (if it exists). Once drained, it removes the VM from Ambari and then from the cluster. For upsizing, HDInsight provisions new VM, installs the Hadoop bits, and adds the worker components (DataNode, NodeManager, and potentially HBase RegionServer). If the Unravel script action was also \"persisted\" to run on \"worker nodes\", then new VMs will automatically run a custom command for the Unravel bootstrap script. " }, 
{ "title" : "Deploying Unravel for Azure HDInsight from Azure Marketplace", 
"url" : "102101-azure-hdi-marketplace-getting-started.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure HDInsight \/ Deploying Unravel for Azure HDInsight from Azure Marketplace", 
"snippet" : "Step 1: Find Unravel for Azure HDInsight application from the Azure Marketplace Search for Unravel for Azure HDInsight in the Azure Marketplace. Once located, click Get It Now . Click Continue in the Create this app in Azure modal. You are then directed to the Azure portal. In the Azure portal click...", 
"body" : "Step 1: Find Unravel for Azure HDInsight application from the Azure Marketplace Search for Unravel for Azure HDInsight in the Azure Marketplace. Once located, click Get It Now . Click Continue in the Create this app in Azure modal. You are then directed to the Azure portal. In the Azure portal click Create to start the cluster creation. Step 3: Launch the Unravel UI In the Azure portal, locate the newly installed cluster. Click Applications in the left panel. Click Portal in the URI column of the row containing the Name unravel-edgenode . The Unravel UI, which typically has a URL format of https:\/\/ clusterName -unr.apps.azurehdinsight.net , is brought up. Log in using the credentials: admin \/ unraveldata . You should see the cluster ID in the main dashboard dropdown on the right. The graphs should be populated with some data. You can now run apps in your cluster and use Unravel to optimize them. An app's run is displayed in the Applications tab, as shown below Next Steps Try the full Unravel product free for 30 days. See Unravel Product Documentation – User Guide to learn more about how to use Unravel Unravel offers insights and recommendations for apps. To see their value run the Unravel Spark benchmark scripts (specifically Benchmarks 2.0.x). To get help for setting up or using Unravel for Azure HDInsight please contact us as at Unravel Marketplace help . " }, 
{ "title" : "Overview", 
"url" : "102101-azure-hdi-marketplace-getting-started.html#UUID-8255ee92-ab82-4a26-434f-fb5a8de7135e_N1570743747104", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure HDInsight \/ Deploying Unravel for Azure HDInsight from Azure Marketplace \/ Overview", 
"snippet" : "This page describes how to launch an Unravel for Azure HDInsight application from the Azure Marketplace. Unravel for Azure HDInsight is the only solution that provides full-stack monitoring, tuning, troubleshooting and resource optimization for big data workloads running on Azure HDInsight. Unravel ...", 
"body" : "This page describes how to launch an Unravel for Azure HDInsight application from the Azure Marketplace. Unravel for Azure HDInsight is the only solution that provides full-stack monitoring, tuning, troubleshooting and resource optimization for big data workloads running on Azure HDInsight. Unravel goes beyond passive monitoring to highly automated and intelligent management and optimization of data pipelines and applications. Azure HDInsight Cluster types supported by Unravel HDInsight Cluster Type Supported Spark ✓ Hadoop ✓ HBase ✓ Kafka ✓ Storm ✖️ Interactive Query ✖️ ML Services ✖️ These instructions are for test or development environments only. For production environments see Microsoft Azure HDInsight . For best results launch the app using the following instructions. The installation details the following steps for getting Unravel for Azure HDInsight up and running via the Azure Marketplace: " }, 
{ "title" : "Step 2: Create the HDInsight cluster (and Accept terms for Unravel app)", 
"url" : "102101-azure-hdi-marketplace-getting-started.html#UUID-8255ee92-ab82-4a26-434f-fb5a8de7135e_N1570744267103", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure HDInsight \/ Deploying Unravel for Azure HDInsight from Azure Marketplace \/ Step 2: Create the HDInsight cluster (and Accept terms for Unravel app)", 
"snippet" : "You can choose any of the four supported cluster types when trying out Unravel. In this example the cluster type is set to Spark. Add the name and other basic settings for the new cluster. Provide the storage account information. Accept Terms for the Unravel app. Wait while your cluster settings are...", 
"body" : "You can choose any of the four supported cluster types when trying out Unravel. In this example the cluster type is set to Spark. Add the name and other basic settings for the new cluster. Provide the storage account information. Accept Terms for the Unravel app. Wait while your cluster settings are validated. After your settings are successfully validated click Create . It takes approximately forty minutes for the cluster to be created and Unravel installed. " }, 
{ "title" : "Adding a new node in an existing HDI cluster monitored by Unravel", 
"url" : "102102-azure-hdi-new-node.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure HDInsight \/ Adding a new node in an existing HDI cluster monitored by Unravel", 
"snippet" : "Make sure that you have successfully connected to your existing Hadoop, HBase, or Spark cluster to ensure your jobs status won't be displayed as failures. If you have changed your Kerberos tokens or principal you must perform the following steps: Update the following properties to ensure the latest ...", 
"body" : "Make sure that you have successfully connected to your existing Hadoop, HBase, or Spark cluster to ensure your jobs status won't be displayed as failures. If you have changed your Kerberos tokens or principal you must perform the following steps: Update the following properties to ensure the latest Kerberos keytab file for Unravel is available on Unravel servers. com.unraveldata.kerberos.principal= new principal \ncom.unraveldata.kerberos.keytab.path= new path Make sure the new file's ownership\/permission is restored to the original setup. Restart all services. sudo \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "Upgrading Unravel Server", 
"url" : "102103-azure-hdi-upgrade.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure HDInsight \/ Upgrading Unravel Server", 
"snippet" : "To upgrade Unravel Server on your Azure VM, download the latest compatible RPM and install it . downloads Upgrading Unravel Server doesn't affect operations on connected clusters and can be done at any time. However, in some cases you'll also need to upgrade your Unravel Sensor(s) as well, and this ...", 
"body" : "To upgrade Unravel Server on your Azure VM, download the latest compatible RPM and install it . downloads Upgrading Unravel Server doesn't affect operations on connected clusters and can be done at any time. However, in some cases you'll also need to upgrade your Unravel Sensor(s) as well, and this requires you to re-submit the Unravel \"action scripts\" to head, worker, and edge nodes. " }, 
{ "title" : "Setting up Azure MySQL for Unravel (Optional)", 
"url" : "102108-azure-mysql.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure HDInsight \/ Setting up Azure MySQL for Unravel (Optional)", 
"snippet" : "Step 1: Create Azure MySQL 5.7 instance. We recommend using Memory Optimized type. Step 2: Disable SSL Enforce in Azure MySQL instance. Step 3: (Optional) Update Azure MySQL service parameters. Tune the following MySQL parameters for better performance for Unravel. max_allowed_packet: 32000000 and b...", 
"body" : "Step 1: Create Azure MySQL 5.7 instance. We recommend using Memory Optimized type. Step 2: Disable SSL Enforce in Azure MySQL instance. Step 3: (Optional) Update Azure MySQL service parameters. Tune the following MySQL parameters for better performance for Unravel. max_allowed_packet: 32000000 and beyond\nsort_buffer_size: 32000000 and beyond or maximum allowed value\nquery_cache_size: 64000000 and beyond or maximum allowed value\nmax_connections: 500 and beyond\nmax_connect_errors: 2000000000 and beyond\ncharacter_set_server: UTF8\ninnodb_file_per_table: ON\ninnodb_buffer_pool_size: maximum allowed value\ninnodb_lock_wait_timeout: 50\ninnodb_thread_concurrency: 20\ninnodb_read_io_threads: 16\ninnodb_write_io_threads: 4\ninnodb_io_capacity: 4000\ninnodb_io_capacity_max: 4000 Step 4: Set up VNET rule to allow Unravel server to access the database via port 3306. Select the subscription, Virtual Network, Subnet that Unravel server located and click OK . Step 6: Update \/usr\/local\/unravel\/etc\/unravel.properties with the JDBC URL to Azure MySQL instance. Check MySQL JDBC driver is located in Unravel Java classpath \/usr\/local\/unravel\/share\/java\/ wget https:\/\/dev.mysql.com\/get\/Downloads\/Connector-J\/mysql-connector-java-5.1.47.tar.gz -O \/tmp\/mysql-connector-java-5.1.47.tar.gz\ncd \/tmp\ntar xvzf \/tmp\/mysql-connector-java-5.1.47.tar.gz\nsudo mkdir -p \/usr\/local\/unravel\/share\/java\nsudo cp \/tmp\/mysql-connector-java-5.1.47\/mysql-connector-java-5.1.47.jar \/usr\/local\/unravel\/share\/java\nsudo cp \/tmp\/mysql-connector-java-5.1.47\/mysql-connector-java-5.1.47.jar \/usr\/local\/unravel\/dlib\/unravel Construct the JDBC URL for Unravel. Modify above properties in \/usr\/local\/unravel\/etc\/unravel.properties . Example: unravel.jdbc.username=<Server admin login name>\nunravel.jdbc.password=<admin password>\nunravel.jdbc.url=jdbc:mysql:\/\/<Server name>:3306\/unravel_mysql_prod\nunravel.jdbc.url.params=useSSL=true&requireSSL=false Restart Unravel daemons. \/etc\/init.d\/unravel_all.sh restart Create database\/tables for Unravel mysql --host= Server name --port=3306 --user Server admin login name -p admin password -e 'create database unravel_mysql_prod'\/usr\/local\/unravel\/dbin\/db_schema_upgrade.sh Create the default admin user for Unravel UI. sudo \/usr\/local\/unravel\/install_bin\/db_initial_inserts.sh | sudo \/usr\/local\/unravel\/install_bin\/db_access.sh " }, 
{ "title" : "Step 5: Log into the Unravel node and install the MySQL Client.", 
"url" : "102108-azure-mysql.html#UUID-503102cb-0d90-f89f-bd60-d70f543c7e56_section-5dca33f7339b1-idm46079258586160", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure HDInsight \/ Setting up Azure MySQL for Unravel (Optional) \/ Step 5: Log into the Unravel node and install the MySQL Client.", 
"snippet" : "sudo yum install mysql...", 
"body" : "sudo yum install mysql " }, 
{ "title" : "Microsoft Azure Databricks", 
"url" : "102635-install-az-databricks.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure Databricks", 
"snippet" : "This topic explains how to deploy Unravel on Microsoft Azure Databricks walking you through the following procedures. Verify you meet the prerequisites for installation of Azure Databricks azure-databricks-pre Quickstart Deploy Unravel for Azure Databricks from Azure Marketplace . az-databricks-mark...", 
"body" : "This topic explains how to deploy Unravel on Microsoft Azure Databricks walking you through the following procedures. Verify you meet the prerequisites for installation of Azure Databricks azure-databricks-pre Quickstart Deploy Unravel for Azure Databricks from Azure Marketplace . az-databricks-marketplace Create Azure components Create Azure VM Create Azure Database for MySQL Create Azure Databricks Install Unravel Install prerequisites on Azure VM Install Unravel on Azure VM Configure and restart Unravel Configure Unravel with basic options 1-az-datdabricks-conf-basic-options Configure Unravel with Azure MySQL 2-az-datdabricks-conf-mysql Configure Unravel with Azure Databricks 3-az-datdabricks-conf-az-databricks Restart Unravel 4-az-databricks-restart Complete the installation Configure Azure Databricks automated (Job) clusters with Unravel. Initializing Azure workspace and setting up a job cluster to monitor . Uninstalling Unravel server and sensors on Azure Databricks . " }, 
{ "title" : "Create Azure VM", 
"url" : "102638-az-databricks-create-vm.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure Databricks \/ Create Azure VM", 
"snippet" : "Sign in to the Azure portal . Select Virtual Machines > Add . In the Basics tab (default) enter the following. Project Details Subscription: Choose the applicable subscription. Resource group: Create a new group or choose an existing one. Instance Details Virtual Machine Name: The MySQL server name....", 
"body" : "Sign in to the Azure portal . Select Virtual Machines > Add . In the Basics tab (default) enter the following. Project Details Subscription: Choose the applicable subscription. Resource group: Create a new group or choose an existing one. Instance Details Virtual Machine Name: The MySQL server name. Region: Select the Azure region. Availability Options: Select No infrastructure is redundancy required . Image: Select the appropriate image. Both Centos-based 7.x + and Red Hat Enterprise Linux 7.x + are supported. Size: Click Change Size . In the modal select Memory optimized image with at least 128 GB memory and Premium Disk support , for example, E16s_v3 in East US 2) Administrator account Authentication type: Select password or SSH Key . Username and Password: Enter your VM login information. Inbound Port Rules Public inbound ports: Select Allow selected ports . Selected Inbound ports: Select both HTTPS and SSH . Click Next: Disks > . In the Disks tab enter the following information: Disk Options OS disk type: Select Premium SSD . Data Disk Click Create and attach a new disk . Note: This disk is formatted so don't choose Attach an existing disk . Enter a Name . Select Source type None (empty disk) . Set Size to at least 512 GiB. Disk type: Select premium SSD . Click Next: Networking > In the Networking tab enter the following information.: Virtual network: Create new or choose an existing one. Subnet: Create new or choose an existing one. Public IP: Create new or choose an existing one. Select Inbound ports: Select HTTPS and SSH . Click Review + create . Your deployment is now created. Click Operation Details for the disk you created, in this case BCTEST_DatDisk_0 . (See Step 5 above for the name.) Select Go to Resource > Networking > Inbound port rules > Add . In the Add Inbound Security Rule modal complete the following, including the port number. Rule Name : Enter rule name, for example DiskName_PORT_3000 . Destination : IP Addresses . Destination IP Address : Enter NIC Private IP . Port : Enter 3000 . Click OK . " }, 
{ "title" : "Create Azure Database for MySQL", 
"url" : "102637-az-databricks-create-mysql.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure Databricks \/ Create Azure Database for MySQL", 
"snippet" : "Select Create a Resource > Azure Database for MySQL . Click Create . In the Basics tab (default) enter the following. Project Details Subscription: Choose the applicable subscription. Resource group: Create a new group or choose an existing one. Server Details Server name: Enter the MySQL server nam...", 
"body" : "Select Create a Resource > Azure Database for MySQL . Click Create . In the Basics tab (default) enter the following. Project Details Subscription: Choose the applicable subscription. Resource group: Create a new group or choose an existing one. Server Details Server name: Enter the MySQL server name. Data Source: Select None . Admin Username: Enter the MySQL admin name. Password\/Confirm Password: Enter Admin password. Location: Select Azure region; it should be same region as the VM. (See Step 3 Create Azure VM, Instance Details.) Version: Select 5.7 . Compute + storage: Click Configure Server . Select Memory Optimized, Compute Generation - Gen 5, 4 vCores, General Purpose Storage of 100GB with Auto-growth enabled . Click OK . Click Review + Create . Select Go to Resource > Connection Security . Click + Add Client IP and add the following information: Subscription : Must be the same subscription as the VM. (See step 1 in Create VM .) Virtual Network : Must be the same virtual network as the VM. (See step 7 in Create VM .) Subnet : Create a new one if a default subnet doesn’t exist. Select Go to Resource > Connection Security > SSL settings , and change the following: Enforce SSL connection : Select Disabled . Click Save . Select Server Parameters , change the following settings Name From To sort_buffer_size 524288 16777216 (32000000 and beyond or maximum allowed value) query_cache_size 0 67108864 (64000000 and beyond or maximum allowed value) max_connect_errors 100 2000000000 (2000000000 and beyond) character_set_server LATIN1 UTF8 innodb_file_per_table OFF ON innodb_thread_concurrency 0 20 innodb_read_io_threads 4 16 innodb_io_capacity 200 4000 innodb_io_capacity_max 2000 4000 Click Save . " }, 
{ "title" : "Create Azure Databricks", 
"url" : "102636-az-databricks-create-databricks.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure Databricks \/ Create Azure Databricks", 
"snippet" : "Review VNET peering options with Azure VM. Workspace VNET Region VNET peering option VNET Injection VNET same as VM - - VNET Injection VNET different from VM Any Azure region Create VNET Peering between the two VNETs Default VNET Any Azure region Create VNET Peering between the two VNETs...", 
"body" : "Review VNET peering options with Azure VM. Workspace VNET Region VNET peering option VNET Injection VNET same as VM - - VNET Injection VNET different from VM Any Azure region Create VNET Peering between the two VNETs Default VNET Any Azure region Create VNET Peering between the two VNETs " }, 
{ "title" : "Install Unravel on Azure VM", 
"url" : "102700-az-databricks-install-unravel.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure Databricks \/ Install Unravel on Azure VM", 
"snippet" : "Install prerequisites on Azure VM Install ntpd . sudo su - yum install ntp ntpd -u ntp:ntp Prepare the data disk with at least 500 GB that was added to Azure VM. Use fdisk -l to check any 500 GB disk without partition (usually \/dev\/sdc ). List disks and partitions. fdisk -l fdisk -l \/dev\/sdc Format ...", 
"body" : "Install prerequisites on Azure VM Install ntpd . sudo su -\nyum install ntp\nntpd -u ntp:ntp Prepare the data disk with at least 500 GB that was added to Azure VM. Use fdisk -l to check any 500 GB disk without partition (usually \/dev\/sdc ). List disks and partitions. fdisk -l\nfdisk -l \/dev\/sdc Format the disk \/dev\/sdc . \/usr\/sbin\/mkfs -t ext4 \/dev\/sdc Mount the disk \/dev\/sdc on \/srv . mkdir -p \/srv\nDISKUUID=`\/usr\/sbin\/blkid |grep ext4 |grep sdc | awk '{ print $2}' |sed -e 's\/\"\/\/g'`\necho $DISKUUID\necho \"${DISKUUID} \/srv ext4 defaults 0 0\" >> \/etc\/fstab\ncat \/etc\/fstab\nmount \/dev\/sdc \/srv Verify the disk space. df -hT \/srv Set permissions for Unravel and symlink Unravel's directories to the \/srv mount. mkdir -p \/srv\/local\/unravel\nchmod -R 755 \/srv\/local\nln -s \/srv\/local\/unravel \/usr\/local\/unravel\nchmod 755 \/usr\/local\/unravel Install MySQL client. yum install mysql Install DBFS CLI . yum install https:\/\/dl.fedoraproject.org\/pub\/epel\/epel-release-latest-7.noarch.rpm \nsudo yum install python-pip\nsudo pip install databricks-cli Install Unravel for Azure VM Download latest 4.5.2.2 rpm for EMR wget --user user --password password https:\/\/preview.unraveldata.com\/unravel\/staging-RPM\/4.5.2\/ latest EMR rpm Install rpm sudo rpm -ivh latest EMR rpm 2> \/tmp\/rpm-install-log.txt " }, 
{ "title" : "Configure Azure Databricks Automated (Job) Clusters with Unravel", 
"url" : "102707-az-databricks-conf-with-unravel.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure Databricks \/ Configure Azure Databricks Automated (Job) Clusters with Unravel", 
"snippet" : "Sign in to the Azure portal . Add the following under Job > Configure Cluster > Spark >Spark Conf . This is generated from a Databricks setup script on Unravel. It needs to be copied on each Automated Clusters. Merge the configuration as needed. spark.executor.extraJavaOptions -Dcom.unraveldata.clie...", 
"body" : "Sign in to the Azure portal . Add the following under Job > Configure Cluster > Spark >Spark Conf . This is generated from a Databricks setup script on Unravel. It needs to be copied on each Automated Clusters. Merge the configuration as needed. spark.executor.extraJavaOptions -Dcom.unraveldata.client.rest.request.timeout.ms=1000 -Dcom.unraveldata.client.rest.conn.timeout.ms=1000 -javaagent:\/dbfs\/databricks\/unravel\/unravel-agent-pack-bin\/btrace-agent.jar=config=executor,libs=spark-2.3\n spark.driver.extraJavaOptions -Dcom.unraveldata.client.rest.request.timeout.ms=1000 -Dcom.unraveldata.client.rest.conn.timeout.ms=1000 -javaagent:\/dbfs\/databricks\/unravel\/unravel-agent-pack-bin\/btrace-agent.jar=config=driver,script=StreamingProbe.btclass,libs=spark-2.3\n spark.eventLog.enabled true\n spark.eventLog.dir dbfs:\/databricks\/unravel\/eventLogs\/\n spark.unravel.server.hostport Unravel VM Private IP Address spark.unravel.shutdown.delay.ms 300 Enable logging Job > Configure Cluster > Spark > Logging . Add the following under Job > Configure Cluster > Spark > Init Scripts . dbfs:\/databricks\/unravel\/unravel-db-sensor-archive\/dbin\/install-unravel.sh " }, 
{ "title" : "Running the Databricks_setup.sh script", 
"url" : "102640-az-databricks-init-setup-script.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure Databricks \/ Running the Databricks_setup.sh script", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Description", 
"url" : "102640-az-databricks-init-setup-script.html#UUID-1de33cb7-8a9f-c463-3d72-514eff16d552_section-5d2e420d8f357-idm45764257746864", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure Databricks \/ Running the Databricks_setup.sh script \/ Description", 
"snippet" : "This topic explains how to configure Unravel for Databricks using \/usr\/local\/unravel\/bin\/databricks_setup.sh on Unravel Server. This script initializes a given workspace and provides instructions required to set up a job cluster for Unravel to monitor. Run this script as the same username that you u...", 
"body" : "This topic explains how to configure Unravel for Databricks using \/usr\/local\/unravel\/bin\/databricks_setup.sh on Unravel Server. This script initializes a given workspace and provides instructions required to set up a job cluster for Unravel to monitor. Run this script as the same username that you used to install Unravel Server. " }, 
{ "title" : "Syntax", 
"url" : "102640-az-databricks-init-setup-script.html#UUID-1de33cb7-8a9f-c463-3d72-514eff16d552_section-5d2e421e4b9d1-idm45315545746416", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure Databricks \/ Running the Databricks_setup.sh script \/ Syntax", 
"snippet" : ".\/databricks_setup.sh [--add-workspace -i workspace-id -n workspace-name -s workspace-instance -t workspace-token -u unravel_server : port [ options ]] | [-h] | [-p -u unravel_server : port [ options ]] Options: Option Description --add-workspace | -a Sets up or updates a Databricks workspace for mo...", 
"body" : ".\/databricks_setup.sh [--add-workspace -i workspace-id -n workspace-name -s workspace-instance -t workspace-token -u unravel_server : port [ options ]] | [-h] | [-p -u unravel_server : port [ options ]]\n Options: Option Description --add-workspace | -a Sets up or updates a Databricks workspace for monitoring by Unravel. Valid values: -i ID of the workspace to be configured. -n Workspace name. -s Workspace instance. Must start with https:\/\/ . For example, https:\/\/eastus.databricks.com -t Personal access token for the workspace. -u IP address and port of Unravel Server. For example, 0.0.0.1:4043 -e Enables\/disables SSL for Databricks sensor and agent. Valid values: true , false . Default: false . -c (Optional) Enables\/disables SSL connections to Unravel endpoints without certificates. This option is only in effect if -e is set to true . Valid values: true , false . Default: false . -v (Optional) Spark version to be used. Default: 2.3 . -d (Optional) Enables debug logs for Unravel Databricks sensor installation. Default: false . -m (Optional) Specifies the frequency in seconds in which to poll cluster metrics. Default: 30 . --switch-to-db | -t Switch this cluster to Azure Databricks. --print-spark-conf | -p Print the minimal Spark configuration required to monitor cluster using Unravel. -u IP address and port of Unravel Server. For example, 0.0.0.1:4043 -e Enables\/disables SSL for Databricks sensor and agent. Valid values: true , false . Default: false . -c (Optional) Enables\/disables SSL connections to Unravel endpoints without certificates. This option is only in effect if -e is set to true . Valid values: true , false . Default: false . -v (Optional) Spark version to be used. Default: 2.3 . --help | -h Prints the usage of this script. If you generate new tokens, re-run this script to update Unravel Server. After running this script, restart Unravel services: service unravel_all.sh restart " }, 
{ "title" : "Examples", 
"url" : "102640-az-databricks-init-setup-script.html#UUID-1de33cb7-8a9f-c463-3d72-514eff16d552_section-5d2e422ddbb41-idm45731664086208", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure Databricks \/ Running the Databricks_setup.sh script \/ Examples", 
"snippet" : "To connect to a new workspace with SSL: .\/usr\/local\/unravel\/bin\/databricks_setup.sh --add-workspace -i 755550310254240 -n Engineering -s https:\/\/eastus2.azuredatabricks.net -t dapi490444403de2c1169e4667a3a02a694a -u azurevm001.unraveldata.com:4443 -e true Copying .. \/tmp\/7674350310254240.properties ...", 
"body" : "To connect to a new workspace with SSL: .\/usr\/local\/unravel\/bin\/databricks_setup.sh --add-workspace -i 755550310254240 -n Engineering -s https:\/\/eastus2.azuredatabricks.net -t dapi490444403de2c1169e4667a3a02a694a -u azurevm001.unraveldata.com:4443 -e true\n\nCopying .. \/tmp\/7674350310254240.properties to dbfs:\/databricks\/unravel\/unravel-db-sensor-archive\/etc\/unravel_db.properties\nCopied file successfully\nCopying .. \/tmp\/agent-pack to dbfs:\/databricks\/unravel\/unravel-agent-pack-bin\n\nCopied file successfully\nCopying .. \/tmp\/sensor_pack to dbfs:\/databricks\/unravel\/unravel-db-sensor-archive\nCopied file successfully\nswitch_to_databricks action already done. No action taken\n\n-----------------------------------\n Cluster Spark Configuration\n-----------------------------------\nspark.executor.extraJavaOptions -Dcom.unraveldata.client.rest.ssl.enabled=true -Dcom.unraveldata.ssl.insecure=true -Dcom.unraveldata.client.rest.request.timeout.ms=1000 -Dcom.unraveldata.client.rest.conn.timeout.ms=1000 -javaagent:\/dbfs\/databricks\/unravel\/unravel-agent-pack-bin\/btrace-agent.jar=config=executor,libs=spark-2.3\nspark.eventLog.enabled true\nspark.unravel.server.hostport azurevm001.unraveldata.com:4443\nspark.driver.extraJavaOptions -Dcom.unraveldata.client.rest.ssl.enabled=true -Dcom.unraveldata.ssl.insecure=true -Dcom.unraveldata.client.rest.request.timeout.ms=1000 -Dcom.unraveldata.client.rest.conn.timeout.ms=1000 -javaagent:\/dbfs\/databricks\/unravel\/unravel-agent-pack-bin\/btrace-agent.jar=config=driver,libs=spark-2.3\nspark.eventLog.dir dbfs:\/databricks\/unravel\/eventLogs\/\nspark.unravel.shutdown.delay.ms 300\n\n-----------------------------------\n Spark Submit Parameters\n-----------------------------------\n\"--conf\", \"spark.executor.extraJavaOptions=-Dcom.unraveldata.client.rest.ssl.enabled=true -Dcom.unraveldata.ssl.insecure=true -Dcom.unraveldata.client.rest.request.timeout.ms=1000 -Dcom.unraveldata.client.rest.conn.timeout.ms=1000 -javaagent:\/dbfs\/databricks\/unravel\/unravel-agent-pack-bin\/btrace-agent.jar=config=executor,libs=spark-2.3\",\n\"--conf\", \"spark.eventLog.enabled=true\",\n\"--conf\", \"spark.unravel.server.hostport=azurevm001.unraveldata.com:4443\",\n\"--conf\", \"spark.driver.extraJavaOptions=-Dcom.unraveldata.client.rest.ssl.enabled=true -Dcom.unraveldata.ssl.insecure=true -Dcom.unraveldata.client.rest.request.timeout.ms=1000 -Dcom.unraveldata.client.rest.conn.timeout.ms=1000 -javaagent:\/dbfs\/databricks\/unravel\/unravel-agent-pack-bin\/btrace-agent.jar=config=driver,libs=spark-2.3\",\n\"--conf\", \"spark.eventLog.dir=dbfs:\/databricks\/unravel\/eventLogs\/\",\n\"--conf\", \"spark.unravel.shutdown.delay.ms=300\"\n\n-----------------------------------\n Databricks Cluster Init Script\n-----------------------------------\ndbfs:\/databricks\/unravel\/unravel-db-sensor-archive\/dbin\/install-unravel.sh\n\n-----------------------------------\n Restart unravel daemons\n-----------------------------------\nservice unravel_all.sh restart\n------------------------------------ \n To connect to a new workspace with no SSL: \/usr\/local\/unravel\/bin\/databricks_setup.sh --add-workspace -i 2982641413555431 -n Finance -s https:\/\/eastus.azuredatabricks.net -t dapiba64a058e8d2e07156288314305eqassw23 -u azurevm001.unraveldata.com:4043\n\nCopying .. \/tmp\/2982641413551061.properties to dbfs:\/databricks\/unravel\/unravel-db-sensor-archive\/etc\/unravel_db.properties\nCopied file successfully\nCopying .. \/tmp\/agent-pack to dbfs:\/databricks\/unravel\/unravel-agent-pack-bin\nCopied file successfully\nCopying .. \/tmp\/sensor_pack to dbfs:\/databricks\/unravel\/unravel-db-sensor-archive\nCopied file successfully\nswitch_to_databricks action already done. No action taken\n\n-----------------------------------\nCluster Spark Configuration\n-----------------------------------\nspark.executor.extraJavaOptions -Dcom.unraveldata.client.rest.request.timeout.ms=1000 -Dcom.unraveldata.client.rest.conn.timeout.ms=1000 -javaagent:\/dbfs\/databricks\/unravel\/unravel-agent-pack-bin\/btrace-agent.jar=config=executor,libs=spark-2.3\nspark.eventLog.enabled true\nspark.unravel.server.hostport azurevm001.unraveldata.com:4043\nspark.driver.extraJavaOptions -Dcom.unraveldata.client.rest.request.timeout.ms=1000 -Dcom.unraveldata.client.rest.conn.timeout.ms=1000 -javaagent:\/dbfs\/databricks\/unravel\/unravel-agent-pack-bin\/btrace-agent.jar=config=driver,libs=spark-2.3\nspark.eventLog.dir dbfs:\/databricks\/unravel\/eventLogs\/\nspark.unravel.shutdown.delay.ms 300\n\n-----------------------------------\nSpark Submit Parameters\n-----------------------------------\n\"--conf\", \"spark.executor.extraJavaOptions= -Dcom.unraveldata.client.rest.request.timeout.ms=1000 -Dcom.unraveldata.client.rest.conn.timeout.ms=1000 -javaagent:\/dbfs\/databricks\/unravel\/unravel-agent-pack-bin\/btrace-agent.jar=config=executor,libs=spark-2.3\",\n\"--conf\", \"spark.eventLog.enabled=true\",\n\"--conf\", \"spark.unravel.server.hostport=azurevm001.unraveldata.com:4043\",\n\"--conf\", \"spark.driver.extraJavaOptions= -Dcom.unraveldata.client.rest.request.timeout.ms=1000 -Dcom.unraveldata.client.rest.conn.timeout.ms=1000 -javaagent:\/dbfs\/databricks\/unravel\/unravel-agent-pack-bin\/btrace-agent.jar=config=driver,libs=spark-2.3\",\n\"--conf\", \"spark.eventLog.dir=dbfs:\/databricks\/unravel\/eventLogs\/\",\n\"--conf\", \"spark.unravel.shutdown.delay.ms=300\"\n\n-----------------------------------\nDatabricks Cluster Init Script\n-----------------------------------\ndbfs:\/databricks\/unravel\/unravel-db-sensor-archive\/dbin\/install-unravel.sh\n\n-----------------------------------\n Restart unravel daemons\n-----------------------------------\nservice unravel_all.sh restart\n------------------------------------\n To print the configuration: \/usr\/local\/unravel\/bin\/databricks_setup.sh -p -u azurevm001.unraveldata.com:4443 -e true\n-----------------------------------\nCluster Spark Configuration\n-----------------------------------\nspark.executor.extraJavaOptions -Dcom.unraveldata.client.rest.ssl.enabled=true -Dcom.unraveldata.ssl.insecure=true -Dcom.unraveldata.client.rest.request.timeout.ms=1000 -Dcom.unraveldata.client.rest.conn.timeout.ms=1000 -javaagent:\/dbfs\/databricks\/unravel\/unravel-agent-pack-bin\/btrace-agent.jar=config=executor,libs=spark-2.3\nspark.eventLog.enabled true\nspark.unravel.server.hostport azurevm001.unraveldata.com:4443\nspark.driver.extraJavaOptions -Dcom.unraveldata.client.rest.ssl.enabled=true -Dcom.unraveldata.ssl.insecure=true -Dcom.unraveldata.client.rest.request.timeout.ms=1000 -Dcom.unraveldata.client.rest.conn.timeout.ms=1000 -javaagent:\/dbfs\/databricks\/unravel\/unravel-agent-pack-bin\/btrace-agent.jar=config=driver,libs=spark-2.3\nspark.eventLog.dir dbfs:\/databricks\/unravel\/eventLogs\/\nspark.unravel.shutdown.delay.ms 300\n\n-----------------------------------\nSpark Submit Parameters\n-----------------------------------\n\"--conf\", \"spark.executor.extraJavaOptions=-Dcom.unraveldata.client.rest.ssl.enabled=true -Dcom.unraveldata.ssl.insecure=true -Dcom.unraveldata.client.rest.request.timeout.ms=1000 -Dcom.unraveldata.client.rest.conn.timeout.ms=1000 -javaagent:\/dbfs\/databricks\/unravel\/unravel-agent-pack-bin\/btrace-agent.jar=config=executor,libs=spark-2.3\",\n\"--conf\", \"spark.eventLog.enabled=true\",\n\"--conf\", \"spark.unravel.server.hostport=azurevm001.unraveldata.com:4443\",\n\"--conf\", \"spark.driver.extraJavaOptions=-Dcom.unraveldata.client.rest.ssl.enabled=true -Dcom.unraveldata.ssl.insecure=true -Dcom.unraveldata.client.rest.request.timeout.ms=1000 -Dcom.unraveldata.client.rest.conn.timeout.ms=1000 -javaagent:\/dbfs\/databricks\/unravel\/unravel-agent-pack-bin\/btrace-agent.jar=config=driver,libs=spark-2.3\",\n\"--conf\", \"spark.eventLog.dir=dbfs:\/databricks\/unravel\/eventLogs\/\",\n\"--conf\", \"spark.unravel.shutdown.delay.ms=300\"\n\n-----------------------------------\nDatabricks Cluster Init Script\n-----------------------------------\ndbfs:\/databricks\/unravel\/unravel-db-sensor-archive\/dbin\/install-unravel.sh\n\n-----------------------------------\nRestart unravel daemons\n-----------------------------------\nservice unravel_all.sh restart\n------------------------------------ \n " }, 
{ "title" : "Uninstalling Unravel Server and Sensors on Azure Databricks", 
"url" : "102641-az-databricks-uninstall.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Platforms \/ Microsoft Azure Databricks \/ Uninstalling Unravel Server and Sensors on Azure Databricks", 
"snippet" : "Delete the Unravel installation location on DBFS for each workspace where Unravel is deployed using DBFS CLI: dbfs rm -r dbfs:\/databricks\/unravel For a list of Databricks workspaces configured, see \/usr\/local\/unravel\/etc\/unravel.properties ....", 
"body" : "Delete the Unravel installation location on DBFS for each workspace where Unravel is deployed using DBFS CLI: dbfs rm -r dbfs:\/databricks\/unravel For a list of Databricks workspaces configured, see \/usr\/local\/unravel\/etc\/unravel.properties . " }, 
{ "title" : "MySQL", 
"url" : "102115-install-mysql.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ MySQL", 
"snippet" : "You must install the MySQL Unravel DB on a separate node from the Unravel server. This topic explains how to install MySQL on a separate node. Installation involves: Installation before the Unravel RPM is installed. Configuration after the Unravel RPM is installed. Installation instructions for plat...", 
"body" : "You must install the MySQL Unravel DB on a separate node from the Unravel server. This topic explains how to install MySQL on a separate node. Installation involves: Installation before the Unravel RPM is installed. Configuration after the Unravel RPM is installed. Installation instructions for platform specific MySQL Setting up Azure MySQL for Unravel Hardware requirements for a dedicated MySQL node Jobs per day Data retention length Cores RAM Disk Less than 50,000 30 days 4 32 GB 1 TB 60 days 4 32 GB 2 TB 50,000 to 100,000 to 30 days 8 64 GB 2 TB 60 days 8 64 GB 4 TB Over 100,000 Contact Unravel Support If you are forced to put Unravel server and MySQL on the same node then the server should meet the following minimal specifications. Unravel does not recommend this type of deployment and strongly discourages your doing so. Hardware requirements using only one node for Unravel server and MySQL. Jobs per day Data retention length Cores RAM Disk Less than 50,000 30 days 8 128 GB 1 TB 60 days 8 128 GB 2 TB 50,000 to 100,000 to 30 days 16 192 GB 2 TB 60 days 16 192 GB 4 TB Over 100,000 Contact Unravel Support " }, 
{ "title" : "Installing MySQL", 
"url" : "102116-mysql-install.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ MySQL \/ Installing MySQL", 
"snippet" : "MySQL is installed before the Unravel RPM. After the RPM installation, you then configure MySQL. 1. Install a compatible version of MySQL server and database . compmatrix-platform On CentOS 6: wget https:\/\/dev.mysql.com\/get\/mysql80-community-release-el6-1.noarch.rpm sudo yum install yum-utils sudo r...", 
"body" : "MySQL is installed before the Unravel RPM. After the RPM installation, you then configure MySQL. 1. Install a compatible version of MySQL server and database . compmatrix-platform On CentOS 6: wget https:\/\/dev.mysql.com\/get\/mysql80-community-release-el6-1.noarch.rpm\nsudo yum install yum-utils\nsudo rpm -ivh mysql80-community-release-el6-1.noarch.rpm\nsudo yum-config-manager --disable mysql80-community\nsudo yum-config-manager --enable mysql57-community\nsudo yum install mysql-community-server On CentOS 7: wget https:\/\/dev.mysql.com\/get\/mysql80-community-release-el7-1.noarch.rpm\nsudo rpm -ivh mysql80-community-release-el7-1.noarch.rpm\nsudo yum-config-manager --disable mysql80-community\nsudo yum-config-manager --enable mysql57-community\nsudo yum install mysql-community-server On SELinux: If you are installing MySQL on an SELinux host and are not using the default datadir , see . 2. Configure and start MySQL server. If the MySQL server is running, stop it and delete the old InnoDB log files. If you need to save your old InnoDB log files copy them to a directory of your choosing, backup-path before deletion. sudo service mysqld stop\ncp \/var\/lib\/mysql\/ib_logfile* backup-path \nrm -rf \/var\/lib\/mysql\/ib_logfile* In the MySQL configuration file (typically \/etc\/my.cnf ), append the following properties at the end of the [mysqld] section. Set datadir to the directory your choice. This directory must have a minimum capacity of 500GB. key_buffer_size = 256M\nmax_allowed_packet = 32M\nsort_buffer_size = 32M\n # Do not add query_cache_size when using MySQL 8 as it is no longer valid \nquery_cache_size = 64M\nmax_connections = 500\nmax_connect_errors = 2000000000\nopen_files_limit = 10000\nport-open-timeout = 121\nexpire-logs-days = 1\ncharacter_set_server = utf8\ncollation_server = utf8_unicode_ci\ninnodb_open_files = 2000\ninnodb_file_per_table = 1\ninnodb_data_file_path = ibdata1:100M:autoextend\n # The innodb_buffer_pool_size depends on load and cluster size.\n# On a dedicated machine, it can be 50% of the RAM size.\n# Using 1G is the absolute minimum. For a large cluster, we use 48G. \ninnodb_buffer_pool_size = 4G\ninnodb_flush_method = O_DIRECT\ninnodb_log_file_size = 256M\ninnodb_log_buffer_size = 64M\ninnodb_flush_log_at_trx_commit = 2\ninnodb_lock_wait_timeout = 50\ninnodb_thread_concurrency = 20\ninnodb_read_io_threads = 16\ninnodb_write_io_threads = 4\nbinlog_format = mixed\n # if SSD disk is used uncomment the line below \n#innodb_io_capacity = 4000 Ensure MySQL server starts at boot time and then start the MySQL server On CentOS 6: sudo chkconfig mysqld on\nsudo service mysqld start On CentOS 7: sudo systemctl enable mysqld\nsudo systemctl start mysqld Check the used and free space on the volume specified by MySQL's datadir property in the MySQL configuration file (typically \/etc\/my.cnf ): Used and Free Space sudo grep \"^datadir=\" \/etc\/my.cnf | awk -F\"[=]\" '{print $2}' | sudo xargs du -sh\nsudo grep \"^datadir=\" \/etc\/my.cnf | awk -F\"[=]\" '{print $2}' | sudo xargs df -h Steps 3 and 4 must be run on the Unravel server. 3. Install the MySQL JDBC driver. Download the JDBC driver for MySQL to \/tmp . Then navigate to \/tmp and extract the driver. wget https:\/\/dev.mysql.com\/get\/Downloads\/Connector-J\/mysql-connector-java-5.1.47.tar.gz -O \/tmp\/mysql-connector-java-5.1.47.tar.gz\ncd \/tmp\ntar xvzf \/tmp\/mysql-connector-java-5.1.47.tar.gz 4. Install MySQL. yum install mysql " }, 
{ "title" : "Configuring MySQL", 
"url" : "102117-mysql-configure.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ MySQL \/ Configuring MySQL", 
"snippet" : "After installing the Unravel RPM, complete the following steps. 1. Copy the MySQL JDBC JAR to \/usr\/local\/unravel\/share\/java\/ . sudo mkdir -p \/usr\/local\/unravel\/share\/java sudo cp \/tmp\/mysql-connector-java-5.1.47\/mysql-connector-java-5.1.47.jar \/usr\/local\/unravel\/share\/java sudo cp \/tmp\/mysql-connect...", 
"body" : "After installing the Unravel RPM, complete the following steps. 1. Copy the MySQL JDBC JAR to \/usr\/local\/unravel\/share\/java\/ . sudo mkdir -p \/usr\/local\/unravel\/share\/java\nsudo cp \/tmp\/mysql-connector-java-5.1.47\/mysql-connector-java-5.1.47.jar \/usr\/local\/unravel\/share\/java\nsudo cp \/tmp\/mysql-connector-java-5.1.47\/mysql-connector-java-5.1.47.jar \/usr\/local\/unravel\/dlib\/unravel 2. Configure Unravel to connect to the MySQL server. Using mysql , create a database and user for Unravel. mysql\nmysql> CREATE DATABASE unravel_mysql_prod;\nmysql> CREATE USER 'unravel'@'localhost' IDENTIFIED BY ' password ';\nmysql> GRANT ALL PRIVILEGES ON unravel_mysql_prod.* TO 'unravel'@'localhost'; In \/usr\/local\/unravel\/etc\/unravel.properties , update the following properties: These properties define the connection to Unravel's MySQL database. Property\/Description Set by user Unit Default unravel.jdbc.username Unravel database user. Required string - unravel.jdbc.password Password for unravel.jdbc.username . Required string - unravel.jdbc.url URL for jdbc, determined by your database. Example: jdbc:mysql:\/\/127.0.0.1:3306\/unravel_mysql_prod Required string (path) - For example, unravel.jdbc.username=unravel\nunravel.jdbc.password= password \nunravel.jdbc.url=jdbc:mysql:\/\/127.0.0.1:3306\/unravel_mysql_prod If you installed MySQL 8.0, add: unravel.jdbc.url.params=disableMariaDbDriver Create a schema for Unravel tables. sudo \/usr\/local\/unravel\/dbin\/db_schema_upgrade.sh Create the default admin user for Unravel UI. sudo \/usr\/local\/unravel\/install_bin\/db_initial_inserts.sh | sudo \/usr\/local\/unravel\/install_bin\/db_access.sh " }, 
{ "title" : "Setting up Azure MySQL for Unravel (Optional)", 
"url" : "102118-azure-mysql.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ MySQL \/ Setting up Azure MySQL for Unravel (Optional)", 
"snippet" : "Step 1: Create Azure MySQL 5.7 instance. We recommend using Memory Optimized type. Step 2: Disable SSL Enforce in Azure MySQL instance. Step 3: (Optional) Update Azure MySQL service parameters. Tune the following MySQL parameters for better performance for Unravel. max_allowed_packet: 32000000 and b...", 
"body" : "Step 1: Create Azure MySQL 5.7 instance. We recommend using Memory Optimized type. Step 2: Disable SSL Enforce in Azure MySQL instance. Step 3: (Optional) Update Azure MySQL service parameters. Tune the following MySQL parameters for better performance for Unravel. max_allowed_packet: 32000000 and beyond\nsort_buffer_size: 32000000 and beyond or maximum allowed value\nquery_cache_size: 64000000 and beyond or maximum allowed value\nmax_connections: 500 and beyond\nmax_connect_errors: 2000000000 and beyond\ncharacter_set_server: UTF8\ninnodb_file_per_table: ON\ninnodb_buffer_pool_size: maximum allowed value\ninnodb_lock_wait_timeout: 50\ninnodb_thread_concurrency: 20\ninnodb_read_io_threads: 16\ninnodb_write_io_threads: 4\ninnodb_io_capacity: 4000\ninnodb_io_capacity_max: 4000 Step 4: Set up VNET rule to allow Unravel server to access the database via port 3306. Select the subscription, Virtual Network, Subnet that Unravel server located and click OK . Step 6: Update \/usr\/local\/unravel\/etc\/unravel.properties with the JDBC URL to Azure MySQL instance. Check MySQL JDBC driver is located in Unravel Java classpath \/usr\/local\/unravel\/share\/java\/ wget https:\/\/dev.mysql.com\/get\/Downloads\/Connector-J\/mysql-connector-java-5.1.47.tar.gz -O \/tmp\/mysql-connector-java-5.1.47.tar.gz\ncd \/tmp\ntar xvzf \/tmp\/mysql-connector-java-5.1.47.tar.gz\nsudo mkdir -p \/usr\/local\/unravel\/share\/java\nsudo cp \/tmp\/mysql-connector-java-5.1.47\/mysql-connector-java-5.1.47.jar \/usr\/local\/unravel\/share\/java\nsudo cp \/tmp\/mysql-connector-java-5.1.47\/mysql-connector-java-5.1.47.jar \/usr\/local\/unravel\/dlib\/unravel Construct the JDBC URL for Unravel. Modify above properties in \/usr\/local\/unravel\/etc\/unravel.properties . Example: unravel.jdbc.username=<Server admin login name>\nunravel.jdbc.password=<admin password>\nunravel.jdbc.url=jdbc:mysql:\/\/<Server name>:3306\/unravel_mysql_prod\nunravel.jdbc.url.params=useSSL=true&requireSSL=false Restart Unravel daemons. \/etc\/init.d\/unravel_all.sh restart Create database\/tables for Unravel mysql --host= Server name --port=3306 --user Server admin login name -p admin password -e 'create database unravel_mysql_prod'\/usr\/local\/unravel\/dbin\/db_schema_upgrade.sh Create the default admin user for Unravel UI. sudo \/usr\/local\/unravel\/install_bin\/db_initial_inserts.sh | sudo \/usr\/local\/unravel\/install_bin\/db_access.sh " }, 
{ "title" : "Step 5: Log into the Unravel node and install the MySQL Client.", 
"url" : "102118-azure-mysql.html#UUID-6e62c60a-461d-e776-c7b6-0bd0a684cc5c_section-5dca33f7339b1-idm46079258586160", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ MySQL \/ Setting up Azure MySQL for Unravel (Optional) \/ Step 5: Log into the Unravel node and install the MySQL Client.", 
"snippet" : "sudo yum install mysql...", 
"body" : "sudo yum install mysql " }, 
{ "title" : "Add-ons", 
"url" : "102119-install-addons.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Add-ons", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "OnDemand", 
"url" : "102120-install-ondemand.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Add-ons \/ OnDemand", 
"snippet" : "This section contains installation and upgrade instructions for the OnDemand service. If you want Unravel to generate any of the following reports, your Unravel DB must be MySQL and install this service on Unravel Server's host: Sessions File Reports Forecasting Small Files Cluster Optimization Clus...", 
"body" : "This section contains installation and upgrade instructions for the OnDemand service. If you want Unravel to generate any of the following reports, your Unravel DB must be MySQL and install this service on Unravel Server's host: Sessions File Reports Forecasting Small Files Cluster Optimization Cluster KPIS Queue Analysis Top X (including User Reports) Cluster Discovery Cloud Mapping Per Host Services and Versions Compatibility " }, 
{ "title" : "Installing or upgrading OnDemand", 
"url" : "102121-ondemand-details.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Add-ons \/ OnDemand \/ Installing or upgrading OnDemand", 
"snippet" : "The OnDemand service works with MySQL only. Start the OnDemand service If your HiveServer2 is kerberized, you should have already added the unravel user to it. Use the unravel user and its KRB ticket to run the commands below. Execute the following four commands in the order shown, replacing run-as-...", 
"body" : "The OnDemand service works with MySQL only. Start the OnDemand service If your HiveServer2 is kerberized, you should have already added the unravel user to it. Use the unravel user and its KRB ticket to run the commands below. Execute the following four commands in the order shown, replacing run-as-user and run-as-group with the username and group you used when running switch_to_user during Unravel server installation. For RHEL 7.x: sudo systemctl stop unravel_ondemand.service\nsudo chown -RL run-as-user : run-as-group \/usr\/local\/unravel\/ondemand\/\nsudo service unravel_all.sh restart\nsudo systemctl start unravel_ondemand.service For RHEL 6.x: sudo service unravel_ondemand stop\nsudo chown -RL run-as-user : run-as-group \/usr\/local\/unravel\/ondemand\/\nsudo service unravel_all.sh restart\nsudo service unravel_ondemand start Confirm that the OnDemand service is running: You should see output similar to the example below. Process IDs vary based on the number of processors in Unravel Server, number of current tasks, and so on. For RHEL 7.x: sudo systemctl status unravel_ondemand.service For RHEL 6.x: sudo service unravel_ondemand status You can also use ps command. You should see output similar to the example below. ps -ef | grep ondemand | grep -v grep root 11159 1 0 Sep21 ? 00:00:00 su - -c bash -c cd \/usr\/local\/unravel\/ondemand\/; nohup unravel-python-*\/bin\/unravel_on_demand_start.sh root 11163 11159 0 Sep21 ? 00:00:00 -bash -c cd \/usr\/local\/unravel\/ondemand\/; nohup unravel-python-*\/bin\/unravel_on_demand_start.sh root 11450 11176 0 Sep21 ? 00:00:42 \/usr\/local\/unravel\/ondemand\/python\/bin\/python2.7 \/usr\/local\/unravel\/ondemand\/python\/\/bin\/flask run hdfs 11452 11176 0 Sep21 ? 00:27:19 \/usr\/local\/unravel\/ondemand\/python\/bin\/python2.7 \/usr\/local\/unravel\/ondemand\/python\/\/bin\/celery -A celery_tasks worker --app=celery_tasks.celeryapp.celery --uid hdfs --gid hdfs --loglevel=info --workdir=\/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/conf\/.. --autoscale=24,3 hdfs 11670 11452 0 Sep21 ? 00:00:00 \/usr\/local\/unravel\/ondemand\/python\/bin\/python2.7 \/usr\/local\/unravel\/ondemand\/python\/\/bin\/celery -A celery_tasks worker --app=celery_tasks.celeryapp.celery --uid hdfs --gid hdfs --loglevel=info --workdir=\/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/conf\/.. --autoscale=24,3 hdfs 11671 11452 0 Sep21 ? 00:00:00 \/usr\/local\/unravel\/ondemand\/python\/bin\/python2.7 \/usr\/local\/unravel\/ondemand\/python\/\/bin\/celery -A celery_tasks worker --app=celery_tasks.celeryapp.celery --uid hdfs --gid hdfs --loglevel=info --workdir=\/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/conf\/.. --autoscale=24,3 hdfs 11672 11452 0 Sep21 ? 00:00:00 \/usr\/local\/unravel\/ondemand\/python\/bin\/python2.7 \/usr\/local\/unravel\/ondemand\/python\/\/bin\/celery -A celery_tasks worker --app=celery_tasks.celeryapp.celery --uid hdfs --gid hdfs --loglevel=info --workdir=\/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/conf\/.. --autoscale=24,3 " }, 
{ "title" : "Install MySQL on the Unravel host", 
"url" : "102121-ondemand-details.html#UUID-d8b8e8ca-9303-fb6e-9a4a-48ef011950db_N1557422693732", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Add-ons \/ OnDemand \/ Installing or upgrading OnDemand \/ Install MySQL on the Unravel host", 
"snippet" : "Install MySQL . Verify that MySQL is installed correctly by running netstats . The output should be as shown below. netstat -tunlp | grep :3306 tcp6 0 0 :::3306 :::* LISTEN 28006\/mysqld Run db_access.sh to verify that the ondemand_tasks , ondemand_sessions , and report_instances tables have been cre...", 
"body" : "Install MySQL . Verify that MySQL is installed correctly by running netstats . The output should be as shown below. netstat -tunlp | grep :3306\ntcp6 0 0 :::3306 :::* LISTEN 28006\/mysqld Run db_access.sh to verify that the ondemand_tasks , ondemand_sessions , and report_instances tables have been created: sudo \/usr\/local\/unravel\/install_bin\/db_access.sh\nmysql> show tables; " }, 
{ "title" : "Install required packages on the Unravel host", 
"url" : "102121-ondemand-details.html#UUID-d8b8e8ca-9303-fb6e-9a4a-48ef011950db_section-5cfffc44075d9-dm45764278769344", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Add-ons \/ OnDemand \/ Installing or upgrading OnDemand \/ Install required packages on the Unravel host", 
"snippet" : "Upgrades Use rpm -q to see what packages are installed. It returns a list showing what packages are installed and which are not. Install the uninstalled packages. rpm -q cyrus-sasl cyrus-sasl-devel cyrus-sasl-gssapi cyrus-sasl-lib cyrus-sasl-plain python-saslwrapper openssl tkinter For example Check...", 
"body" : "Upgrades Use rpm -q to see what packages are installed. It returns a list showing what packages are installed and which are not. Install the uninstalled packages. rpm -q cyrus-sasl cyrus-sasl-devel cyrus-sasl-gssapi cyrus-sasl-lib cyrus-sasl-plain python-saslwrapper openssl tkinter For example Check whether cyrus-sasl , open-ssl , and python are installed. rpm -q cyrus-sasl openssl python\ncyrus-sasl-2.1.26-23.el7.x86_64\npackage openssl is not installed\npython-2.7.5-80.el7_6.x86_64 open-ssl is not installed, so install it. yum install openssl Verify open-ssl is installed. rpm -q openssl If you had to install tkinter and it failed to install, install it manually. wget https:\/\/www.rpmfind.net\/linux\/centos\/7.7.1908\/os\/x86_64\/Packages\/tk-8.5.13-6.el7.x86_64.rpm\nyum install tk-8.5.13-6.el7.x86_64.rpm\nsystemctl restart unravel_ondemand.service If you have Small Files and Files Report enabled see Settings for Sentry-Secured CDH Cluster for necessary updates. Fresh installs Use the yum install command to install these packages: cyrus-sasl cyrus-sasl-devel cyrus-sasl-gssapi cyrus-sasl-lib cyrus-sasl-plain python-saslwrapper saslwrapper saslwrapper-devel openssl tkinter yum install cyrus-sasl cyrus-sasl-devel cyrus-sasl-gssapi cyrus-sasl-lib cyrus-sasl-plain python-saslwrapper openssl tkinter Verify the packages have been installed. rpm -q cyrus-sasl cyrus-sasl-devel cyrus-sasl-gssapi cyrus-sasl-lib cyrus-sasl-plain python-saslwrapper openssl tkinter If yum install tkinter fails, install it manually: wget https:\/\/www.rpmfind.net\/linux\/centos\/7.7.1908\/os\/x86_64\/Packages\/tk-8.5.13-6.el7.x86_64.rpm\nyum install tk-8.5.13-6.el7.x86_64.rpm\nsystemctl restart unravel_ondemand.service " }, 
{ "title" : "Install the OnDemand service on the Unravel host", 
"url" : "102121-ondemand-details.html#UUID-d8b8e8ca-9303-fb6e-9a4a-48ef011950db_N1557422760344", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Add-ons \/ OnDemand \/ Installing or upgrading OnDemand \/ Install the OnDemand service on the Unravel host", 
"snippet" : "Download the OnDemand package to the \/tmp directory on the Unravel host. downloads If \/usr\/local\/unravel\/ondemand exists, save a copy of your (possibly customized) hive_properties.hive , and then delete \/usr\/local\/unravel\/ondemand . cd \/usr\/local\/unravel mv ondemand\/unravel-python-1.0.0\/scripts\/hive...", 
"body" : "Download the OnDemand package to the \/tmp directory on the Unravel host. downloads If \/usr\/local\/unravel\/ondemand exists, save a copy of your (possibly customized) hive_properties.hive , and then delete \/usr\/local\/unravel\/ondemand . cd \/usr\/local\/unravel\nmv ondemand\/unravel-python-1.0.0\/scripts\/hive_queries_reporting\/hive_properties.hive \/tmp\/hive_properties.hive\nsudo rm -rf ondemand In the \/tmp directory, extract the contents of the tarball. cd \/tmp\ntar xvf ondemand- version -rhel[6|7].tar.gz This creates a new directory, \/tmp\/ondemand . Move the new directory, \/tmp\/ondemand , to \/usr\/local\/unravel . sudo mv \/tmp\/ondemand\/ \/usr\/local\/unravel\/\n Run the OnDemand installation script. cd \/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\nsudo .\/install\/ondemand_quick_install.sh\nsudo \/etc\/init.d\/unravel_all.sh restart (Optional) If you want Unravel to generate Small Files reports and you've customized hive_properties.hive , copy your customized hive_properties.hive to the ondemand directory: cp \/tmp\/hive_properties.hive \/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/scripts\/hive_queries_reporting\/hive_properties.hive If your HiveServer2 is kerberized, add the unravel user to it. In \/usr\/local\/unravel\/etc\/unravel.properties , specify the required properties for OnDemand: General Property\/Description Set by user Unit Default unravel.server.ip FQDN or IP-Address of Unravel Server Required string - com.unraveldata.python.enabled Enable\/disable all ondemand reports and Sessions features in UI. (This property is configured during the OnDemand installation.) boolean true HiveServer2 Property\/Description Set by user Unit Default unravel.hive.server2.host FQDN or IP-Address of the HiveServer2 instance. string - unravel.hive.server2.port Port for the HiveServer2 instance. number 10000 Celery Property\/Description Set by user Unit Default com.unraveldata.ngui.proxy.celery string (URL) http:\/\/localhost:5000 unravel.celery.broker.url Optional string (URL) - unravel.celery.result.backend Optional - Hive Metastore Property\/Description Set by user Unit Default javax.jdo.option.ConnectionDriverName JDBC Driver class name for the data store containing the metadata. Examples: MySQL: com.mysql.jdbc.Driver Oracle: oracle.jdbc.driver.OracleDriver Microsoft: com.microsoft.sqlserver.jdbc.SQLServerDriver Required string - javax.jdo.option.ConnectionPassword Password used to access the data store. Required string - javax.jdo.option.ConnectionUserName Username used to access the data store. Required string - javax.jdo.option.ConnectionURL JDBC connection string for the data store containing the metadata of the form: jdbc: DB_Driver :\/\/ HOST : PORT \/hive Example: Oracle: jdbc:oracle:thin:@prodHost:1521:ORCL Microsoft: jdbc:sqlserver:\/\/ jdbc_url Requited string (URL) - If KERBEROS ( unravel.hive.server2.authentication =KERBEROS) set unravel.hive.server2.kerberos.service.name =hive define Kerberos properties . See Configuring Forecasting and Migration Planning Reports for required cluster configurations. See for complete configuration requirements. Restart Unravel Server. sudo \/etc\/init.d\/unravel_all.sh restart If your host operating system is running a security-enhanced Linux (SELinux), you might get alerts like these after restarting Unravel Server, depending on your environment. You can ignore them: sealert -a \/var\/log\/audit\/audit.log\nAlert 1: SELinux is preventing \/usr\/bin\/bash from using the rlimitinh access on a process.\nAlert 2: SELinux is preventing \/usr\/bin\/python2.7 from using the rlimitinh access on a process. If you encounter problems, contact Unravel Support . " }, 
{ "title" : "Enable and configure OnDemand-based reports and features", 
"url" : "102121-ondemand-details.html#UUID-d8b8e8ca-9303-fb6e-9a4a-48ef011950db_section-5d2cb0ac311f5-idm45764258368480", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Add-ons \/ OnDemand \/ Installing or upgrading OnDemand \/ Enable and configure OnDemand-based reports and features", 
"snippet" : "Migration Planning is disabled by default. To enable, set unravel.python.reporting.cloudreport.enable to true . Migration Planning and Forecasting (enabled by default) both require extensive configuration . Small Files reports and File reports are enabled by default but require extensive configurati...", 
"body" : "Migration Planning is disabled by default. To enable, set unravel.python.reporting.cloudreport.enable to true . Migration Planning and Forecasting (enabled by default) both require extensive configuration . Small Files reports and File reports are enabled by default but require extensive configuration . Some reconfiguration is necessary when upgrading the OnDemand package. Cluster KPIS, Cluster Optimization, Queue Analysis, Sessions, and Top X reports are enabled by default and don't need configuration. Sessions is enabled by default. However, prior to 4.5.1 Sessions was disabled by default and its properites had no defaults. For installations of Unravel 4.5.0.x and earlier to enable see OnDemand Configurations . Edit unravel.properties and set all the Session properties to the default values listed. See OnDemand Configurations for all properties which affect the OnDemand reports, including enabling and disabling the various reports. " }, 
{ "title" : "Library versions and licensing for OnDemand", 
"url" : "102122-ondemand-libraries-licensing.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Add-ons \/ OnDemand \/ Library versions and licensing for OnDemand", 
"snippet" : "Library Licensing MySQL-python>=1.2.5 GPL Celery>=4.1.0 BSD pyhive>=0.5.1 Apache 2.0 sasl>=0.2.1 Apache License, Version 2.0 Thrift>=0.11.0 Apache License, Version 2.0 pandas>=0.20.3 BSD Flask>=0.12.2 BSD cm_api>=16.0.0 Apache 2.0 six>=1.10.0 MIT matplotlib>=2.1.0 BSD pystan>=2.16 GPLv3 (dependency ...", 
"body" : "Library Licensing MySQL-python>=1.2.5 GPL Celery>=4.1.0 BSD pyhive>=0.5.1 Apache 2.0 sasl>=0.2.1 Apache License, Version 2.0 Thrift>=0.11.0 Apache License, Version 2.0 pandas>=0.20.3 BSD Flask>=0.12.2 BSD cm_api>=16.0.0 Apache 2.0 six>=1.10.0 MIT matplotlib>=2.1.0 BSD pystan>=2.16 GPLv3 (dependency of FBprophet) fbprophet>=0.1.1 BSD scipy>=0.19.1 BSD seasonal>=0.3.1 MIT statsmodels>=0.8.0 BSD gatspy>=0.3 BSD 3-Clause numpy>=1.13.1 BSD PyAstronomy>=0.12.0 MIT python_dateutil>=2.6.1 Simplified BSD fastdtw>=0.3.2 MIT requests==2.20.1 Apache 2.0 seaborn>=0.8.1 BSD 3-Clause sqlalchemy==1.2.7 MIT License pymysql>=0.8.0 MIT psycopg2==2.7.6.1 LGPL, Version 3.0 cython 0.27.3 Apache License, Version 2.0 kombu 4.1.0 BSD 3-Clause \"New\" Thrift-sasl 0.3.0 Apache License, Version 2.0 " }, 
{ "title" : "Matomo (preview only)", 
"url" : "102123-install-matomo.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Add-ons \/ Matomo (preview only)", 
"snippet" : "This feature is in beta\/preview mode. Preview features are in beta and are subject to change. The design and code is less mature than official GA features and is being provided as-is with no warranties. Preview features are not subject to the support SLA of official GA features. We do not recommend ...", 
"body" : "This feature is in beta\/preview mode. Preview features are in beta and are subject to change. The design and code is less mature than official GA features and is being provided as-is with no warranties. Preview features are not subject to the support SLA of official GA features. We do not recommend you deploy Preview features in a production environment. Matomo is a free and open-source web analytics platform that runs on MySQL or MariaDB. When you integrate Matomo with Unravel you get information on page visits in Unravel UI. You can co-locate Matomo with Unravel Server or install it on a separate host. Make sure that your Matomo host meets the sizing requirements for tracking 100,000 pageviews per month . Matomo has no data collection or storage limits , so if you need to adjust your MySQL database size, follow their instructions . Matomo works with RHEL 6.0-7.6 and CentOS 6.10-7.6. It is only available for Unravel 4.5.1.2 and onward. compmatrix-platform Install NGINX. yum install epel-release\nyum install nginx Install PHP. If the host machine's underlying operating system is CentOS 7.x, install PHP7.3, because the default PHP version on CentOS 7 is PHP 5.4, which isn't supported by Matomo. yum install http:\/\/rpms.remirepo.net\/enterprise\/remi-release-7.rpm\nyum install --enablerepo=remi-php73 php php-mcrypt php-cli php-gd php-curl php-mysql php-ldap php-zip php-fileinfo php-fpm php-mbstring php-dom php-xml php-simplexm Create a user and database for Matomo and connect it to Unravel. Perform only one of the following steps based up the database you are using. MySQL (recommended) Find Unravel’s MySQL database connection information in unravel.properties . This information is in the unravel.jdbc.* properties. Make a note of the values of these properties. Using the connection information you noted in the previous step, connect to the database. mysql -h 127.0.0.1 -P port -u user -p Create a user and database for Matomo. create database matomo;\ncreate user matomo identified by ' agoodpassword ';\ngrant all on matomo.* to matomo;\nexit; Mariadb Install MariaDB. yum install mariadb-server Secure the MariaDB installation by running \/usr\/bin\/mysql_secure_installation and following the instructions. Answer Y to all prompts and assign a password for root, disable remote root access, disable anonymous access and delete the test database. Connect to the database. mysql -u root -p Create a user and database for Matomo. create database matomo;\ncreate user matomo identified by ' agoodpassword ';\ngrant all on matomo.* to matomo;\nexit; Enable and start MariaDB. systemctl enable mariadb\nsystemctl start mariadb Configure PHP. On the Matomo host, edit \/etc\/php-fpm.d\/www.conf as follows: Change these lines: Old New user = apache user = nginx group = apache group = nginx After listen = 127.0.0.1:9000 add: listen = \/var\/run\/php-fpm\/php-fpm.sock Change these lines (notice the removal of the semicolon): Old New ;listen.owner = nobody listen.owner = nginx ;listen.group listen.group = nginx ;listen.mode = 0660 listen.mode = 0660 Configure nginx. On the Matomo host, create the directory \/etc\/nginx\/snippets . mkdir \/etc\/nginx\/snippets Create \/etc\/nginx\/snippets\/fastcgi-php.conf with the contents shown below. fastcgi_connect_timeout 60;\nfastcgi_send_timeout 180;\nfastcgi_read_timeout 180;\nfastcgi_buffer_size 512k;\nfastcgi_buffers 512 16k;\nfastcgi_busy_buffers_size 1m;\nfastcgi_temp_file_write_size 4m;\nfastcgi_max_temp_file_size 4m;\nfastcgi_intercept_errors off;\n\nfastcgi_param SCRIPT_FILENAME $request_filename;\nfastcgi_param PATH_INFO $fastcgi_path_info;\nfastcgi_param PATH_TRANSLATED $document_root$fastcgi_path_info;\nfastcgi_param QUERY_STRING $query_string;\nfastcgi_param REQUEST_METHOD $request_method;\nfastcgi_param CONTENT_TYPE $content_type;\nfastcgi_param CONTENT_LENGTH $content_length;\nfastcgi_param SCRIPT_NAME $fastcgi_script_name;\nfastcgi_param REQUEST_URI $request_uri;\nfastcgi_param DOCUMENT_URI $document_uri;\nfastcgi_param DOCUMENT_ROOT $document_root;\nfastcgi_param SERVER_PROTOCOL $server_protocol;\nfastcgi_param REQUEST_SCHEME $scheme;\nfastcgi_param HTTPS $https if_not_empty;\nfastcgi_param HTTP_PROXY \"\";\nfastcgi_param GATEWAY_INTERFACE CGI\/1.1;\nfastcgi_param SERVER_SOFTWARE nginx\/$nginx_version;\nfastcgi_param REMOTE_ADDR $remote_addr;\nfastcgi_param REMOTE_PORT $remote_port;\nfastcgi_param SERVER_ADDR $server_addr;\nfastcgi_param SERVER_PORT $server_port;\nfastcgi_param SERVER_NAME $server_name;\nfastcgi_param REDIRECT_STATUS 200; Create \/etc\/nginx\/conf.d\/matomo.conf with the contents shown below. server {\n listen [::]:80; # remove this if you don't want Matomo to be reachable from IPv6\n listen 80;\n server_name YOUR_SERVER.YOUR_DOMAIN; # list all domains Matomo should be reachable from\n access_log \/var\/log\/nginx\/matomo.access.log;\n error_log \/var\/log\/nginx\/matomo.error.log;\n\n ## uncomment if you want to enable HSTS with 6 months cache\n ## ATTENTION: Be sure you know the implications of this change (you won't be able to disable HTTPS anymore)\n #add_header Strict-Transport-Security max-age=15768000;\n\n ## replace with your SSL certificate\n# ssl_certificate \/path\/to\/fullchain.pem;\n# ssl_certificate_key \/path\/to\/privkey.pem;\n# include ssl.conf; # if you want to support older browsers, please read through this file\n\n add_header Referrer-Policy origin; # make sure outgoing links don't show the URL to the Matomo instance\n \n root \/var\/www\/matomo\/; # replace with path to your matomo instance\n \n index index.php;\n \n ## only allow accessing the following php files\n location ~ ^\/(index|matomo|piwik|js\/index).php {\n include snippets\/fastcgi-php.conf; # if your Nginx setup doesn't come with a default fastcgi-php config replace this with the one from this repository\n fastcgi_param HTTP_PROXY \"\"; # prohibit httpoxy: https:\/\/httpoxy.org\/\n fastcgi_pass unix:\/var\/run\/php-fpm\/php-fpm.sock; #replace with the path to your PHP socket file\n #fastcgi_pass 127.0.0.1:9000; # uncomment if you are using PHP via TCP sockets\n }\n \n ## needed for HeatmapSessionRecording plugin\n location = \/plugins\/HeatmapSessionRecording\/configs.php { \n include snippets\/fastcgi-php.conf;\n fastcgi_param HTTP_PROXY \"\";\n fastcgi_pass unix:\/var\/run\/php-fpm\/php-fpm.sock; #replace with the path to your PHP socket file\n #fastcgi_pass 127.0.0.1:9000; # uncomment if you are using PHP via TCP sockets\n }\n \n ## deny access to all other .php files\n location ~* ^.+\\.php$ {\n deny all;\n return 403;\n }\n\n ## serve all other files normally \n location \/ {\n try_files $uri $uri\/ =404;\n }\n \n ## disable all access to the following directories \n location ~ \/(config|tmp|core|lang) {\n deny all;\n return 403; # replace with 404 to not show these directories exist\n }\n location ~ \/\\.ht {\n deny all;\n return 403;\n }\n\n location ~ \\.(gif|ico|jpg|png|svg|js|css|htm|html|mp3|mp4|wav|ogg|avi|ttf|eot|woff|woff2|json)$ {\n allow all;\n ## Cache images,CSS,JS and webfonts for an hour\n ## Increasing the duration may improve the load-time, but may cause old files to show after an Matomo upgrade\n expires 1h;\n add_header Pragma public;\n add_header Cache-Control \"public\";\n }\n\n location ~ \/(libs|vendor|plugins|misc\/user) {\n deny all;\n return 403;\n }\n\n ## properly display textfiles in root directory\n location ~\/(.*\\.md|LEGALNOTICE|LICENSE) {\n default_type text\/plain;\n }\n}\n In \/etc\/nginx\/conf.d\/matomo.conf , replace server_name with the name of your Matomo server. On the Matomo host, enable and start all services. systemctl enable php-fpm\nsystemctl start php-fpm\nsystemctl enable nginx\nsystemctl start nginx On the Matomo host, install Matomo. Download the latest version of Matomo. curl https:\/\/builds.matomo.org\/matomo-latest.tar.gz -o \/tmp\/matomo-latest.tar.gz Install it in \/var\/www . tar zxf \/tmp\/matomo-latest.tar.gz -C \/var\/www\/ Change the owner to nginx . chown -R nginx \/var\/www\/ Configure Matomo. Using a web browser, go to http:\/\/ server_name (as configured in \/etc\/nginx\/conf.d\/matomo.conf ) and enter the following information at the prompts. Prompt Value Database server If you installed Matomo on the same host as your database, enter 127.0.0.1 . Otherwise, enter the IP address of your database server, which depends on which database you're using (the database bundled with Unravel, your own MySQL, or your own MariaDB). Login matomo Password The password you assigned to the user matomo Database name matomo Confirm Matomo's connection to Unravel. On the Matomo UI, you should see the dashboard, which gives you a single pane of glass overview of Unravel UI usage: Visitors | Visits Log shows you what pages visitors have looked at in Unravel UI. Matomo properties Property\/Description Set by user Unit Default com.unraveldata.do.not.track Enable or disable analytics collection by Mixpanel and Matomo. true: Do not track user's activites. false: Allows tracking of activities and analytics collection by Mixpanel and Matomo. boolean false com.unraveldata.mixpanel.analytics Enable or disable Mixpanel analytics collection. true: Allows collection by Mixpanel. Note: com.unraveldata.do.not.track must be set to false . false: Disallows collection by Mixpanel. boolean false com.unraveldata.custom.analytics Enable or disable collection of custom analytics by Matomo. true: Allows collection by Matomo. Note: com.unraveldata.do.not.track must be set to false and com.unraveldata.custom.analytics.url must be defined. false: Don't collect user's activites. boolean true com.unraveldata.custom.analytics.url Custom analytics URL for Matomo. You must set this property when com.unraveldata.custom.analytics =true. Format: http:\/\/unravehost:80   string (URL) - " }, 
{ "title" : "Next steps", 
"url" : "102124-install-post.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Next steps", 
"snippet" : "This section lists common ways you can customize Unravel for your environment, services, and apps. These steps are all optional. Set com.unraveldata.customer.organization to your company name. Configure Email settings . Hive metastore permissions . HBase instrumentation . Kafka instrumentation . Ooz...", 
"body" : "This section lists common ways you can customize Unravel for your environment, services, and apps. These steps are all optional. Set com.unraveldata.customer.organization to your company name. Configure Email settings . Hive metastore permissions . HBase instrumentation . Kafka instrumentation . Oozie instrumentation . Tez instrumentation . Using notebooks with Spark . Users access Unravel through the Unravel UI. The installation process creates one administrator account. To add more administrators, see Add more admins to Unravel UI . To add read-only administrators, see Add read-only admins . To add LDAP authentication for Unravel UI, see and modify the To add SAML authentication for Unravel UI, see enable SAML authentication and modify the . To use role-based access control (RBAC) to restrict the views and applications users can see when they log into Unravel UI, see Roles and Role Based Access Control . To enable TLS (SSL) for the Unravel UI, see Enabling TLS . Create multiple workers for high volume data . Enable live monitoring of Spark streaming applications . Install the OnDemand service to allow Unravel to generate the following OnDemand-based reports: Migration Planning is disabled by default. To enable, set unravel.python.reporting.cloudreport.enable to true . Migration Planning and Forecasting (enabled by default) both require extensive configuration . Small Files reports and File reports are enabled by default but require extensive configuration . Some reconfiguration is necessary when upgrading the OnDemand package. Cluster KPIS, Cluster Optimization, Queue Analysis, Sessions, and Top X reports are enabled by default and don't need configuration. Sessions is enabled by default. However, prior to 4.5.1 Sessions was disabled by default and its properites had no defaults. For installations of Unravel 4.5.0.x and earlier to enable see OnDemand Configurations . Edit unravel.properties and set all the Session properties to the default values listed. See OnDemand Configurations for all properties which affect the OnDemand reports, including enabling and disabling the various reports. " }, 
{ "title" : "Upgrades", 
"url" : "102125-upgrade.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Upgrades", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Upgrading Unravel Server", 
"url" : "102126-upgrade-server.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Upgrades \/ Upgrading Unravel Server", 
"snippet" : "Contact Unravel Support at before upgrading from Unravel 4.2.x or 4.3.x to Unravel 4.5.x To determine if it's mandatory to upgrade Unravel sensors when you upgrade Unravel Server, see the release notes . rn-45xx This topic explains how to upgrade the Unravel Server RPM. Download the new RPM. downloa...", 
"body" : "Contact Unravel Support at before upgrading from Unravel 4.2.x or 4.3.x to Unravel 4.5.x To determine if it's mandatory to upgrade Unravel sensors when you upgrade Unravel Server, see the release notes . rn-45xx This topic explains how to upgrade the Unravel Server RPM. Download the new RPM. downloads Copy the new RPM to the Unravel host. Stop all services. sudo \/etc\/init.d\/unravel_all.sh stop Upgrade RPM on the host. sudo rpm -U unravel- version .rpm* Add your license key to unravel.properties . Add your license key to unravel.properties before starting or restarting Unravel Server. After all the RPM upgrades finish, restart all services. sudo \/etc\/init.d\/unravel_all.sh start Complete any platform-specific upgrade steps. " }, 
{ "title" : "Upgrading sensors on CDH", 
"url" : "102127-upgrade-sensors-cdh.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Upgrades \/ Upgrading sensors on CDH", 
"snippet" : "Only activate the sensors when there are no Hive or Spark jobs running as the restart may affect running jobs. In Cloudera Manager, check for newer versions of UNRAVEL_SENSOR. In Cloudera Manager, click the parcel icon in the top menu bar. Click Check for New Parcels and look for UNRAVEL_SENSOR entr...", 
"body" : "Only activate the sensors when there are no Hive or Spark jobs running as the restart may affect running jobs. In Cloudera Manager, check for newer versions of UNRAVEL_SENSOR. In Cloudera Manager, click the parcel icon in the top menu bar. Click Check for New Parcels and look for UNRAVEL_SENSOR entries. If newer sensors are available, Cloudera Manager indicates this. Click Download | Distribute , and activate the newer version of the sensors. When activating the new sensors, Cloudera Manager notifies you that Hive and Spark services must be restarted. Click OK . When activation is complete, Cloudera Manager automatically disables the old version. Deploy the new sensor JARs . " }, 
{ "title" : "Upgrading EMR and Unravel Sensors", 
"url" : "102128-emr-upgrade.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Upgrades \/ Upgrading EMR and Unravel Sensors", 
"snippet" : "1 - Upgrade Unravel Server Download the latest Unravel RPM. downloads Upgrade the RPM. sudo rpm -Uvh unravel-*.rpm Run fix up script after installation. \/usr\/local\/unravel\/install_bin\/await_fixups.sh Start unravel daemons \/etc\/init.d\/unravel_all.sh start 2 - Upgrade Unravel Sensors and unravel_es da...", 
"body" : "1 - Upgrade Unravel Server Download the latest Unravel RPM. downloads Upgrade the RPM. sudo rpm -Uvh unravel-*.rpm Run fix up script after installation. \/usr\/local\/unravel\/install_bin\/await_fixups.sh Start unravel daemons \/etc\/init.d\/unravel_all.sh start 2 - Upgrade Unravel Sensors and unravel_es daemon in EMR nodes Case 1: Cluster was brought up via bootstrap. Case 2: Ansible script was run after the cluster was brought up, no bootstrap action. EMR bootstrap action does not upgrade sensor on existing nodes, run ansible script once to perform the sensor upgrade. Follow the steps in Connect the Unravel EC2 Instance to an Existing EMR Cluster to upgrade sensor on all nodes. Restart hive-server2. sudo stop hive-server2\nsudo start hive-server2 Case 3: Using Unravel sensor autoscaling to connect the cluster. (Upgrading from 4.5.2.0.) Stop unravel_es . sudo \/etc\/init.d\/unravel_es stop Remove unravel_es database. sudo rm -rf \/usr\/local\/unravel_es\/unravel_h2* Rerun unravel_emr_bootstrap.py script. sudo python unravel_emr_bootstrap.py --unravel-server Unravel Server --sensor-autoscaling " }, 
{ "title" : "Upgrading sensors on HDP", 
"url" : "102129-upgrade-sensors-hdp.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Upgrades \/ Upgrading sensors on HDP", 
"snippet" : "unravel-host is the Unravel Server's fully qualified domain name or IP address. spark-version is the target Spark version. For example, 1.6.0 , 2.0.1 , 2.2.0 . hive-version is the target Hive version For example, 1.2.0 . Only activate the sensors when there are no Hive or Spark jobs running as the r...", 
"body" : "unravel-host is the Unravel Server's fully qualified domain name or IP address. spark-version is the target Spark version. For example, 1.6.0 , 2.0.1 , 2.2.0 . hive-version is the target Hive version For example, 1.2.0 . Only activate the sensors when there are no Hive or Spark jobs running as the restart may affect running jobs. On Unravel Server and on all nodes in the cluster that have Unravel Sensors deployed, create backups of the old sensor folders and then remove the folders. cd \/usr\/local\/\nsudo tar -cvf unravel-sensors-`date +%m%d%y`.tar unravel-agent unravel_client\nsudo rm -rf \/usr\/local\/unravel-agent\nsudo rm -rf \/usr\/local\/unravel_client On Unravel Server, run the sensor package script. cd \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/\nsudo .\/unravel_hdp_setup.sh install -y --unravel-server unravel-host :3000 --spark-version spark-version --hive-version hive-version Running the sensor package script creates the sensor packages in \/usr\/local\/unravel-agent and \/usr\/local\/unravel_client . Compress \/usr\/local\/unravel-agent and \/usr\/local\/unravel_client and copy the compressed file ( unravel-new-sensors.tar ) to all nodes in the cluster. cd \/usr\/local\/\ntar -cvf unravel-new-sensors.tar unravel-agent unravel_client\nscp unravel-new-sensors.tar cluster-node :\/usr\/local\/ On the cluster node, extract the copied unravel-new-sensors.tar file on the cluster nodes. cd \/usr\/local\/ \ntar -xvf unravel-new-sensors.tar " }, 
{ "title" : "Upgrading sensors on MapR", 
"url" : "102130-upgrade-sensors-mapr.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Installation \/ Upgrades \/ Upgrading sensors on MapR", 
"snippet" : "unravel-host is the Unravel Server's fully qualified domain name or IP address. spark-version is the target Spark version. For example, 1.6.0 , 2.0.1 , 2.2.0 . hive-version is the target Hive version For example, 1.2.0 . Only activate the sensors when there are no Hive or Spark jobs running as the r...", 
"body" : "unravel-host is the Unravel Server's fully qualified domain name or IP address. spark-version is the target Spark version. For example, 1.6.0 , 2.0.1 , 2.2.0 . hive-version is the target Hive version For example, 1.2.0 . Only activate the sensors when there are no Hive or Spark jobs running as the restart may affect running jobs. On Unravel Server and on all nodes in the cluster that have Unravel Sensors deployed, create backups of the old sensor folders and then remove the folders. cd \/usr\/local\/\nsudo tar -cvf unravel-sensors-`date +%m%d%y`.tar unravel-agent unravel_client\nsudo rm -rf \/usr\/local\/unravel-agent\nsudo rm -rf \/usr\/local\/unravel_client\ncd \/opt\/mapr\/spark\/spark- spark-version \/conf\/\nsudo mv spark-defaults.conf.pre_unravel spark-defaults.conf.pre_unravel.copy On Unravel Server, run the sensor package script. cd \/usr\/local\/unravel\/install_bin\/unraveldata-clients\/\nsudo .\/unravel_mapr_setup.sh install -y --unravel-server unravel-host :3000 --spark-version spark-version --hive-version hive-version Running the sensor package script creates the sensor packages in \/usr\/local\/unravel-agent and \/usr\/local\/unravel_client . Compress \/usr\/local\/unravel-agent and \/usr\/local\/unravel_client and copy the compressed file ( unravel-new-sensors.tar ) to all nodes in the cluster. cd \/usr\/local\/\ntar -cvf unravel-new-sensors.tar unravel-agent unravel_client\nscp unravel-new-sensors.tar cluster-node :\/usr\/local\/ On the cluster node, extract the copied unravel-new-sensors.tar file on the cluster nodes. cd \/usr\/local\/ \ntar -xvf unravel-new-sensors.tar " }, 
{ "title" : "User guide", 
"url" : "102131-uguide.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide", 
"snippet" : "Unravel Web UI helps you to analyze, optimize, and troubleshoot big data applications and operations. Unravel supports clusters both on-premise and in the cloud. Our Web UI for the Azure Databricks varies to accommodate the differences between Azure Databricks and other clusters (on-premise or in th...", 
"body" : "Unravel Web UI helps you to analyze, optimize, and troubleshoot big data applications and operations. Unravel supports clusters both on-premise and in the cloud. Our Web UI for the Azure Databricks varies to accommodate the differences between Azure Databricks and other clusters (on-premise or in the cloud). For a list of what is not supported in Azure Databricks and other cloud platforms please refer to the Release notes. For a list of what is not supported in Azure Databricks and other cloud platforms please refer to the Release Notes. Getting Started Overview overview Getting started videos Use Cases End user Access to UI features is dependent on the role assigned by Admin, ranging from highly restrictive to complete UI access. Applications > Applications Applications > Sessions APMs Event and Insights (applications) AutoActions (informational) Operations > Usage Details > Infrastructure Operations > Usage Details > Impala Usage Reports > Operational Insights > Chargeback Admin\/Cluster Management Applications APMs Event and Insights (applications, workflows and clusters) Operations Sessions Workflows Operational Insights Data Insights AutoActions (generation) Reports Migration Planning Project Management\/Management Admin can schedule regular reports to be automatically emailed. Enduser (track top users, applications, etc.) Executive KPIs (cluster usage KPIs, including for YARN and Impala) " }, 
{ "title" : "Getting started", 
"url" : "102132-getting-started.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Getting started", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Simplifying DataOps", 
"url" : "102132-getting-started.html#UUID-c17c78ea-b73b-ccb7-8a8d-9cf8e1c1c0f1_UUID-da5cf353-d2f3-5079-5956-5942f5b54f79", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Getting started \/ Simplifying DataOps", 
"snippet" : "Time: 20 min...", 
"body" : "[video] Time: 20 min " }, 
{ "title" : "How to run Spark apps reliably and optimize performance", 
"url" : "102132-getting-started.html#UUID-c17c78ea-b73b-ccb7-8a8d-9cf8e1c1c0f1_UUID-4c004bcc-6595-e711-9fec-c02d01f0bfc9", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Getting started \/ How to run Spark apps reliably and optimize performance", 
"snippet" : "Time: 25 min...", 
"body" : "[video] Time: 25 min " }, 
{ "title" : "Spark troubleshooting", 
"url" : "102132-getting-started.html#UUID-c17c78ea-b73b-ccb7-8a8d-9cf8e1c1c0f1_UUID-5966a401-8894-7131-0bd1-b4c45d3a05ca", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Getting started \/ Spark troubleshooting", 
"snippet" : "Time: 6 min...", 
"body" : "[video] Time: 6 min " }, 
{ "title" : "Spark tuning and debugging demo", 
"url" : "102132-getting-started.html#UUID-c17c78ea-b73b-ccb7-8a8d-9cf8e1c1c0f1_UUID-5f659773-ef44-6485-9904-eefbf44604c0", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Getting started \/ Spark tuning and debugging demo", 
"snippet" : "Time: 3 min...", 
"body" : "[video] Time: 3 min " }, 
{ "title" : "Automated root cause analysis demo", 
"url" : "102132-getting-started.html#UUID-c17c78ea-b73b-ccb7-8a8d-9cf8e1c1c0f1_UUID-494d21bc-5895-b9c7-640b-df46c1b2d4fb", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Getting started \/ Automated root cause analysis demo", 
"snippet" : "Time: 3 min...", 
"body" : "[video] Time: 3 min " }, 
{ "title" : "Data insights demo", 
"url" : "102132-getting-started.html#UUID-c17c78ea-b73b-ccb7-8a8d-9cf8e1c1c0f1_UUID-68fcaa3b-08b9-7348-93d4-c62246d22cda", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Getting started \/ Data insights demo", 
"snippet" : "Time: 5 min...", 
"body" : "[video] Time: 5 min " }, 
{ "title" : "Azure Databricks overview and demo", 
"url" : "102132-getting-started.html#UUID-c17c78ea-b73b-ccb7-8a8d-9cf8e1c1c0f1_UUID-465b4c88-84f2-e4c4-836b-e042482bd3b3", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Getting started \/ Azure Databricks overview and demo", 
"snippet" : "Time: 6 min...", 
"body" : "[video] Time: 6 min " }, 
{ "title" : "Using chargeback reports", 
"url" : "102132-getting-started.html#UUID-c17c78ea-b73b-ccb7-8a8d-9cf8e1c1c0f1_UUID-db79547a-7239-b498-4dd6-90104ad0275b", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Getting started \/ Using chargeback reports", 
"snippet" : "Time: 4 min...", 
"body" : "[video] Time: 4 min " }, 
{ "title" : "Common UI features", 
"url" : "102133-common-ui-features.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Common UI features", 
"snippet" : "Every page has the Unravel title bar. The pages available to you are listed on the left. The page you are viewing is underlined and listed in the black bar. To the right there is a search box, Docs Button, possibly a support button, and a pull-down menu. If you are an end user restricted by Role Bas...", 
"body" : "Every page has the Unravel title bar. The pages available to you are listed on the left. The page you are viewing is underlined and listed in the black bar. To the right there is a search box, Docs Button, possibly a support button, and a pull-down menu. If you are an end user restricted by Role Based Access Control , you can only view a subset of the UI's pages and the pull-down menu only has About and Logout options. If you are an unrestricted end user or an admin, all the pages are available with possible read\/write restrictions. The pull-down menu has Manage , About and Logout available. When your admin has disabled Support there is no Support Button, for example, If you can configure the date range , time period , or cluster these options will be available in the title bar. You set them using the pull down menus on the banner. When there are multiple tabs, click the tab to display its contents. When detailed or further information is available is in the upper right-hand corner; click it for further information. Click to expand a section to full tile width; click to close it. Click an app name\/id\/workflow to bring up information on the app, fragment etc., for example, the Spark Application Manager, table information, etc. When applicable, the Notifications column ( ) notes if Unravel has tuning suggestions ( ), an AutoAction alert ( ) or both ( ). When relevant there is an AutoActions\/Events column ( ). The number of AutoActions\/events (0-n) triggered is noted. If an app has a parent a link to it is in the GoTo column ( ), its title bar or a more info glyph ( ). Click it to display the app's parent APM. Click the block glyph ( ) to open or close the row details. When open its details are shown in a tile below. Green indicates the block is currently being displayed; gray indicates it's closed. Graphs Hovering over a line in a graph causes the information to be displayed in a text box ( ). \" Applications running at mm\/dd\/yy hh:mm:ss\" is displayed below the graph with a list of relevant apps, if any, displayed. Clicking within the graph displays the relevant, if any, apps for that point in time. Clicking expands the chart to the full width of your browser. Click on the close window on the right side of title bar to return to the previous view. When the graph can be displayed based upon Group By , Tags , Metric , etc., there is a pull-down ( ) in the upper right-hand corner. Click to print or download the information in the graph in various forms, i.e., JPG, PNG, CSV. When you can zoom in\/out of a diagram\/execution graph there are magnifiers ( , ) in the upper right-hand corner. Click to return to the initial view. " }, 
{ "title" : "Event panel & insights", 
"url" : "102134-events-panels-insights.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Event panel & insights", 
"snippet" : "The Unravel intelligence (UI) engine helps you manage your applications more efficiently by providing insights into their run. The UI engine gives its insights and tuning suggesting via the Events Panel. Not all UI engine insights results in concrete recommendations, so to take full advantage you mu...", 
"body" : "The Unravel intelligence (UI) engine helps you manage your applications more efficiently by providing insights into their run. The UI engine gives its insights and tuning suggesting via the Events Panel. Not all UI engine insights results in concrete recommendations, so to take full advantage you must read the efficiency panel. There is not a one-to-one correspondence between the event and recommendation number. A single event might lead to none or many recommendations. Recommendations Lists the parameters to change, shows their current and recommended value. Efficiency The efficiency list details the inefficiencies. The UI engine might: Make a recommendation and note the expected result from such a change. Make a configuration suggestion. Suggest where to look to increase efficiency. Below are two examples. Each type of job and instance of a job has events relevant to that particular job and instance. MapReduce job example This MapReduce job is part of a Hive Query. In this example the UI engine lists list four events and has three recommendations. Recommendations Efficiency 1: Used too many reducers Resulted in the one recommendation (#1). Efficiency 2: Reduce tasks that start before map phase finishes. Resulted in one suggestion . Efficiency 3: Too many mappers. Resulted in the two recommendations (#2 and #3). Efficiency 4: Large data shuffle from map to reduce. Resulted in a suggestion. Tez DAG example This Tez DAG job is part of a Hive Query. In this example the UI engine lists list three events and has four recommendations. Recommendations Efficiency 1: Tez DAG map vertex used too many tasks. Resulted in two suggestions (#3 and #4) and explanation of the problem. Efficiency 2: Tez DAG reducer vertex used too many tasks. Resulted in one recommendation (#1). Efficiency 3: hive.exec.parallel is set to false. Resulted in one recommendation (#2). " }, 
{ "title" : "Operations page", 
"url" : "102135-operations-page.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Operations page", 
"snippet" : "The Operations page provides a synopsis of a cluster and its activities. It has two tabs: Dashboard Usage details By default, it opens showing Operations > Dashboard tab. You can change the date range using the date picker, or update the time period a change the cluster by using the pull downs on th...", 
"body" : "The Operations page provides a synopsis of a cluster and its activities. It has two tabs: Dashboard Usage details By default, it opens showing Operations > Dashboard tab. You can change the date range using the date picker, or update the time period a change the cluster by using the pull downs on the far right. By default, all the clusters are used to populate the page. When you interact with a page, for example, click in graph, Unravel stops updating the data. When that occurs, a button appears in the Dashboard \/ Usage Details bar. Click the button so restart refreshing the page. See Common UI Features for general information about Unravel's UI. Dashboard To view the Dashboard, click Operations > Dashboard . The Dashboard provides an overview of cluster activities with links to drill down into YARN apps, resource usage, app inefficiencies, and events\/alerts. By default, it is configured to display all clusters hourly for the past 24 hours . Finished YARN applications tile The line graphs display the successful, failed, and killed jobs for the time period , using the time increment and cluster specified. It textually displays the total number for the time period. Clicking on Open Section brings up the corresponding apps window listing all apps. See All Applications . Running YARN applications tile The line graphs display the running and pending jobs for the current time. It textually displays the total number at the current time. Clicking on opens Operations > Usage Details > Jobs . Inefficient applications tile There are four tabs, Hive , MapReduce , Spark , and Impala each list the inefficiencies that apps of job type has experienced and the number of apps that experienced the event. Click the sub-tab to change the app type. Click the Event Name to bring up the list of apps that experienced the event. Inefficient apps list is equivalent to Applications > All Applications except it only displays apps that have experienced the event. The event type is noted in the upper left-hand corner. Recent events and alerts sidebar The sidebar lists all events and alerts that have occurred. They are listed in descending order by date then time within the date. A separate entry appears for each time a particular AutoAction was triggered. In the example below, the same AutoAction triggered at 10.54 and 11.54. Clicking an event\/alert brings up a Cluster Resource view ( Operations > Usage Detail > Infrastructure ) displaying a 10-minute time slice (±5 minutes of the event) and a list of apps running at that time. See AutoActions Overview for more information. Click Add new AutoAction or Alert to create a new AutoAction. Click Clear to clear the list. " }, 
{ "title" : "Resources tile", 
"url" : "102135-operations-page.html#UUID-3869f888-0a5d-61a7-8c5f-17b9c3a7a1ea_id_TheOperationsPage-ResourcesTile", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Operations page \/ Resources tile", 
"snippet" : "Displays the available and allocated vCore and Memory for the entire cluster. Clicking on expands the graph to full width of the window. Clicking opens the Operations > Usage Details > Infrastructure tab....", 
"body" : "Displays the available and allocated vCore and Memory for the entire cluster. Clicking on expands the graph to full width of the window. Clicking opens the Operations > Usage Details > Infrastructure tab. " }, 
{ "title" : "Usage details", 
"url" : "102135-operations-page.html#UUID-3869f888-0a5d-61a7-8c5f-17b9c3a7a1ea_section-5d4900dac46d0-idm45764245643136", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Operations page \/ Usage details", 
"snippet" : "Usage Detail has six tabs: Infrastructure Jobs Nodes Impala Usage Kafka HBase One of the most difficult challenges in managing multi-tenant Hadoop clusters is understanding how resources are being used by the apps running in the clusters. Through Usage Details , Unravel Web UI provides a unique fore...", 
"body" : "Usage Detail has six tabs: Infrastructure Jobs Nodes Impala Usage Kafka HBase One of the most difficult challenges in managing multi-tenant Hadoop clusters is understanding how resources are being used by the apps running in the clusters. Through Usage Details , Unravel Web UI provides a unique forensic view into each cluster's key performance indicators (KPIs) over time and how they relate to the apps running in the cluster. For example, Unravel can pinpoint the apps causing a sudden spike in the total vCores or memory MB usage. This lets you easily drill down into the apps to understand their behavior. Whenever possible, Unravel provides recommendations and insights to help improve the app's run. All the charts and tables are automatically refreshed; however refreshing is disabled when you interact within a page to alter its display, for example, click a point within a graph. When disabled, button is displayed in Dashboard \/ Usage Details title bar. Click Refresh to resume automatic refreshes. By default, the Usage Details tab opens showing the Infrastructure tab. For all charts, click , for print and download options, e.g. CSV, JPEG. Click to expand it. For a particular point in time, hover over chart to see a tool tip with details. Click the graph to see all apps running at that particular point. Once expanded, click the to return to the initial view. To zoom in drag over a section of the graph; to return to the complete graph click Reset Graph . The examples below are showing the vCores-Total graph from the Infrastructure Tab expanded. The first graph shows the selection of an area to zoom in on, and the second result of the zooming in. Nodes This chart graphs the total number of Nodes by node status, active , lost , unhealthy , decommissioned , and rebooted . total = active + unhealthy Where: active : currently running and healthy nodes. unhealthy : currently running and unhealthy nodes. You can toggle the display of an item by clicking on its name. " }, 
{ "title" : "Infrastructure", 
"url" : "102135-operations-page.html#UUID-3869f888-0a5d-61a7-8c5f-17b9c3a7a1ea_N1553155703543", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Operations page \/ Usage details \/ Infrastructure", 
"snippet" : "This tab contains four graphs. The upper two list available and allocated vCores and memory for the entire Cluster, and The bottom two shows the vCores and memory used by specific view, for example, Application Type , User , Queue . Clicking within a chart displays the apps running for that point in...", 
"body" : "This tab contains four graphs. The upper two list available and allocated vCores and memory for the entire Cluster, and The bottom two shows the vCores and memory used by specific view, for example, Application Type , User , Queue . Clicking within a chart displays the apps running for that point in time. You can choose how to display the bottom two graphs by clicking on the View By type ( 2 ). Once you have chosen your view, the first three available values for that view are displayed in the Showing box ( 3 ). Above the box notes the number displayed of the total available values for the View type. Regardless of the total available values by View By choice, you may only graph up to four. Click within the Showing box to see all available values; to remove a value click the x next to it ( 3 ). By default, Infrastructure opens displaying Application Type . In the example below only vCores-Total was expanded ( Show More ) and then a section was zoomed in on. To View By tags, use the Business Tags pull down menu, listing all available tags. The tag you selected is displayed in blue and to left of the Business Tags . To add a tag, click in the Showing box to show all available tags, click the tag to select it. You can only select up to four tag values. Delete a particular value by clicking the x next to remove the value. " }, 
{ "title" : "Jobs", 
"url" : "102135-operations-page.html#UUID-3869f888-0a5d-61a7-8c5f-17b9c3a7a1ea_N1553155650893", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Operations page \/ Usage details \/ Jobs", 
"snippet" : "Graphs the running and accepted jobs as applicable. You can Group by State, App Type, User, and Queue. By default, the chart uses State. You can change the display of an item via the Group By pull-down....", 
"body" : "Graphs the running and accepted jobs as applicable. You can Group by State, App Type, User, and Queue. By default, the chart uses State. You can change the display of an item via the Group By pull-down. " }, 
{ "title" : "Impala usage", 
"url" : "102135-operations-page.html#UUID-3869f888-0a5d-61a7-8c5f-17b9c3a7a1ea_N1553155840446", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Operations page \/ Usage details \/ Impala usage", 
"snippet" : "Graphs memory MB consumption and Query Number. The # Queries graph can be displayed by Tags and Group By (User or Queue)....", 
"body" : "Graphs memory MB consumption and Query Number. The # Queries graph can be displayed by Tags and Group By (User or Queue). " }, 
{ "title" : "Kafka", 
"url" : "102135-operations-page.html#UUID-3869f888-0a5d-61a7-8c5f-17b9c3a7a1ea_N1553155826925", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Operations page \/ Usage details \/ Kafka", 
"snippet" : "Lists all the configured Kafka clusters. See Kafka Application Manager for more information. See Kafka detecting lagging or stalled partitions for information on drilling down into a Cluster to locate lagging and stalled Topics\/Partitions. Clicking the cluster name brings detailed information about ...", 
"body" : "Lists all the configured Kafka clusters. See Kafka Application Manager for more information. See Kafka detecting lagging or stalled partitions for information on drilling down into a Cluster to locate lagging and stalled Topics\/Partitions. Clicking the cluster name brings detailed information about the Kafka Cluster " }, 
{ "title" : "HBase", 
"url" : "102135-operations-page.html#UUID-3869f888-0a5d-61a7-8c5f-17b9c3a7a1ea_N1553155816069", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Operations page \/ Usage details \/ HBase", 
"snippet" : "Please see HBase Configuration for configuring Unravel's UIX for HBase clusters. See HBase Alerts and Metrics for HBase metrics descriptions and list of alerts with suggested actions. Clusters list Clusters page lists all the available HBase clusters. Click a cluster name to bring up the cluster's i...", 
"body" : "Please see HBase Configuration for configuring Unravel's UIX for HBase clusters. See HBase Alerts and Metrics for HBase metrics descriptions and list of alerts with suggested actions. Clusters list Clusters page lists all the available HBase clusters. Click a cluster name to bring up the cluster's information the HBase Cluster view. Cluster view This view is divided into four sections. When a component's health is noted, hovering over its health glyph brings up details, . Cluster Information A bar shows what cluster you are displaying with a pull-down which lets you switch between clusters. Listed immediately below are the cluster metrics. You can choose to tab between clusters by choosing all cluster . Shown below is the tabbed view. Region servers Lists the cluster's region servers, with their KPIs and health. You can search on the region server by name. Click the server's name to bring up its details. Region server's KPIs Graphs the regional server metrics, the graphs are linked with the table list below them. Click within a graph to see its associated servers for that point in time. Hover over a point to bring up a tool tip displaying the information for that point in time. Click Show More to expand the graph with the table list to full window width. Click to print or download the graph. Tables Lists all the tables associated with the cluster, their KPIs and the table's health. Click the table name to bring up its information. You can search for a table by name; any table with a name matching or containing the string is displayed. Region server view Server, Operational, and OS Metrics are displayed. Hover over the metric for its description. For more information on the metrics see here . You can change the date range to display using the date picker. The first four regional server metrics are graphed using the daily view. Use the pull down menu to switch to the weekly view. Click to print or download the graph. The table list contains all the tables that were accessed by this server. You can search the table list by name. Click the table name to bring up its details. Table view Table has two tabs, Table and Region . It opens in the Table view, which displays the Table's KPIs. Three of the KPIs, regionCount , readRequestCount , and writeRequestCount are graphed, click within the graph to see the apps for that point in time Click to print or download the graph. Hover over the graph to display the information for that point in time. Click within the graph to display the apps running at that point in time. Click the app's name to open it in its app manager. Table regions Lists all the regions with their KPIs and health. " }, 
{ "title" : "Applications page", 
"url" : "102136-applications.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Applications page", 
"snippet" : "Unravel supports both ad hoc applications, and repeatedly running workflows or data pipelines. Ad hoc applications are Hive queries, MapReduce jobs, Spark applications, Impala queries, and so on; these are generated by end user tools (such as BI tools like Tableau, Microstrategy, etc.) or submitted ...", 
"body" : "Unravel supports both ad hoc applications, and repeatedly running workflows or data pipelines. Ad hoc applications are Hive queries, MapReduce jobs, Spark applications, Impala queries, and so on; these are generated by end user tools (such as BI tools like Tableau, Microstrategy, etc.) or submitted via the command-line. Repeatedly running workflows or data pipelines are created using cron or schedulers like Oozie, Airflow, or ETL tools like Informatica, Pentaho, and others. The Applications page has four tabs: All applications (Renamed in Unravel v4.5.3.0.) Running applications (Added in Unravel v4.5.3.0.) Workflows Sessions " }, 
{ "title" : "All applications", 
"url" : "102137-applications-allapps.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Applications page \/ All applications", 
"snippet" : "Note Click here for common features used throughout Unravel's UI. By default, this tab lists all apps for the past hour. The apps are sorted in descending order on Start Time . To order the results by another column, click the sort arrows ( ) next to the column header. Click the top arrow to sort th...", 
"body" : "Note Click here for common features used throughout Unravel's UI. By default, this tab lists all apps for the past hour. The apps are sorted in descending order on Start Time . To order the results by another column, click the sort arrows ( ) next to the column header. Click the top arrow to sort the column by ascending order and the bottom for descending order. The column the list is currently sorted on has the sort choice highlighted ( ). You can search or filter the table by: Selecting a date and time range by using the date picker above the table. Using the pagination boxes, to the right above and below the table, to view the pages. Filter the apps by the options in the left sidebar. App Name: Any apps which contain or match the string are displayed. App type and Status: The number of apps matching each category is listed. Select the check box to change the display for the app type or status. Tags (if any): Select the box next to the tag. Click in the text box to display the tag values. Click a value to select it. You can add up to all the available options. Queue , User , Cluster: Click in the text box for valid options. Click an option to select it. You can add up to all the available options. Duration: The range of duration for the currently displayed apps is shown. You can select a subrange either by entering the From and To in the text boxes or using the slider to denote the upper and lower bounds. Number of Events: The range of events is shown, for example, 0-4. You can select a subrange by using the slider to specify the upper and lower bounds. The notification column ( ) contains: A fine-tuning glyph ( ) when Unravel has tuning suggestions for the job. A when the job has triggered an AutoAction. Hover over the glyph to see a tooltip listing the AutoActions the app has violated ( ). The Goto column has a link to the app's parent, workflow or related app. For instance, a Hive-on-Tez job has a link to the Tez app, Hive-on-Spark to the Spark job, etc. Click the link to it bring up the parent's APM. Click anywhere within the app's row to bring it up in its APM. " }, 
{ "title" : "Running applications", 
"url" : "102138-applications-runapps.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Applications page \/ Running applications", 
"snippet" : "Note Click here for common features used throughout Unravel's UI. This tab lists all the running apps on the cluster. Since filtering by status or selecting a date range is irrelevant these options aren't available; except for this differences the tab is identical to the All applications tab....", 
"body" : "Note Click here for common features used throughout Unravel's UI. This tab lists all the running apps on the cluster. Since filtering by status or selecting a date range is irrelevant these options aren't available; except for this differences the tab is identical to the All applications tab. " }, 
{ "title" : "Sessions", 
"url" : "102139-applications-sessions.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Applications page \/ Sessions", 
"snippet" : "Note Click here for common features used throughout Unravel's UI. You must be using MySQL and have the OnDemand package installed to use this report. Sessions lets you run your app expressly to tune its performance for either: Speedup: Decrease the app's time (end-to-end duration) and resources (sho...", 
"body" : "Note Click here for common features used throughout Unravel's UI. You must be using MySQL and have the OnDemand package installed to use this report. Sessions lets you run your app expressly to tune its performance for either: Speedup: Decrease the app's time (end-to-end duration) and resources (shortening the app's duration is first priority). Reliability: In attempting to reduce resources Unravel prioritizes memory allocation to ensure the app doesn't fail due to \"out of memory\" exceptions. Why use sessions when Unravel already offers insights and recommendations on an app's run? You direct the tuning goal. You can: Provide multiple runs of an app to provide a larger data pool for Unravel to analyze. Have Unravel apply the recommendations for you and run the newly configured app. See the effects, both positive and negative, the tuning has on an app's run. Compare the various runs' configurations. Repeatedly tune the app until Unravel has no more recommendations. Your session is saved and can be run again, e.g., new runs added, cluster configuration changed. You can tune: Spark Hive on MapReduce Alternatively, you can Sessions simply as a tool to compare two runs of the same app. The Sessions tab opens displaying all current sessions sorted on Sessions Name in ascending order. The Start Time is the time of the most recent app run. Number of Apps is the number of runs inside the Session. Some of these runs were added when the session was created and some were created when the recommendations. The four KPIs Duration , IO , vCore Seconds , and Memory Seconds are essentially what the fine-tuning hopes to minimize. The column contains a trend line which plots all the apps (vertices) contained in the Session. Beneath the trend line the average and the best value is noted. For example, VCore9 , has 3 apps, for Duration the average is 19 m 24 s and the best time was 38.31% better than this. You can't draw the conclusion that the same app performed best for each KPI. Finally, the Cluster ID is listed. You can sort the table on Session Name , Start Time , and Cluster . Click the edit button to edit the session. You can search for a session by name. Enter the string in the search box; any session name matching or containing the string will be displayed. " }, 
{ "title" : "Creating a session", 
"url" : "102139-applications-sessions.html#UUID-a553752b-0b3c-25e8-befe-ed1a90845281_id_TheApplicationsPage-CreatingaSession", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Applications page \/ Sessions \/ Creating a session", 
"snippet" : "You can use sessions Manually: To control the analysis and app of recommendations. Automatically: To automatically analyze and apply recommendation. Manual session Click Create Session . You must name your session, it doesn't have to be unique, but we recommend you do so. Chose the Application Type ...", 
"body" : "You can use sessions Manually: To control the analysis and app of recommendations. Automatically: To automatically analyze and apply recommendation. Manual session Click Create Session . You must name your session, it doesn't have to be unique, but we recommend you do so. Chose the Application Type and Tuning Goal from the pull-down menu and the tuning goal. Add the App IDs of an app you want to tune. Click +Add another App ID to add more apps, to delete an App ID. Click Add to create the session or Cancel to close the modal. If you are tuning a Spark App you can optionally supply the JAR path , Class Name , Class Arguments , or Files . Auto tune session You have the additional option to specify the maximum number of runs. When specified, the iteration stops at the maximum number or lack of recommendations, whichever comes first. Via the events panel If an app has events with recommendations you can create a session directly from the recommendations tab. You can create either a manual or auto tune session. Click the type of session you wish to create and a session modal is filled in except for the session name. " }, 
{ "title" : "Via the Spark actions box", 
"url" : "102139-applications-sessions.html#UUID-a553752b-0b3c-25e8-befe-ed1a90845281_section-5cd04de6dfbbf-idm46542968824768", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Applications page \/ Sessions \/ Creating a session \/ Via the Spark actions box", 
"snippet" : "Once a Spark app has completed, you may select to Create a Session or Auto Tune Session from the Actions tab....", 
"body" : "Once a Spark app has completed, you may select to Create a Session or Auto Tune Session from the Actions tab. " }, 
{ "title" : "Session", 
"url" : "102139-applications-sessions.html#UUID-a553752b-0b3c-25e8-befe-ed1a90845281_id_TheApplicationsPage-Session", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Applications page \/ Sessions \/ Session", 
"snippet" : "The Session's APM layout is similar to all APMs. Instead of KPIs reflecting an app's KPIs they are trend lines of the app's KPIs across the various tuning runs. The example below is a session view immediately after creation. The left tab, Applications , is the list of runs; initially it is whatever ...", 
"body" : "The Session's APM layout is similar to all APMs. Instead of KPIs reflecting an app's KPIs they are trend lines of the app's KPIs across the various tuning runs. The example below is a session view immediately after creation. The left tab, Applications , is the list of runs; initially it is whatever was loaded when the session was created. As analysis and applications of recommendations occur more runs are added. Right tabs Progress tab: Keeps a log of all the activity. See example above. Trends: Expanded graphs of Duration, IO, and Resources. Compare: Allows you to compare two runs. " }, 
{ "title" : "Workflow", 
"url" : "102140-applications-workflow.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Applications page \/ Workflow", 
"snippet" : "The layout of this window mirrors the Applications tab. When there are no workflows tagged, the table is empty. You can use the date picker to select a date and time range. To search for a user or workflow enter a string, any user or workflow containing or matching the string is displayed. Click the...", 
"body" : "The layout of this window mirrors the Applications tab. When there are no workflows tagged, the table is empty. You can use the date picker to select a date and time range. To search for a user or workflow enter a string, any user or workflow containing or matching the string is displayed. Click the workflow name to view it in the Workflow Manager Application Manager . " }, 
{ "title" : "Reports page", 
"url" : "102141-reports.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page", 
"snippet" : "Unravel provides a variety of reports helping you manage your clusters. The page has four tabs. As of 4.5.1 Cloud Reports has been replaced with Migration Planning. Operational insights: Provides the ability to generate a variety of reports, including chargebacks, cluster summaries, and cluster comp...", 
"body" : "Unravel provides a variety of reports helping you manage your clusters. The page has four tabs. As of 4.5.1 Cloud Reports has been replaced with Migration Planning. Operational insights: Provides the ability to generate a variety of reports, including chargebacks, cluster summaries, and cluster compares. Data insights: Provides data level insights including a snapshot of tables and partitions over the last 24 hours and reports on disk capacity forecasting and small files. Migration planning: Helps you plan your migration to the cloud by analyzing your on-prem cluster, two migration methods and the costs associated each method. Report archives: Lists all reports and attempts to generate a report, whether the reports were scheduled or run on an ad hoc basis. Scheduled reports: Lists all scheduled reports. Scheduling Reports: All user-generated reports can be run on a scheduled or ad hoc basis. The Reports page opens displaying Operational Insights . " }, 
{ "title" : "Operational insights", 
"url" : "102142-reports-opinsights.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page \/ Operational insights", 
"snippet" : "Chargeback YARN - chargeback reports YARN jobs. Chargeback Impala - chargeback reports for Impala jobs. Cluster summary - summary reports for cluster usage. Cluster compare - compares cluster activity between two time periods on the same cluster. Cluster optimization - analyzes the cluster performan...", 
"body" : "Chargeback YARN - chargeback reports YARN jobs. Chargeback Impala - chargeback reports for Impala jobs. Cluster summary - summary reports for cluster usage. Cluster compare - compares cluster activity between two time periods on the same cluster. Cluster optimization - analyzes the cluster performance and provides fine-tuning insights\/recommendations. Queue analysis - analyzes queue activity by apps, vCores and memory. Cluster workload - shows the aggregated workload for all clusters. Top X - the top X applications by various metrics, for example the longest duration and most memory. Cluster KPIS - lists the basic KPIs for the cluster, including node health, apps and events over the time period. When you can specify a date range or cluster, the pull down menu for it is on the right-hand side of the Operational Insights title bar. By default, it opens showing Chargeback tab grouped by Application Type for all clusters over the last 24 hours. Click here for common features used throughout Unravel's UI. " }, 
{ "title" : "Chargeback YARN\/Impala", 
"url" : "102142-reports-opinsights.html#UUID-e9ca207e-8d0d-e8dc-d829-799d3f8f32f7_UUID-29962a5e-0129-b2a0-24f3-499f15fdba9c", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page \/ Operational insights \/ Chargeback YARN\/Impala", 
"snippet" : "The Chargeback YARN and Impala tabs are identical except the reports are limited to YARN and Impala jobs respectively. You can generate chargeback reports for multi-tenant cluster usage costs sort by the Group By options: Application Type , Real User , User , Queue , and Other (tags, tables, and rea...", 
"body" : "The Chargeback YARN and Impala tabs are identical except the reports are limited to YARN and Impala jobs respectively. You can generate chargeback reports for multi-tenant cluster usage costs sort by the Group By options: Application Type , Real User , User , Queue , and Other (tags, tables, and realuser). The default filter is Application Type . The tab is divided into three sections: Donut graphs showing the top results for the Group By selection. Chargeback report showing costs, filtered and sorted by the Group By choice. List of all YARN or Impala applications. Generate chargeback report You can set the date range and the cluster to use for the report in the Operational Insights title bar. Use the Group By to filter the information based on your selection. You must select one Group By and may select up to two. Each time you select an Other option it is added to the Group By options. If two Group By options are selected, the sort priority is noted. Click an option to deselect it. In this example the report is filtered and then sorted first Application than on the tag project . Note, that while you Group By tags, you can not by tag values. For instance, given <project, projname> you can Group By on <project> but not <projname>. Clicking a Group By selection toggles it and changes the sort priority. If you only have one group value selected you can not deselect it until you add another one, i.e., there must always be one Group By choice selected. Using this example, if you deselect Application , the tag project becomes the first priority and you cannot deselect it until you add a second choice. To specify the vCore\/Hour and Memory MB\/Hour costs fractionally enter them directly into the text box. Hovering over the chart brings up a tooltip for that selection. Click Update Report to generate the report. A new chargeback report is generated each time you change the Group By filters. However, if you change the base costs, you must click Update Report to apply them. This is a Chargeback YARN report. Click Download CSV above the table to down it as CSV file. " }, 
{ "title" : "Cluster summary", 
"url" : "102142-reports-opinsights.html#UUID-e9ca207e-8d0d-e8dc-d829-799d3f8f32f7_UUID-09f6456e-7e73-804b-440a-56fec29cdc04", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page \/ Operational insights \/ Cluster summary", 
"snippet" : "The Cluster Summary can be grouped by Applications , User or Queue . You can choose the date range and cluster in the title bar. By default, Cluster Summary opens displaying the User information. If you group by Applications, you must then choose to Sort by vCore Seconds or Memory Seconds . To downl...", 
"body" : "The Cluster Summary can be grouped by Applications , User or Queue . You can choose the date range and cluster in the title bar. By default, Cluster Summary opens displaying the User information. If you group by Applications, you must then choose to Sort by vCore Seconds or Memory Seconds . To download the displayed report, click Download Report As and chose either JSON or CSV format. Note, the current visible report is downloaded. For User or Queue this is the complete report, but for Applications it is what the sort is on. Applications You can sort applications on vCore or Memory seconds. User Queue " }, 
{ "title" : "Cluster compare", 
"url" : "102142-reports-opinsights.html#UUID-e9ca207e-8d0d-e8dc-d829-799d3f8f32f7_UUID-b9e5f8f6-9d79-0851-1094-c79162a17a34", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page \/ Operational insights \/ Cluster compare", 
"snippet" : "This tab opens displaying the cluster group by User with the Time Range and Compare with Range both set to the Last 7 Days , i.e., no comparison is displayed. Use Group By to generate the report by User or Queue . Use the Time Range and Compare With Range pull-down menus to specify the time ranges. ...", 
"body" : "This tab opens displaying the cluster group by User with the Time Range and Compare with Range both set to the Last 7 Days , i.e., no comparison is displayed. Use Group By to generate the report by User or Queue . Use the Time Range and Compare With Range pull-down menus to specify the time ranges. Any deviation in metrics across the time ranges is highlighted (3). A green highlight with an upward arrow indicates an increase in usage, while red with a down arrow denotes a decrease. If the Time or Compare With range is invalid for the Group By choice the row for that time range is dashed (2). " }, 
{ "title" : "Cluster optimization", 
"url" : "102142-reports-opinsights.html#UUID-e9ca207e-8d0d-e8dc-d829-799d3f8f32f7_UUID-2318aa98-dca1-1eac-8bb2-366340279c2e", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page \/ Operational insights \/ Cluster optimization", 
"snippet" : "The OnDemand package must be installed to use this report. This report analyzes your cluster workload over a specified period. It provides insights and configuration recommendations to optimize throughput, resources, and performance. Currently, this feature only supports Hive on MapReduce. You can u...", 
"body" : "The OnDemand package must be installed to use this report. This report analyzes your cluster workload over a specified period. It provides insights and configuration recommendations to optimize throughput, resources, and performance. Currently, this feature only supports Hive on MapReduce. You can use these reports to: Fine tune your cluster to maximize its performance and minimize your costs. Compare your cluster's performance between two time periods. Reports are generated on an ad hoc or scheduled basis. All reports are archived and can be accessed via the Reports Archive tab. The tab opens displaying the last report, if any, generated. Download or generate a report Click Download JSON to download the displayed report in JSON. To download a prior report go to Reports Archive . Click Generate New Report , the default is one day. To change the date click on it for the date picker to select a new Date Range . Click Run . Running replaces Run and a countdown is displayed until Unravel starts collecting the data. Generate New Report pulsates blue until the report is completed. When the report is successfully generated, a light green bar is displayed. Click Schedule instead of run to schedule the report at some future date and time. You can schedule your report to run once or regularly. All reports (successful or failed attempts) are in the Reports Archive . Optimization report The Report has three sections. Header Contains the basic report information author, time run, and dates used to generate the report. KPIs Number of Jobs: Per day average Number of vCore Hours: Per day average Number of MapReduce Containers % containers for Map % containers for Reduce Amount of memory (in MB) from of MapReduce containers % containers from Map containers % containers from Reduce containers The KPIs are a per-day average for the number of days in the report. In this case we generated a report for a two day period. All the insights\/recommendations are based upon the analysis of all jobs, in this case 113. Insights\/Recommendations This section contains a tab for each app type with the relevant properties under consideration for tuning. These are cluster wide properties , and they are the defaults for all apps. However, you can override these properties on an app by app basis. MapReduce : mapreduce.map.memory.mb mapreduce.reduce.memory.mb mapreduce.input.fileinputformat.split.maxsize mapreduce.job.reduce.slowstart.completedmaps Hive : hive.exec.reducers.bytes.per.reducer hive.exec.parallel You can expand the insight tile to the full width of the window. Further, below we go into greater detail for two of the insights. Insight\/Recommendations tile details Tune the size of the map containers Each tile is entitled with the property being tuned. This image is the expanded view of the first tile. Immediately beneath the title is the property name, in this case mapreduce.map.memory.mb. Click on the to bring up related properties. Next ( 1 ) is the tuning suggestion (1460), the projected impact (High) and the effect on the current jobs. In this case the recommendation effects 51% of the total jobs. The final section has the analysis information. The default ( 2 ) is the current property value (8192) and the percent of the apps that are currently using that value (51%). Click on the to see the frequency map showing the job distribution by memory usage. As expected 51% of the jobs (58) used the default, while 33% (37) used 512 MB with the remaining jobs distributed across the remaining values. The graph shows Unravel's analysis of the property potential values. It shows each candidate (proposed value) and the effect it had on the % of memory saved for the input workload and the % of jobs from the workload that would still run with the candidate . In this case we want to maximize the ability to run jobs while minimizing the memory allocation. When there are tuning instructions it is noted above the graph (3). Click on to display the instructions explaining where and how to set the property and any additional information you should take into consideration. You'll notice that all the \"related\" properties and their relationship to what we're tuning are discussed here. Tune the number of the map containers Tune the number of reduce containers in Hive queries This analysis a tuning suggestion, instructions and additional information. Click Additional Info to see its contents. In this case, the information was simply informative. There can be cases where tuning suggestions for specific apps are offered. " }, 
{ "title" : "Queue analysis", 
"url" : "102142-reports-opinsights.html#UUID-e9ca207e-8d0d-e8dc-d829-799d3f8f32f7_UUID-0cd02e95-7c2c-e8ab-61f3-f8c94ec56c69", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page \/ Operational insights \/ Queue analysis", 
"snippet" : "The OnDemand package must be installed to use this report. You can generate a report of active queues for all your clusters or just one. The report analyzes queue activity by apps, vCores, memory, and disk. As with all reports, it can be generated on an ad hoc or scheduled basis. The tab opens displ...", 
"body" : "The OnDemand package must be installed to use this report. You can generate a report of active queues for all your clusters or just one. The report analyzes queue activity by apps, vCores, memory, and disk. As with all reports, it can be generated on an ad hoc or scheduled basis. The tab opens displaying the last report, if any, generated. Reports are archived and can be accessed via the Reports Archive tab. Generate a report Click New Report to create new report. Enter the History (Date Range) , the default range is one week. Use the Cluster pull down menu to select from your available clusters. Click Run . Running replaces Run and a countdown is displayed until Unravel starts collecting the data. A green banner bar is briefly displayed noting that \"Queue Analysis Started\" and New Report pulsates blue while the report is being generated. If Unravel is unable to start the report or failed to produce a report, a red banner bar is displayed noting report wasn't started along with any error messages. The time to generate a report varies based upon the time range selected and number of active queues on the cluster. Click Schedule to run the report at some future date and time. You can schedule your report to run once or on a regular basis. All (successful or failed attempts) are stored in the Reports Archive . Analysis report If the report was successfully generated a light green bar appears and a table listing all the queues existing during the time range is displayed. The table lists each queue with its KPIs average ( Apps Running , vCores Allocated , and Memory Allocated ). You can sort the queues on any column; by default the queues are sorted by name in ascending order. Sorting the list by average KPIs lets you identify the most or least active queue and focus on them instead of scrolling though the complete list of queues. Click in the Filter By box for the queue list and to select the queues to filter on. Click in a queue's row to download the report. Click in a queue's row to see its Applications , vCore Usage , and Memory Usage graphed. When monitoring a MapR Cluster a fourth graph, Disk Usage , is displayed. Click the selection box to chose from the available metrics to graph. You can select up to two metrics for each graph. Clicking within a graph opens a cluster view Operations > Usage Details > Infrastructure for that point in time. Hovering over a point in graph brings up a pop-up in all graphs displaying metrics and values along with the averages for the metric for that point in time. Click to expand the graphs to window width. The relationship between the graphs still exist, hovering over a graph bring ups the data for that point in time for all the graphs. The expanded graph has its metric legend beneath it. Hover over a metric to see its full definition and the current impact of the metric on the cluster. In this example the app's graph is expanded. Click to return the graph to its original size. " }, 
{ "title" : "Cluster workload", 
"url" : "102142-reports-opinsights.html#UUID-e9ca207e-8d0d-e8dc-d829-799d3f8f32f7_UUID-3e1f0bc9-65e8-4885-dbe9-94492edc2a10", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page \/ Operational insights \/ Cluster workload", 
"snippet" : "Displays your cluster's YARN apps' workload across a date range using the following views: Month - by date, for example, October 10. Hour - by hour regardless of date, for example, 10.00 - 11.00. Day - by weekday regardless of date, for example, Tuesday. Hour\/Day - by hour for a given weekday, for e...", 
"body" : "Displays your cluster's YARN apps' workload across a date range using the following views: Month - by date, for example, October 10. Hour - by hour regardless of date, for example, 10.00 - 11.00. Day - by weekday regardless of date, for example, Tuesday. Hour\/Day - by hour for a given weekday, for example, 10.00 -11.00 on Tuesday. You can filter each view by App Count , vCores Hour , and Memory Hour . To measure the vCores or Memory Hour usage is straightforward; at any given point the Memory or vCore is being used or not. The App Count isn't a count of unique app instances because apps can span boundaries, i.e., begin and end in different hours\/days. The App Count reflects the apps that were running within that interval up to and including the boundary, i.e., date, hour, day. Therefore, an app can be counted multiple times in time slice. On multiple dates, for example, October 11 and 12. In multiple hours, for example, 10 PM, 11 PM & 12 AM. On multiple days, Thursday & Friday. In multiple hour\/day slots. This results in anomalies where the Sum(24 hours in Hour\/Day App Count) > Sum(App Counts for dates representing the day) . For instance, in the below example: App Count for Wednesdays (October 10, 17, and 24) = 2492, and App Count across Hour\/Day intervals for Wednesday = 2526. We point this out not because it necessarily has a significant impact in how you can use the data, but to inform you such variations exist. The tab opens in the Month view filtered on App Count for the past 24 hours. Use the Date Range date picker to change the range. We suggest using a short range as the longer the range the more processing time consumed. Click App Count , vCores Hour , and Memory Hour to change the display metric. The metric you select is used for all subsequent views until changed. Click View By to change between views. Immediately above the graph it textually notes the metric being used for the time range. When the date range is greater than one day the Hour , Day , and Hour\/Day views allow you to display the data by either as an Average or Sum . See Drilling Down below for information on how to retrieve the detailed information within each view. Hour, day and hour\/day view These graphs don't link jobs to any specific date at the graph level. For instance, the Hour graph shows that 856 jobs ran at 2 AM (between 2 AM and 3 AM); the Day graph that 2,492 jobs ran on Wednesday, and the Hour\/Day that 68 jobs ran at 2 am on a Wednesday. But none of these graphs directly indicate the date these jobs ran on. Only the Month view visually links job counts to a specific date; above we see October 10 had an app count of 822. Each view opens using the metric selected for the prior view. For instance, if vCores Hour is used to display Month and you switch to Day it is filtered using vCores Hour . When the DATE RANGE spans multiple days, you have the choice to display the data as either the: Sum - aggregated sum of job count, vCore or memory hour during the time range (default view). Average - Sum \/ (# of Days in Date Range). Day view Displays the jobs run on a specific weekday. Hover over an interval for its details. Click the interval to drill down into it. Hour\/Day view This view shows the intersection of Hour and Day graphs. The Hour graph showed 856 jobs ran between at 2 AM - 3 AM while the Day graph (immediately above) that 2,492 jobs ran on Wednesday. Below we see that 68 of Wednesday's jobs (2.7%) were running between 2 AM - 3 AM. " }, 
{ "title" : "Month view", 
"url" : "102142-reports-opinsights.html#UUID-e9ca207e-8d0d-e8dc-d829-799d3f8f32f7_N1558589178395", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page \/ Operational insights \/ Cluster workload \/ Month view", 
"snippet" : "Displays the jobs run on the particular date. The color indicates how the day's load compares with the other days within the date range. The day with the least jobs\/hours is , while the days with the highest load are . Therefore, the color of any particular day varies in context to the other days be...", 
"body" : "Displays the jobs run on the particular date. The color indicates how the day's load compares with the other days within the date range. The day with the least jobs\/hours is , while the days with the highest load are . Therefore, the color of any particular day varies in context to the other days being displayed, e.g., when only one day is displayed it is colored . Use Previous and Next in the month's title bar to navigate between months. " }, 
{ "title" : "Hour view", 
"url" : "102142-reports-opinsights.html#UUID-e9ca207e-8d0d-e8dc-d829-799d3f8f32f7_N1558588881152", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page \/ Operational insights \/ Cluster workload \/ Hour view", 
"snippet" : "Breaks out information by hour. The interval label indicates the start, i.e., 2 AM is 2 AM - 3 AM. Hover over an interval for its details. Click the interval to drill down into it....", 
"body" : "Breaks out information by hour. The interval label indicates the start, i.e., 2 AM is 2 AM - 3 AM. Hover over an interval for its details. Click the interval to drill down into it. " }, 
{ "title" : "Drilling down in a workload view", 
"url" : "102142-reports-opinsights.html#UUID-e9ca207e-8d0d-e8dc-d829-799d3f8f32f7_N1558588459131", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page \/ Operational insights \/ Cluster workload \/ Drilling down in a workload view", 
"snippet" : "Click an interval to bring up its information. In our example, we selected October 11 in the Month view which was filtered on App Count (921 apps). A list breaking out the jobs by app type is displayed. Below we see all 921 were MR jobs. Click to display User and Queue details. User is displayed by ...", 
"body" : "Click an interval to bring up its information. In our example, we selected October 11 in the Month view which was filtered on App Count (921 apps). A list breaking out the jobs by app type is displayed. Below we see all 921 were MR jobs. Click to display User and Queue details. User is displayed by default; click Queue to see all the queues. In this case there are two users, HDFS (910 jobs) and ROOT (11 jobs). Click (job details) to see the running apps for that row. When there are multiple choices shown, Unravel notes which detail is being displayed by highlighting the row. Below there are three options: App Type: MR User: HDFS User: ROOT We selected the user ROOT so its row is highlighted. Immediately above table is noted what's being displayed. See Applications > Applications for more information on the table. Click an app to bring it up in its APM. When you change the metric ( App Count , vCores Hour and Memory Hour ) the window reverts to displaying the graph. " }, 
{ "title" : "Top X", 
"url" : "102142-reports-opinsights.html#UUID-e9ca207e-8d0d-e8dc-d829-799d3f8f32f7_UUID-c371dc42-22fa-72f0-77de-c8671b81dc77", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page \/ Operational insights \/ Top X", 
"snippet" : "The OnDemand package must be installed to use this report. This tab generates two reports: The top X Hive, Impala, and Spark apps for the following categories: Longest Duration: Time to completion. Highest Disk I\/O: Summary of total dfs bytes read and written. Highest Cluster Usage: Summary of map\/r...", 
"body" : "The OnDemand package must be installed to use this report. This tab generates two reports: The top X Hive, Impala, and Spark apps for the following categories: Longest Duration: Time to completion. Highest Disk I\/O: Summary of total dfs bytes read and written. Highest Cluster Usage: Summary of map\/reduce slot duration. Highest CPU Usage: vCore seconds (Hive on Tez not supported) Highest Memory Usage: Memory seconds (Hive on Tez not supported) User Report: which is a Top X report mailed to specific Users. (Available in Unravel 4.5.3.0 and later.) Top X Apps Click New Report to generate the Top X report by apps. Use the pull down menu to change the History (Date Range) . (When you schedule a user report your choice of Date Range is constrained.) Enter the number of apps ( Top X ) to include in the report. To filter by a subset of Users , Real Users , or Queues click in the text box below it for a list of values. To filter on Tags (when available), select the box next to the tag and then click in the selection box for the available values. Click a value to select it; once selected to deselect it click the x next to it. You can select zero or more values. Click Run . Running replaces Run and a countdown is displayed until Unravel starts collecting the data. A dark green banner is briefly displayed noting that \"Top X Started\". New Report pulsates blue while the report is being generated. When Unravel is unable to start the report, a red banner containing an error message is displayed. When the report is successfully completed, a light green bar appears noting \"Top X Completed Successfully\". The report displays up to the top X apps per app type. The number of apps displayed can vary by app type. You can download the report by clicking the Download JSON button. The downloaded report is unfiltered and contains the information for Hive, Impala, and Spark. If Unravel was unable to generate the report, an error message in a light red bar is displayed and New Report becomes orange. Click the Filter By option to switch between the app types. The first tile is Top X Report: Input Parameters . The number of apps ( Top X ) and Date Range are always listed since they are required. Any optional parameters selected are also listed. On the right-hand side is the Report Id which the report is archived under. The report itself is composed of three tiles. (Note: the information can vary by app type). The Applications tile lists by app type: Total Apps: Number of apps on the cluster. Successful Apps: Number of apps that have successfully completed. Failed Apps: Number of apps that have failed to complete. Others: Number of apps that were killed, or those that are in a pending, running, waiting, or unknown state. Immediately beneath the Application Summary are five tables showing the top X apps for each category. Specific apps can be listed in multiple tables. If there are less than X apps, each table contains all the apps sorted on the metric. Regardless of app type, the table has the following columns: Status: The status of the app: success, failed, killed, or running. User: The app's owner. Real User: The User who actually submitted the job, for example, jdoe@company.com. The real user may be different from User . ID: The app's ID, hover over it for the full id. CPU: The app's CPU usage over the lifetime of the app. Memory: The app's memory usage. # Events: The total events the app had. Tags: Tags associated with the app. If you are filtering based upon tags only apps with matching tags are displayed. All the tags for the app are displayed. App Parent: App's parent. Each app type also has app specific columns, specifically Hive : Query Snippet and App Parent (links to the app's parent when applicable). Impala : Query Snippet . Spark : # Spark Stages and # Spark Tasks . No Data Found is listed when there is no information for the available for the app type. The longest duration, highest CPU usage, and highest memory usage tables These tables vary by app type, but within the app type they are the same. Each table displays the top X apps for the metric. Examples: Hive Impala Spark The highest disks I\/O usage and highest cluster usage tables These tables are similar to those above except the DURATION column is replaced by the metric. Examples: Hive Impala Spark Resources This tile varies by app type, but always has up to top ten Queues sorted by App Count . Examples: Hive Impala Spark Data Lists the Total Read I\/O and Total Write I\/O across all apps for the app type. Example Spark report. User report This report is designed for a variety of end users, for instance: App developers who can use it to determine what apps need attention to improve duration, resource usage, etc. Team leaders or managers who can use it to track how the team uses resources and identify user's who overuse resources (rogue users). It provides a concise and clear view of: How apps are performing on the platform. The resources the apps are using. The number of recommendations or insights Unravel has for the app. This report is like the Top X report above except the columns are ordered differently. Click Schedule User Report to bring up the Report:User modal. You must enter a Schedule Name . The Scheduling information defaults to Daily and the current time. Change the schedule to the time you want the report to run and how often. At the bottom is a table listing the configurations you have added. The table lists all the available tags and you can see what has been selected for each user. You can add multiple configurations but must have at least one. See Adding configurations manually for using Add New Configuration and Bulk Upload for adding multiple configurations via a CSV file. Once you have finished, click Save Schedule. Click to close the modal; if you close it before saving all your input is lost. Adding a configuration manually Click Add New Configuration , the modal is like the Top X one above except you have a limited History (Date Range) and an Email text box. You can use the default settings, but you must enter an email. Multiple configurations do not have to match, but each configuration must have a unique email. If you want to send multiple reports to the same User you must create a new User report for the configuration. As with all reports, when you do not select an option, the report is left unfiltered on that option. For example, if you don't select a subset of Users , a report is generated for the top X across all users. In the example above, the two configurations only have the Date Range and Top X number in common. Once a configuration is entered, click to edit it and to delete it. Bulk Upload Import Data from CSV lets you upload a CSV file containing configurations for multiple users. Unlike when you manually enter configuration, you cannot specify a Date Range as Unravel automatically sets it to seven days. Unravel parses the complete file, rejecting any malformed entries, or those containing a duplicate email. The new report modal lists the possible columns, these can vary based by cluster. Here we have two user defined tags, dept and project. For each column you need to know the possible values. (When you enter the configuration manually Unravel provides you with all possibilities.) In this example, there are eleven possible columns as the Date Range and Action columns are irrelevant. You do not have to include (define) all the column headers or order them the same. However, whatever columns\/tags you include must be defined for all entries in the order you chose. You must define an email column. Any column\/tag which is not added to the CSV, or left undefined for a configuration is unfiltered. For instance, if Users is undefined, the report is generated across all users. CSV Format: Col header1,Col header2,Col header3, ... \/\/ up to the number of possible options\nentry1, entry2, entry3, ... \/\/ up to the number of columns defined\n In the following example, the CSV file only defines five columns Email (required), Top X , Users , RealUsers , and Dept . Therefore, the reports are not filtered by Queue , Project , DBS , Inputtables or Outputtables . topx,email,Users,realUsers,dept\n6,a@unraveldata.com,hdfs,hdfs,eng \/\/ all columns are defined, entry is accepted\n10,b@unraveldata.com,root,,eng \/\/ realUsers is defined as empty, entry is accepted\n7,b@unraveldata.com,root,,eng \/\/ duplicate email, the entry is rejected \n7,z@unraveldata.com,,,test,root,fin \/\/ 2 extra columns were added, the entry is rejected\n7,y@unraveldata.com,root,,eng \/\/ realUsers is defined as empty, entry is accepted The UI lists the results of the CSV import. In this case only lines 1, 2, and 5 were loaded. See above for the explanation. Once you have loaded the configuration you can click to edit it a configuration manually or to delete it. Example User report " }, 
{ "title" : "Cluster KPIS", 
"url" : "102142-reports-opinsights.html#UUID-e9ca207e-8d0d-e8dc-d829-799d3f8f32f7_UUID-6053aaef-bb68-5410-eb85-ece9f8230e3e", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page \/ Operational insights \/ Cluster KPIS", 
"snippet" : "The OnDemand package must be installed to use this report. This report lets you quickly see the overall health of your cluster. You can also schedule it to be emailed to users on a regular basis. The report has eight tiles. Overall Health of Platform Resources Nodes Usage (HDFS) across the cluster Y...", 
"body" : "The OnDemand package must be installed to use this report. This report lets you quickly see the overall health of your cluster. You can also schedule it to be emailed to users on a regular basis. The report has eight tiles. Overall Health of Platform Resources Nodes Usage (HDFS) across the cluster YARN Consumers Impala Consumers By default, this page displays the data for the past hour. You can change the time range by clicking Schedule Report , then click History (Date Range) and select the available time period. Click Apply . The report is updated for the time period selected. If you wish to schedule a report, select your date range and then click Schedule . Once the report is generated it is sent to all listed recipients. See Scheduling for an explanation Overall health of the cluster KPIs across the entire cluster. Resources Graphs the available and allocated vCores and memory for the entire cluster. Nodes Graphs the total number of nodes and the breakdown by node status, active, lost, unhealthy, decommissioned and rebooted. Total = Active + Unhealthy Where: Active: currently running and healthy nodes. Unhealthy: currently running and unhealthy nodes. Usage (HDFS) across the cluster (forecasting report) This is the last Reports > Data Insights > Forecasting report, for HDFS disk capacity. YARN and Impala consumers These tables show the Databases and all the tags associated with the YARN or Impala jobs The following examples have eight sections. By DBS: Databases the YARN or Impala apps use. By Dept: Tag key. By Inputtables: Input tables. By Outputtables: Output tables. By Project: Tag key. By Realuser: User who submits the app. By unravel.app.name: App name. By User: User who runs the app. Each section of the table lists all its members. For instance, DBS contains a row for each database while Project contains all the values for the tag. When no data is found only the By User tables are shown for YARN and Impala since the user is the basis of the YARN and Impala tables. YARN consumers Table columns Section Name: Each row lists a section member, for example, DBS will list all the databases. App Count: Number of apps which accessed the section member. CPU Hours: Aggregated CPU hours across all the apps accessing the section member. % Total CPU Hours: Percentage of the (Total CPU Hours)\/(Aggregated CPU Hours across all apps). Memory Hours: Aggregated member hours time across all the apps accessing the section member. % Memory Hours: Percentage of the (Total Memory Hours)\/(Aggregated Memory Hours across all apps). Impala consumers Table Columns Section Name: Each row lists a section member, for example, DBS has a row for each database. App Count: Number of apps which accessed the section member. Total Processing Time Hours: Aggregated processing hours across all the apps accessing the section member. % Total Processing Hours: Percentage of the (Total Processing Time Hours)\/(Aggregated Processing Time Hours across all apps). Memory Hours: Aggregated member hours time across all the apps accessing the section member. % Memory Hours: Percentage of the (Total Memory Hours)\/(Aggregated Memory Hours across all apps). Example report sent to user " }, 
{ "title" : "Data insights", 
"url" : "102143-reports-datainsights.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page \/ Data insights", 
"snippet" : "You must have MySQL installed as your Unravel DB for this page to be populated, including all the OnDemand reports. In order to use Forecasting, Small Files, and File Reports you must have the OnDemand package installed. As of 4.5.1 Top X has moved to the Operational Insights tab. The first two tabs...", 
"body" : "You must have MySQL installed as your Unravel DB for this page to be populated, including all the OnDemand reports. In order to use Forecasting, Small Files, and File Reports you must have the OnDemand package installed. As of 4.5.1 Top X has moved to the Operational Insights tab. The first two tabs provide data level insights including a snapshot of tables and partitions over the last 24 hours within a historical context. Overview - gives a quick view into the tables' and partitions' sizes. Details - drills down into the tables. See Hive Metastore Configuration for information on necessary configuration settings to populate these tabs. Tables and partitions have color coded labels when applicable: Hot ( ), Warm ( ), or Cold ( ). The label definitions are defined via the Configuration . The last four provide disk management insights to help you manage your disk usage both in terms of capacity and cluster performance. Forecasting - forecasts future disk capacity requirements based upon past performance. Small Files - generates a list of small files based upon specified criteria. File Reports - similar to Small Files, except canned reports for large, medium, tiny, and empty files. Click here for common features used throughout Unravel's UI. " }, 
{ "title" : "Overview", 
"url" : "102143-reports-datainsights.html#UUID-ae104fa5-4d52-b229-7734-e60a696ac7f5_UUID-da3a2659-9406-f737-ba51-eff24827b2a3", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page \/ Data insights \/ Overview", 
"snippet" : "The Overview Dashboard gives a quick view into the tables' and partitions' sizes, usage, and KPIs. It has two sections. Table KPI Partition The time period used to populate the page is shown in the upper right-hand corner and the tool tips. Tables & partitions tiles Both Table and Partition KPIs sec...", 
"body" : "The Overview Dashboard gives a quick view into the tables' and partitions' sizes, usage, and KPIs. It has two sections. Table KPI Partition The time period used to populate the page is shown in the upper right-hand corner and the tool tips. Tables & partitions tiles Both Table and Partition KPIs sections contain: # Accessed: Number of Tables\/Partitions accessed, # Created: Number of Tables\/Partitions created, Size Created: Size of Tables\/Partitions created, and Total Number: Total Number of Tables\/Partitions currently in the system. The Table KPIs also contains: Accessed Queries: Total number of queries accessing the tables, and Total Read IO: Total Read IO due to accessing the tables. Donut charts These display the Current Label Distribution for the tables\/partitions. See Configuration for the operating definition of the labels. The graph shows the relationship between the labels; hover over a label to see the total of tables\/partitions with that label. Below we see that three tables are warm. " }, 
{ "title" : "Details", 
"url" : "102143-reports-datainsights.html#UUID-ae104fa5-4d52-b229-7734-e60a696ac7f5_UUID-c9248b76-9ded-a809-6209-935421d95c22", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page \/ Data insights \/ Details", 
"snippet" : "The details tab has two sections, a graph and a table list. By default, the graph uses the Total Users metric and displays the first table in the list. The list is sorted on Total Users in descending order. You can also use the metrics Total Users, Total Apps, or Total Size to display the graph. The...", 
"body" : "The details tab has two sections, a graph and a table list. By default, the graph uses the Total Users metric and displays the first table in the list. The list is sorted on Total Users in descending order. You can also use the metrics Total Users, Total Apps, or Total Size to display the graph. The Total Apps metric (corresponding Apps column in the table) is the total number of Hive and Impala queries on the table. Graph Use the Metric pull down menu (1) to select Total Users , Total Apps , or Total Size as the metric to graph. Click Reset Graph (1) to revert to displaying first table using the Total Users metric. The menu bars allow you to print or download the graph. You can select one or more tables to graph by selecting box (2), next to the table's name. You can select tables over multiple pages, in the image below shows five tables yet only three have been checked on the page showing. The other two tables were selected from other pages. Table list You can Search by string; any table matching or containing the specified name\/string is displayed. Use Show (1) to specify the label type to use for displaying the tables. All is selected below, so every table is shown. You can sort the list by the various metrics in ascending or descending order. By default, the list is sorted on Read IO in descending order. If you have selected a table, the More Info glyph is available. Click it to display the Table Detail pane. Click Configure Policy (2) to edit the label rules or Download CSV to download the table (2). Table detail This view Summarizes table usage and access metric. Lets you to browse trends (KPIs). Drill down into applications that used the table. Lists both Hive and Impala queries. The first table in the list above is used for the examples below. The panel's top row lists the table name, start date\/time and the name\/path. Hover over the name\/path to display the complete path. Three KPIs are displayed: Users , # Apps, and Size . There are three tabs, Table Detail , Partition Detail , and Retention Detail (1); the default view is the Table Detail . Use the Metric (2) pull down menu to select Total Users , Total Apps, or Total Size as the metric to graph. The Application Detail lists the applications that accessed the table in the given time range. See the Application Tab section for detailed information on its format. Below the table shows both Hive and Impala queries. Partition detail Click the Partition Detail tab for partition information. The top left of the tab notes the number of partitions loaded, the displayed partition's name, and the view type ( Partition Size or MR jobs ). By default, the 100 latest partitions are loaded with the first partition listed graphed in the Partition Size view (1). To load all the partitions click Load All Partitions (2). To switch to the MR Jobs view click MR Jobs (2). Chose the partition to graph by selecting the check box to the left of the partition's name. Hovering over the partition name displays the complete name\/path. The partition list can be sorted on Last Access date, Created date, Current Size, or Users . Hovering over the Users number brings up the list of user(s) who accessed the partition. Retention tab This graph initially displays the number of Applications ; the pull down menu lets you switch to the Partition Access View . Listed below the graph are the results from the partition analysis. " }, 
{ "title" : "Configuration", 
"url" : "102143-reports-datainsights.html#UUID-ae104fa5-4d52-b229-7734-e60a696ac7f5_N1553871184841", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page \/ Data insights \/ Details \/ Configuration", 
"snippet" : "You can define the rules for labeling a Table\/Partition either Hot , Warm , or Cold . These rules are used for the Donut chart and in the Details tab. While the labels are immediately associated with the Tables\/Partitions, the Overview Dashboard donut charts typically populate within 24 hours. You a...", 
"body" : "You can define the rules for labeling a Table\/Partition either Hot , Warm , or Cold . These rules are used for the Donut chart and in the Details tab. While the labels are immediately associated with the Tables\/Partitions, the Overview Dashboard donut charts typically populate within 24 hours. You access this modal pane from the Data > Details tab. The rules are defined per label and you can define up to two rules per label. To define a rule: From the pull down menus: Chose Age (days) or Last Access (days) , and Chose the comparison operator: <= or >=. Enter the number of days. To add a second rule: Click the Plus glyph, Select the AND or OR operator from the pull down menu, and Repeat steps 1 & 2. To delete a second rule, click the Minus glyph. Click Save . " }, 
{ "title" : "Forecasting", 
"url" : "102143-reports-datainsights.html#UUID-ae104fa5-4d52-b229-7734-e60a696ac7f5_UUID-bd5ca3aa-03b1-8dd8-4e02-dd0d6253d1ff", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page \/ Data insights \/ Forecasting", 
"snippet" : "The OnDemand package must be installed to use this report. See here for properties which control this report. It currently only works on Cloudera (CDH) and Hortonworks (HDP). This report helps you monitor CPU, Memory and HDFS disk capacity usage and plan for future needs. Unravel uses your historica...", 
"body" : "The OnDemand package must be installed to use this report. See here for properties which control this report. It currently only works on Cloudera (CDH) and Hortonworks (HDP). This report helps you monitor CPU, Memory and HDFS disk capacity usage and plan for future needs. Unravel uses your historical usage to extrapolate capacity trends allowing you to more effectively plan for, and allocate your disk resources. Unravel stores up to two years of data. Each time you generate a report, Unravel stores any new data it generates. This lets you generate reports based upon a larger pool of data for more accurate forecasting. The tab opens displaying the last forecasting report, if any, generated. These graphs display the trend (blue area) from the historical range start date to the forecast range end date (x-axis). The trend shows the upper and lower trend, with the dark blue line indicates the trend. The y-axis is determined by your actual physical CPU, memory, and disk capacity. Click to download the graph is CSV format. Click to expand all the graphs to full width. To generate the report use the data picker to set the History (Date Range) and the number of Forecasting (#Days) . Click Run to generate the report or Schedule to generate it on a regular basis. (See Scheduling Reports .) While Unravel prepares to generate the report Run is replaced with Running and a countdown appears above it. Once Unravel starts the generation the pop-up closes and the New Report button pulsates blue. A light green bar appears when the report was completed successfully and results are displayed. Upon failure the bar is light red and the New Report button becomes orange. The New Report remains orange until a new report is successfully generated. All reports, whether scheduled or ad hoc, are archived. Successful reports can be viewed or downloaded from the Report Archives tab. " }, 
{ "title" : "Small files", 
"url" : "102143-reports-datainsights.html#UUID-ae104fa5-4d52-b229-7734-e60a696ac7f5_UUID-78a004ea-1bcc-4f5a-acfb-30c815bfa1aa", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page \/ Data insights \/ Small files", 
"snippet" : "The OnDemand package must be installed to be able to use this report. It requires HDFS privileges and currently only works on HDP\/CDH. If you can't grant HDFS privileges, you must configure these properties. Each small file is accessed by a single mapper. Therefore, a large number of small files can...", 
"body" : "The OnDemand package must be installed to be able to use this report. It requires HDFS privileges and currently only works on HDP\/CDH. If you can't grant HDFS privileges, you must configure these properties. Each small file is accessed by a single mapper. Therefore, a large number of small files can lead to a large number of mappers. Mappers are costly to run and drive up your app's costs. This report helps you identify users who create\/use an excessive amount of small files. You can use this information to take corrective action such as: Combine multiple files into large files. Notify, limit, or block users who create or use an excessive amount. Taking action Corrects and prevents future performance degradation. Lowers your costs to run apps. The tab opens showing the last report, if any, that was generated. It is sorted in descending order of the total number of small files in the directory. The report's parameters are listed above the table headings. You can search the table by path list, any path matching or containing the search string is displayed. Click Download CSV to download the report as a CSV file. Click New Report to generate a new report. The parameters are: File Size (bytes): The absolute file size in a directory. To use a custom Average files size (bytes), select the check the box and enter Average file size (bytes) to use. Prior to 4.5.3.0 the check box is selected by default. Minimum # of Small Files: Which are in the directory. # of Directories to Show: Is the maximum number of directories to display. Advanced Options: Min parent directory depth: Minimum depth to start at, root + x descendants, i.e., 0=root, 1=root's children (\/one), etc. Max parent directory depth: Maximum depth to end at, root + x descendants, i.e., 1=root's children (\/one), 2=root's grandchildren, (\/one\/two), etc. Drill down sub-directories: Determines how\/where the files are listed. Yes (default): lists the file in all its ancestor's list. No: list file in its directory list only. Min parent directory depth and Max parent directory depth must be between 0 and 50. Click Run to generate the report and Schedule to generate the report on a regular basis. See Scheduling Reports . While Unravel prepares to generate the report Run is replaced with Running and a countdown appears above it. Once Unravel starts the generation the pop-up closes and the New Report button pulsates blue. A light green bar appears when the report was completed successfully and results are displayed. Upon failure the bar is light red and the New Report button becomes orange. Click Download CSV to download the current report being displayed. All reports, whether scheduled or ad hoc, are archived. Successful reports can be viewed or downloaded from the Report Archives tab. " }, 
{ "title" : "Files report", 
"url" : "102143-reports-datainsights.html#UUID-ae104fa5-4d52-b229-7734-e60a696ac7f5_UUID-05af8635-8d7f-90fc-4685-4a5733577dc1", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page \/ Data insights \/ Files report", 
"snippet" : "The OnDemand package must be installed to use this report. It currently only works for CDH\/HDP and requires HDFS administrator privileges. If you can't grant HDFS administrator privileges to unravel user, please refer to Triggering an import of FSImage so you can generate the report. This report is ...", 
"body" : "The OnDemand package must be installed to use this report. It currently only works for CDH\/HDP and requires HDFS administrator privileges. If you can't grant HDFS administrator privileges to unravel user, please refer to Triggering an import of FSImage so you can generate the report. This report is the same as Small Files except they are automatically generated using the File Reports properties. By default, these reports are updated every 24 hours and are archived. The default size for the files are: large file is any file with more than 100 GB size, medium file is any file with 5 GB - 10 GB size, and tiny file is any file with less than 100 KB size. Click on the size buttons ( Large , Medium , Tiny , and Empty ) to view the report. You can search by string; any directory matching or containing the string is displayed. Click Download CSV to download the report. " }, 
{ "title" : "Migration planning", 
"url" : "102144-reports-migration.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page \/ Migration planning", 
"snippet" : "Overview Migration Planning helps you to understand your current cluster's configuration, its current usage and then helps you plan your migration to cloud such that you can achieve it in a faster and more cost-efficient way. Cluster Discovery analyzes your on-prem cluster usage, workload patterns, ...", 
"body" : "Overview Migration Planning helps you to understand your current cluster's configuration, its current usage and then helps you plan your migration to cloud such that you can achieve it in a faster and more cost-efficient way. Cluster Discovery analyzes your on-prem cluster usage, workload patterns, and your hosts' hardware configuration. Clouding Mapping provides an optimal cloud topology based on your on-prem cluster and an estimate of how much it would cost. Unravel provides this information for two migration strategies. Lift and Shift is a one-to-one mapping of each existing host’s capacity (based on CPU, memory, and disk) to the closest fit on the cloud that meets or exceeds the host's hardware specs. This method provides an estimate of what your current on-prem cluster configuration would cost on the cloud. It doesn't consider workload or actual resource usage. It is the more expensive way to migrate, but it minimizes your risks associated with migrating to the cloud Cost Reduction is a-one-to-one mapping of each existing host’s actual usage (based on CPU, memory, and disk) to the closest fit on the cloud that meets or exceeds the host's usage requirements. While still a one-to-one mapping, this method is usually more cost-effective than lift and shift as it minimizes over-provisioning (the under-utilization of your host's resources). This method optimizes for cost, but not necessarily peak-usage. If your on-prem hosts are under-utilized this method is usually less expensive than lift and shift. Services and versions compatibility compares what is on your cluster and the component versions offered by the target you selected for the report. The reports are generated based upon an analysis of your cluster workload over a specific date range. You can: Analyze on-prem HDP, CDH clusters. Examine the costs of running your clusters on: Prior to Unravel 4.5.3.0 only a subset of these cloud services are available Google Compute Engine (IaaS) Google DataProc Amazon EC2 (Iaas) Amazon EMR Azure (Iaas) Azure HDInsight Limitations Your cluster must have at least seven days of metrics for Unravel to generate useful reports. Clusters running MapR Control System aren't supported. In order to use Migration Planning Reports you must have OnDemand installed, and set the cluster manager properties. See here for common features used throughout Unravel's UI. Migration Planning contains three tabs. Cluster Discovery a dashboard containing detailed information about your on-prem cluster. Clouding Mapping per Host is a report organized by host that shows the host details and the cloud instance it was mapped to. As of Unravel 4.5.3.0 the information in the Clouding Mapping per Instance tab has been merged into this tab. Services and Versions Compatibility maps the services on your cluster to either Google DataProc, Amazon EMR, and Azure HDInsight, and determines what services are both available and compatible on the cloud product. Cloud mapping per instance (deprecated Unravel 4.5.3.0) This tab provides a summary of the Clouding Mapping per Host reports. By default, the tab opens displaying the last report generated for Lifet and Shift . This report is the instance view of the migration. It shows the Instance name, # of Cores , Memory and the Number of Hosts which were mapped to the instance. There is one row for each instance Unravel mapped one or more hosts to. Click Cost Reduction to see the summation of that analysis. " }, 
{ "title" : "Cluster discovery", 
"url" : "102144-reports-migration.html#UUID-1f5e3fa4-0768-b1c0-4d37-8803ffa7c8ac_UUID-f34db69d-0b3e-756d-f15e-2e0b1e51909d", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page \/ Migration planning \/ Cluster discovery", 
"snippet" : "The dashboard provides overall information about your cluster and has six tiles. On-Prem Cluster Identity contains the cluster configuration details and host information. Overall cluster usage graphs of Applications submitted By App Type , By user , and By queue . CPU Memory A CPU\/Memory heat-map th...", 
"body" : "The dashboard provides overall information about your cluster and has six tiles. On-Prem Cluster Identity contains the cluster configuration details and host information. Overall cluster usage graphs of Applications submitted By App Type , By user , and By queue . CPU Memory A CPU\/Memory heat-map that aggregates usage by weekday, and then hour within the day. Click New Report to generate the report. Select a History (Date Range) . Before the initial report generation, the default is a seven day history. Click Run ; Running replaces Run and a countdown is displayed until Unravel starts collecting the data. A dark green bar banner is briefly displayed noting the report has started. New Report pulsates blue until the report is completed. If the report has been generated New Report remains blue, a light green bar notes the success, and the page is populated with the new data. If report generation was unsuccessful, a red bar containing an error message is displayed. The New Report button becomes orange and remains so until a report is successfully generated. The tab continues to show the last successfully generated report. All successful reports are archived . Click Download JSON to download the report. On-Prem Cluster Identity This tile contains information about your cluster, including the hosts. The Host Summary section shows the cluster's capacity across all hosts. To see each host's hardware specifications and the host's roles click . The table can be searched on host name. The potential roles are: Server: Has at least one server component, such as Zookeeper Server, HDFS. Worker: Has at least one daemon component such as HDFS DataNode, YARN NodeManager, or HBase RegionServer. Client: Has at least one client component, such as Zookeeper Client, Hadoop Client, Hive Client, etc. Cluster overall usage of applications grouped by app type, user, and queue. The donut graphs display the top 10 for each category. This example has three app types, four users, and six queues over the period analyzed. Cluster resource availability and usage The first two graphs display the cluster's CPU and memory utilization over the time period. The average usage is listed on the right-hand side of the tile bar. Hover over the parenthetical text next to the resource's name to see Unravel's analysis of your cluster's usage for that resource. Here, the CPU and Memory are both \"Very under-utilized and over-provisioned\". The heat-map is a map of the CPU\/Memory usage and capacity by a weekday and hour, e.g., Monday between 5 and 6 a.m. Each time slot is color coded to show how relatively hot the time slot is relative to the rest of the map. You can quickly see the load distribution across your cluster.age. You can filter the heatmap by CPU or memory. It opens displaying CPU. The CPU graph above noted the CPU is under-utilized and the heatmap graphically supports that analysis. " }, 
{ "title" : "Cloud mapping per host", 
"url" : "102144-reports-migration.html#UUID-1f5e3fa4-0768-b1c0-4d37-8803ffa7c8ac_UUID-06ff6661-637a-12f8-1c81-daf528e1f21f", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page \/ Migration planning \/ Cloud mapping per host", 
"snippet" : "Report Layout By default, the tab opens displaying the last report generated for Lift and Shift . Click Cost Reduction to see that report. Each host is mapped to one of the instances you selected when generating the report. Immediately below the method tab is a brief explanation of the method along ...", 
"body" : "Report Layout By default, the tab opens displaying the last report generated for Lift and Shift . Click Cost Reduction to see that report. Each host is mapped to one of the instances you selected when generating the report. Immediately below the method tab is a brief explanation of the method along with the Cloud Product\/Service used for the report. A summary for the Total Hourly Cost , Total Object Storage Required (or Local Storage ), and instance your hosts are mapped to. Immediately below a table lists the mapping for each host to a VM type. The table contains: Host: Your on-prem host. Host Role: Shows the host role, server, worker, or client. A host can have more than one role. This column is only available in Unravel 4.5.3.0 or later. Actual Usage: The host's actual resource usage. Capacity: The total capacity of the host. Recommendation: The cloud instance Unravel maps your host to. Total Cost ($\/Hour): The hourly cost of the instance. Lift and Shift This method is a one-to one mapping of each on your on-prem host's capacity to an equivalent VM type in the cloud (the mapping meets or exceeds, where necessary, your host's capacity). host that is under-utilized. Since Unravel is matching capacity, the mapped instance is also underutilized with a Total Hourly Cost of 2.211. Cost Reduction This method maps each host to an instance based on the host's actual usage not its capacity. This report maps the same host used in the Lift and Shift report. The Total Hourly Cost is $1.147, a saving of $1.064. Comparison of lift and shift to cost reduction Both Lift and Shift and Cost Reduction perform a one-to-one mapping, but Cost Reduction typically is less expensive since it maps host to a VM based upon the host's actual usage. " }, 
{ "title" : "Generating reports", 
"url" : "102144-reports-migration.html#UUID-1f5e3fa4-0768-b1c0-4d37-8803ffa7c8ac_N1552540242292", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page \/ Migration planning \/ Cloud mapping per host \/ Generating reports", 
"snippet" : "Click New Report . If you have previously generated a report, the modal opens with your prior settings. You must select: Cloud Product\/Service : Google Compute Engine (IaaS) Google DataProc Amazon EC2 (Iaas) Amazon EMR Azure (Iaas) Azure HDInsight Region: The available regions are specific to the Cl...", 
"body" : "Click New Report . If you have previously generated a report, the modal opens with your prior settings. You must select: Cloud Product\/Service : Google Compute Engine (IaaS) Google DataProc Amazon EC2 (Iaas) Amazon EMR Azure (Iaas) Azure HDInsight Region: The available regions are specific to the Cloud Product\/Service selected. Storage Type: You can choose Local Storage or Object Storage. The Storage Name is determined by the storage type chosen. One or more VM types: The list is populated based upon the Cloud Product\/Service and Region you chose. To select a subset of VM types, check the box for each VM Type . To select all types check the box in the header. Unravel maps each host to the best fit of the VM types you selected. Click Reset to clear your selections. Unravel uses publicly listed prices. You can enter a custom cost for a VM type for Unravel to use for the cost calculation for that VM type. Once you have made your selections you can either run or schedule your report. Click Run to immediately generate the report. The pop-up is greyed out and then closed once Unravel starts to generate the report. If Unravel can generate the report a dark green banner is briefly displayed at the top of the window noting the report has started; if the report can't be started a dark red banner appears with an error message. New Report pulsates blue until the report is completed. If the report is successful a light green bar appears and New Report remains blue. If Unravel couldn't generate the report, New Report becomes orange and light red bar noting the error is displayed. (Note: the button remains orange until a report is successfully generated.) To schedule the report, click Schedule . Click Download JSON to download the displayed report in the JSON. Unravel generates reports using: Lift and Shift: a one-to-one mapping of each on-prem host on the cloud based on your host's capacity . Cost Reduction: a one-to-one mapping of each on-prem host on the cloud based on your host's actual usage . " }, 
{ "title" : "Services and versions compatibility", 
"url" : "102144-reports-migration.html#UUID-1f5e3fa4-0768-b1c0-4d37-8803ffa7c8ac_UUID-b6e641d0-17dc-8c83-a750-b12dca8ef9b4", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page \/ Migration planning \/ Services and versions compatibility", 
"snippet" : "This report is only available in 4.5.3.0 and later. The report is a matrix mapping on-prem platforms to the service which are available and compatible on a cloud provider. To generate a report select New Report . Select the Cloud Product (Google DataProc, Amazon EMR, Azure HDInsight) you want to cre...", 
"body" : "This report is only available in 4.5.3.0 and later. The report is a matrix mapping on-prem platforms to the service which are available and compatible on a cloud provider. To generate a report select New Report . Select the Cloud Product (Google DataProc, Amazon EMR, Azure HDInsight) you want to create the report for. The matrix maps : Services and Versions are Compatible: The service is on your cluster, the cloud product and are compatible. ( ) Services and Versions are not Compatible: The service is on your cluster and the cloud product, but they aren't compatible. ( ) Service available in Source, but missing in Target: The service is on your cluster but not on the cloud product. ( ) Service missing in Source, but available in Target: The service is missing in your cluster but available on the cloud product. ( ) " }, 
{ "title" : "Reports archive", 
"url" : "102145-reports-archive.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page \/ Reports archive", 
"snippet" : "Note Click here for common features used throughout Unravel's UI. Lists all reports and attempts to generate a report, whether scheduled or on ad hoc basis. The status of the report is color coded; in this example, three reports succeeded while one failed. You can Filter By report type, for example,...", 
"body" : "Note Click here for common features used throughout Unravel's UI. Lists all reports and attempts to generate a report, whether scheduled or on ad hoc basis. The status of the report is color coded; in this example, three reports succeeded while one failed. You can Filter By report type, for example, Capacity Forecasting. You can Search by a string; any report containing the string in the Report ID , Report Type , or Created Date field is displayed. Successful reports can be viewed or downloaded. Click the Report ID to view and to download it and to delete it. To delete more than one report select the check box for reports you wish to delete, click on the next to the search box. To delete all reports select the check box in the header bar. Once a report is deleted it can not be recovered. Example of Capacity Forecasting CSV file Date Max Capacity Observed Capacity Trend Lower Trend Upper Trend 08\/23\/18 02:00 AM 3.172 KB 528.000 B 528.098 B 527.120 B 528.970 B 08\/29\/18 02:00 AM 3.172 KB 527.000 B 528.171 B 527.248 B 529.097 B 08\/28\/18 01:00 PM 3.172 KB 527.000 B 526.385 B 525.443 B 527.316 B 08\/27\/18 06:00 PM 3.172 KB 528.000 B 528.332 B 527.420 B 529.231 B 08\/29\/18 07:00 PM 3.172 KB 532.000 B 530.328 B 529.377 B 531.289 B 08\/26\/18 11:00 PM 3.172 KB 528.000 B 527.933 B 527.033 B 528.801 B 08\/26\/18 04:00 AM 3.172 KB 528.000 B 528.101 B 527.199 B 529.004 B 08\/22\/18 11:00 AM 3.172 KB 528.000 B 527.806 B 526.847 B 528.694 B 08\/24\/18 12:00 PM 3.172 KB 528.000 B 527.651 B 526.691 B 528.616 B " }, 
{ "title" : "Scheduled reports", 
"url" : "102146-reports-scheduled.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page \/ Scheduled reports", 
"snippet" : "Lists all the currently scheduled reports by report type: Cluster Optimization , Cluster KPIs , Capacity KPIs , Queue Analysis , Small Files , or Top X . At a quick glance you can see a report's Schedule and its New Run . If a report doesn't run daily, the schedule notes the day it runs. You can sea...", 
"body" : "Lists all the currently scheduled reports by report type: Cluster Optimization , Cluster KPIs , Capacity KPIs , Queue Analysis , Small Files , or Top X . At a quick glance you can see a report's Schedule and its New Run . If a report doesn't run daily, the schedule notes the day it runs. You can search by report type to see all scheduled reports of that type, for example, Small Files. Edit ( ) lets you alter a report's schedule . You can't change the parameters of a report. To change a report's parameters, you must create a new report. If you wish to alter an existing report, delete it and go to the appropriate tab to generate a new one. More info ( ) brings up the report's parameters. Click Close to return to Scheduled Reports. " }, 
{ "title" : "Scheduling reports", 
"url" : "102147-reports-schedulingreports.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Reports page \/ Scheduling reports", 
"snippet" : "Instead of generating your report immediately you can schedule the report to run on an ongoing basis. For any given report, you enter the necessary parameters and then click Schedule . The scheduling example below is a capacity forecasting report. You can't alter the Report name which is automatical...", 
"body" : "Instead of generating your report immediately you can schedule the report to run on an ongoing basis. For any given report, you enter the necessary parameters and then click Schedule . The scheduling example below is a capacity forecasting report. You can't alter the Report name which is automatically set to the report type, for example, capacity forecasting. You schedule the report to run at a specified time on either a Daily or weekday basis; use the pull down menu to set the weekday. By default, reports are scheduled to run daily at the same time you create the schedule. You can send an email Notification to one or more recipients to notify them the report run has started. Click Save Schedule to schedule the report. Upon saving your report appears in the Scheduled Reports tabs. Click the in the upper right-hand corner to report to the initial report page, for example, Disk Capacity. reports-schedulingreports " }, 
{ "title" : "Unravel for Azure Databricks", 
"url" : "102148-azure-databricks.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Unravel for Azure Databricks", 
"snippet" : "A single deployment of Unravel for Azure Databricks can monitor all your clusters across all your Databricks instances and workspaces. Unravel for Azure Databricks provides: A single pane of glass to help you understand your resources, infrastructure, applications, and users across Databricks instan...", 
"body" : "A single deployment of Unravel for Azure Databricks can monitor all your clusters across all your Databricks instances and workspaces. Unravel for Azure Databricks provides: A single pane of glass to help you understand your resources, infrastructure, applications, and users across Databricks instances and workspaces. Unified view across workspaces and instances. Usage breakdown and trending. Insights into and recommendation for your applications. Visibility into data usage including what tables are being accessed by which user and application, and the extent of the access (hot, warm, and cool tables). Unravel for Azure can help you: Visual and understand your resources, apps, and users across all Databricks instances and workspaces. Speed up applications. Improve the efficiency of your resource utilization. Root cause and resolve application problems. Using Unravel for Azure Databricks is likely to reduce your costs. " }, 
{ "title" : "Operations page", 
"url" : "102149-operations-databricks.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Unravel for Azure Databricks \/ Operations page", 
"snippet" : "(Azure Databricks) The Operations Page provides a synopsis of your cluster(s) and its activities. It has two tabs: Dashboard Usage Details By default, it opens showing Operations > Dashboard tab. See Common UI Features for general information about Unravel's UI. Dashboard To view the Dashboard, clic...", 
"body" : "(Azure Databricks) The Operations Page provides a synopsis of your cluster(s) and its activities. It has two tabs: Dashboard Usage Details By default, it opens showing Operations > Dashboard tab. See Common UI Features for general information about Unravel's UI. Dashboard To view the Dashboard, click Operations > Dashboard . The Dashboard provides an overview of cluster activities with links to drill down into the Job runs, resource usage, application inefficiencies, and events\/alerts. By default, it is configured to display all clusters hourly for the past 24 hours . When you interact with the page, for example, click a graph, Unravel stops refreshing the page. Click refresh to start Unravel refreshing the page. Finished jobs tile The line graphs display the successful, failed, and killed jobs for the time period , using the time increment and instance(s) specified. It textually displays the total number over the time period. Clicking on Open Section brings up am application window listing all apps. See Applications (cloud) . Running jobs tile The line graphs display the running and pending jobs for the current time. It textually displays the total number at the current time period. Resources tile Displays the Core and Memory. Clicking on opens Operations > Usage Details > Infrastructure tab. Inefficient jobs Lists events that your runs have had. The Event Name is the type of event which has occurred, the # Jobs Found is the number of jobs which have experienced the event. Click on an event to bring up a list of apps that have experienced it. The event type is noted in the upper left-hand corner. The app list is equivalent to Applications > Applications (cloud) except it only displays runs that have experienced the event. Usage details Usage Detail has two tabs: Infrastructure Nodes Unravel can pinpoint apps causing a sudden a spike in core or memory usage. This lets you easily drill down into these apps in order to understand their behavior. Whenever possible, Unravel provides recommendations and insights to help improve an app's run. All the charts and tables are automatically refreshed; however refreshing is disabled when you interact within a page to alter its display, for example, change the date range or click a point within a graph. When disabled, a button appears in Usage Details title bar. Click it to resume automatic refreshes. By default, the tab opens showing the Infrastructure tab. For all charts, click the menu bars ( ), print and download options, for example, CSV, JPEG. Click Show More to expand it. For a particular point in time, hover over chart to see a tool tip with details. Click a graph to see all apps running at that particular point in time. Nodes Graphs the total , active , and decommissioned nodes. Click within the graph to see what apps were running at that point in time. " }, 
{ "title" : "Infrastructure", 
"url" : "102149-operations-databricks.html#UUID-39459b8b-d47d-de7b-71f7-37bda71dffba_N1553155703543", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Unravel for Azure Databricks \/ Operations page \/ Infrastructure", 
"snippet" : "This tab contains four graphs. The upper two list available and allocated Cores and Memory . The bottom two shows the Cores and Memory by User or Workspace . Clicking within a chart displays the apps running for that point in time. For bottom two graphs, click User or Workspace to change the view. Y...", 
"body" : "This tab contains four graphs. The upper two list available and allocated Cores and Memory . The bottom two shows the Cores and Memory by User or Workspace . Clicking within a chart displays the apps running for that point in time. For bottom two graphs, click User or Workspace to change the view. You can further filter the graph by specific users or workspaces. Click the text box above to the graphs to see the options. Click an option to select it. Once selected, click the x next to the option to deselect it. In the prior example three users were selected; below two workspaces were selected. " }, 
{ "title" : "Applications page", 
"url" : "102150-applications-databricks.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Unravel for Azure Databricks \/ Applications page", 
"snippet" : "(Azure Databricks) The Applications page for Databricks page has two tabs: Applications (Added in Unravel v4.5.2.0.) Jobs : a table showing the jobs run....", 
"body" : "(Azure Databricks) The Applications page for Databricks page has two tabs: Applications (Added in Unravel v4.5.2.0.) Jobs : a table showing the jobs run. " }, 
{ "title" : "Applications", 
"url" : "102150-applications-databricks.html#UUID-26e0744e-fcc3-ed4f-a783-cf1facfd27c6_UUID-92c63567-8733-50c5-3713-eeadb343cfe0", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Unravel for Azure Databricks \/ Applications page \/ Applications", 
"snippet" : "Note Click here for common features used throughout Unravel's UI. This tab lists all apps for the current day by default. The apps are sorted in descending order on Start Time . To order the results by another column, click the sort arrows ( ) next to the column header. Click the top arrow to sort t...", 
"body" : "Note Click here for common features used throughout Unravel's UI. This tab lists all apps for the current day by default. The apps are sorted in descending order on Start Time . To order the results by another column, click the sort arrows ( ) next to the column header. Click the top arrow to sort the column by ascending order and the bottom for descending order. The column the list is currently sorted on has the sort choice highlighted ( ). Because these apps are in the cloud there is no Type , Cluster Id , or Queue for the app. The columns are: Status: App status. User: Name of the user who submitted the app. App Name\/Id Notification Column ( ) : A fine-tuning ( ) indicates Unravel has tuning recommendation for the apps. Start Time: Start time, day and time. Duration: Total time taken by the app. Read: Total data written by the app. Write: Total data read by the app. Instance: Name of the Databrick Instance Id. Workspace: Workspace name. Job Id: Id that is unique across a workspace. Job Name: Job's run name. Run Id: The run number and is unique across a workspace. Goto: Links to the app's workflow. You can filter apps by: Selecting a date and time range of the apps to display using the date picker above the table. Using the pagination boxes, to the right above and below the table, to view the pages. The left sidebar lets you filter the apps by: App Name: Apps containing or matching the string are displayed, Status: Status of the app, success, failed, or killed. Tags: The following tags are always available. Click the check box next to the tag and then click in the text box for the valid values. You may select none to all the options available. ClusterId ClusterName Creator Databricks\/Environment Any user specific tags are also listed, for example, project. Workspace: Click in text box for a list of valid values. Click the value to select it. User: Click in text box for a list of valid values. Click the value to select it. Databricks Instance: Click in text box for a list of valid values. Click the value to select it. Duration: You can use the slider to set the range, or enter it directly in the from and to text boxes. Number of Events: The range of events is shown, for instance, 0-4. You can select a range by using the slider to specify the upper and lower bounds. Click anywhere within the app's row to bring it up in its APM. " }, 
{ "title" : "Jobs", 
"url" : "102150-applications-databricks.html#UUID-26e0744e-fcc3-ed4f-a783-cf1facfd27c6_UUID-f912e58b-e0a9-9908-cee1-ad69dbcf83a6", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Unravel for Azure Databricks \/ Applications page \/ Jobs", 
"snippet" : "This tab lists all the jobs, with a trend line showing its runs. The columns are: Status: App status. User: Name of the user who submitted the app. Job Name: Job's run name. Workflow Name: This is the app name\/Id Notification Column ( ) : A fine-tuning ( ) indicates Unravel has tuning recommendation...", 
"body" : "This tab lists all the jobs, with a trend line showing its runs. The columns are: Status: App status. User: Name of the user who submitted the app. Job Name: Job's run name. Workflow Name: This is the app name\/Id Notification Column ( ) : A fine-tuning ( ) indicates Unravel has tuning recommendations for the app. Start Time: Start time, day and time. Duration: Total time taken by the app, with the average duration across all the workflows. Read: Total data read by the app, with the average amount read across all the workflows. Write: Total data written by the app, with the average amount written across all the workflows. Workflow Trend: Workspace name. The trend across all the run of the workflow, the total number of runs is listed across the trend line. Instance: Name of the Databricks Instance Id. Workspace: Workspace name. Job Id: Id that is unique across a workspace. " }, 
{ "title" : "Reports page", 
"url" : "102151-reports-az-datatbricks.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Unravel for Azure Databricks \/ Reports page", 
"snippet" : "Azure Databricks Unravel provides a variety of reports helping you manage your clusters. The page has four tabs. As of 4.5.1 Cloud Reports has been replaced with Migration Planning. Operational insights - provides the ability to generate a variety of reports, including chargebacks, cluster summaries...", 
"body" : "Azure Databricks Unravel provides a variety of reports helping you manage your clusters. The page has four tabs. As of 4.5.1 Cloud Reports has been replaced with Migration Planning. Operational insights - provides the ability to generate a variety of reports, including chargebacks, cluster summaries, and cluster compares. Data insights - provides data level insights including a snapshot of tables and partitions over the last 24 hours and reports on disk capacity forecasting and small files. The Reports page opens displaying Operational insights . " }, 
{ "title" : "Operational Insights", 
"url" : "102152-reports-operational-databricks.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Unravel for Azure Databricks \/ Reports page \/ Operational Insights", 
"snippet" : "Azure Databricks Usage Trending - a trend report by user and workspace. Chargeback - chargeback report. Cluster Workload - shows the aggregated workload for all clusters. When you can specify a date range or instance, the pull down menu for it is on the right-hand side of the Operational Insights ti...", 
"body" : "Azure Databricks Usage Trending - a trend report by user and workspace. Chargeback - chargeback report. Cluster Workload - shows the aggregated workload for all clusters. When you can specify a date range or instance, the pull down menu for it is on the right-hand side of the Operational Insights title bar. By default, it opens showing Chargeback tab grouped by Application Type , for all clusters over the last 24 hours. Click here for common features used throughout Unravel's UI. " }, 
{ "title" : "Usage trending", 
"url" : "102152-reports-operational-databricks.html#UUID-a423229e-34bf-19fb-fb57-a87c66647f8f_section-5d485626a1fdd-idm44897685257376", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Unravel for Azure Databricks \/ Reports page \/ Operational Insights \/ Usage trending", 
"snippet" : "This shows the usage trending By Workspace and By User . By default, the window opens showing all instances, workspaces, and users for the current day in 30 minute increments. You can select the date range, time period and instance in the Operational Insights title bar. You can select a subset of us...", 
"body" : "This shows the usage trending By Workspace and By User . By default, the window opens showing all instances, workspaces, and users for the current day in 30 minute increments. You can select the date range, time period and instance in the Operational Insights title bar. You can select a subset of user or workspaces using the pull down on the right above the graphs. " }, 
{ "title" : "Chargeback", 
"url" : "102152-reports-operational-databricks.html#UUID-a423229e-34bf-19fb-fb57-a87c66647f8f_N1555063264507", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Unravel for Azure Databricks \/ Reports page \/ Operational Insights \/ Chargeback", 
"snippet" : "The tab is divided into three sections filtered by the Group By selection. Donut graphs showing the top results. Chargeback report showing usage. List of all apps. Generate chargeback report You can set the date range and the instance to use for the report in the Operational Insights title bar. Use ...", 
"body" : "The tab is divided into three sections filtered by the Group By selection. Donut graphs showing the top results. Chargeback report showing usage. List of all apps. Generate chargeback report You can set the date range and the instance to use for the report in the Operational Insights title bar. Use the Group By to filter the information. By default, User is selected. Each time you select an Other option it is added to the Group By list. If two Group By options are selected, the sort priority is noted. Click an option to deselect it. In the example below the report is filtered on User . Note, that while you can Group By tags, you can not by tag values. For instance, given <project, projname> you can Group By on <project> but not <projname>. Clicking a Group By selection toggles it and changes the sort priority. If you only have one group value selected you can not deselect it until you select another one, i.e., there must always be one Group By choice selected. Using the below example, if you would have select Workspace or an Other tag to be able to deselect User . A new chargeback report is generated each time you change the Group By filters. All the reports and donut graph are filtered by your Group By choices. You can download the Chargeback report and the apps list by selecting Download CSV on the right-hand side above the report. " }, 
{ "title" : "Cluster workload", 
"url" : "102152-reports-operational-databricks.html#UUID-a423229e-34bf-19fb-fb57-a87c66647f8f_section-5d487aa9889bf-idm44897685276112", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Unravel for Azure Databricks \/ Reports page \/ Operational Insights \/ Cluster workload", 
"snippet" : "This displays your instance workload, so you can see when your workload is heaviest, etc. It displays the workload by: Month: By date, for example, July 30. Hour: By hour regardless of date, for example, 10.00 - 11.00. Day: By weekday regardless of date, e.g., Tuesday. Hour\/Day: By hour for a given ...", 
"body" : "This displays your instance workload, so you can see when your workload is heaviest, etc. It displays the workload by: Month: By date, for example, July 30. Hour: By hour regardless of date, for example, 10.00 - 11.00. Day: By weekday regardless of date, e.g., Tuesday. Hour\/Day: By hour for a given weekday, e.g.,10.00 -11.00 on Tuesday (a heatmap). The app count is not a count of unique app instances because apps can span boundaries, i.e., begin and end in different hours\/days. The app count reflects the apps that were running within that interval up to and including the boundary, i.e., date, hour, day. Therefore, an app can be counted multiple times in any given time period. On multiple dates, or example, July 30 and 31. In multiple hours, for example, 2 AM and 3 AM. On multiple days, Tuesday and Wednesday. In multiple hour\/day slots. This results in anomalies where the Sum(24 hours in Hour\/Day app count) > Sum(app counts for dates representing the day) . We point this out not because it necessarily has a significant impact in how you can use the data, but to inform you such variations exist. When you can specify a date range or instance, the pull down menus are on the right-hand side of the Operational Insights title bar. We recommend using a short date range, as bigger ranges take up more processing time. By default, it opens showing the Month tab for the past week across all instances. Click the View By buttons to change between views. For the Hour , Day and Hour\/Day you can view the data as an Average or Sum . See Drilling Down below for information on how to retrieve the detailed information within each view. Month view Displays the jobs run by date. The color indicates how the day's load compares with the other days within the date range. The day with the least jobs\/hours is , while the days with the highest load are . Therefore, the color of any particular day varies in context to the other days being displayed. For example, when only one day is displayed it is colored . Use Previous and Next in the month's title bar to navigate between months. Hour, day and hour\/day view These graphs do not link jobs to any specific date. For instance, the Hour graph shows that 11 jobs ran at 2 AM (between 2 AM and 3 AM); the Day graph that 78 jobs ran on Wednesday, and the Hour\/Day that five jobs ran at 2 am on a Wednesday. But none of these graphs indicate the date these jobs ran on. Only the Month view links the job counts to a specific date; above we see 22 July had an app count of 77. You have the choice to display the data as either the: Sum: Aggregated sum of job count during the time range (default view). Average: Aggregated sum of job count average across the time slice, for instance (Sum of all jobs which ran on Wednesday)\/(Number of Wednesday's in the Sample). Day view Displays the jobs run on a specific weekday. Hover over an interval for its details. Click the interval to drill down into it. Hour\/Day view This view shows the intersection of Hour and Day graphs, it's basically a heatmap. The Hour graph showed 11 jobs ran between at 2-3 AM while the Day graph (immediately above) shows that 78 jobs ran on Wednesday. Below we see that five of Wednesday's jobs ran between 2-3 AM. " }, 
{ "title" : "Hour view", 
"url" : "102152-reports-operational-databricks.html#UUID-a423229e-34bf-19fb-fb57-a87c66647f8f_N1565047358876", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Unravel for Azure Databricks \/ Reports page \/ Operational Insights \/ Cluster workload \/ Hour view", 
"snippet" : "Breaks out information by hour. The interval label indicates the start, i.e., 2 AM is 2 AM -3 AM. Hover over an interval for its details. Click the interval to drill down into it....", 
"body" : "Breaks out information by hour. The interval label indicates the start, i.e., 2 AM is 2 AM -3 AM. Hover over an interval for its details. Click the interval to drill down into it. " }, 
{ "title" : "Drilling down in a workload view", 
"url" : "102152-reports-operational-databricks.html#UUID-a423229e-34bf-19fb-fb57-a87c66647f8f_N1565046707537", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Unravel for Azure Databricks \/ Reports page \/ Operational Insights \/ Cluster workload \/ Drilling down in a workload view", 
"snippet" : "Click an interval to bring up its information. In this example we clicked on the Hour view for 2 AM. There were two users who were running apps at that hour. Immediately above the table on the left-hand side it notes what is being displayed, in this case Apps at 09 AM . Click to display all the apps...", 
"body" : "Click an interval to bring up its information. In this example we clicked on the Hour view for 2 AM. There were two users who were running apps at that hour. Immediately above the table on the left-hand side it notes what is being displayed, in this case Apps at 09 AM . Click to display all the apps running during the hour. Click to display User or WORKSPACE details. By default, User is displayed, click Workspace to see the apps distributed across the relevant workspaces Click to see the apps for a specific user or workspace. When there are multiple row, an expanded row has a green block displayed. In the example below the first user information is displayed. Click an app to bring it up in its APM. " }, 
{ "title" : "Data insights", 
"url" : "102153-reports-data-insights-databricks.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Unravel for Azure Databricks \/ Reports page \/ Data insights", 
"snippet" : "(Azure Databricks) Data Insights provide data level insights including a snapshot of tables and partitions over the last 24 hours within a historical context. Overview: Provides a quick view into the tables' and partitions' sizes. Details: Drills down into the tables. Click here for common features ...", 
"body" : "(Azure Databricks) Data Insights provide data level insights including a snapshot of tables and partitions over the last 24 hours within a historical context. Overview: Provides a quick view into the tables' and partitions' sizes. Details: Drills down into the tables. Click here for common features used throughout Unravel's UI. Overview The Overview Dashboard gives a quick view into the tables' and partitions' sizes, usage, and KPIs. It has two sections. Table KPI Partition The time period used to populate the page is noted in the upper right-hand corner and the tool tips. Tables & partitions tiles Both Table and Partition KPIs sections contain: # Accessed: Number of tables\/partitions accessed. # Created: Number of tables\/partitions created. Size Created: Size of tables\/partition created. # Total: Total number of tables\/partition currently in the system. The Table KPIs also contains: Accessed Queries: Total number of queries accessing the tables. Total Size: Total size of all the tables. # of Users: Number of users in the last 24 hours. Donut charts These display the Current Label Distribution for the tables\/partitions. See Configuration for the operating definition of the labels. The graph shows the relationship between the labels; hover over a label to see the total of tables\/partitions with that label. Below we see that three tables are warm. Details The details tab has two sections, a graph and a table list. By default, the graph uses the Total Users metric and displays the first table in the list. The list is sorted on Total Users in descending order. You can also use the metrics Total Users, Total Apps, or Total Size to display the graph. The Total Apps metric (corresponding Apps column in the table) is the total number of Hive and Impala queries on the table. Graph Use the Metric pull down menu to select Total Users , Total Apps , or Total Size as the metric to graph. Click Reset Graph to revert to displaying first table using the Total Users metric. Select the menu bars to print or download the graph as a XLS or CSV file. Select the checkbox next to the table's name to dispaly the table information. You can select tables over multiple pages. In the example below we have selected the first three tables. Between the graph and the table list you can choose how to filter graph and tables based upon their table activity, All , Warm , Cold , or Hot . Click Download CSV to download the table. Click Configure Policy to set the conditions for determining whether a table is Warm , Cold , or Hot . You can Search by string; any table matching or containing the name\/string is displayed. All is selected below, so every table is shown. You can sort the list by the various metrics in ascending or descending order. By default, the list is sorted on Table Name in ascending order. If you have selected a table, the More Info glyph is available. Click it to display the Table Detail view. Click Configure Policy to edit the label rules or Download CSV to download the table list. Table List Table detail This view: Summarizes table usage and access metric. Lets you to browse trends (KPIs). Drill down into apps that used the table. Lists both Hive and Impala queries. The first table in the list above is used for the examples below. The panel's top row lists the table name, start date\/time and the name\/path. Hover over the name\/path to display the complete path. Three KPIs are displayed: Users , # Apps, and Size . There are three tabs, Table Detail , Partition Detail , and Retention Detail (1); the default view is the Table Detail . Use the Metric pull down menu to select Total Users , Total Apps, or Total Size as the metric to graph. The Application Detail lists the apps that accessed the table in the given time range. The Partition Detail and the Retention Detail give an analysis of each. " }, 
{ "title" : "Graphs", 
"url" : "102153-reports-data-insights-databricks.html#UUID-e387ed57-a8bd-b58e-0aed-5bc9f046d233_section-5d4893ae5b2a9-idm45764245826176", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Unravel for Azure Databricks \/ Reports page \/ Data insights \/ Graphs", 
"snippet" : "They display the Table Trends and Partition Trends over the time range....", 
"body" : "They display the Table Trends and Partition Trends over the time range. " }, 
{ "title" : "Configure Policy", 
"url" : "102153-reports-data-insights-databricks.html#UUID-e387ed57-a8bd-b58e-0aed-5bc9f046d233_N1553871184841", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Unravel for Azure Databricks \/ Reports page \/ Data insights \/ Configure Policy", 
"snippet" : "This pane lets you define the rules for labeling a Table\/Partition either Hot , Warm , or Cold . These rules are used for the Donut chart and in the Details tab. While the labels are immediately associated with the Tables\/Partitions, the Overview Dashboard donut charts typically populate within 24 h...", 
"body" : "This pane lets you define the rules for labeling a Table\/Partition either Hot , Warm , or Cold . These rules are used for the Donut chart and in the Details tab. While the labels are immediately associated with the Tables\/Partitions, the Overview Dashboard donut charts typically populate within 24 hours. You access this modal pane from the Data > Details tab. The rules are defined per label and you can define up to two rules per label. To define a rule: From the pull down menus: chose Age (days) or Last Access (days) , and chose the comparison operator: <= or >=. Enter the number of days. To add a second rule: Click the Plus glyph, Select the AND or OR operator from the pull down menu, and Repeat steps 1 and 2. To delete a second rule, click the Minus glyph. Click Save . " }, 
{ "title" : "Application Performance Management (APM)", 
"url" : "102154-apms.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Application Performance Management (APM)", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Typical APM layout", 
"url" : "102155-apms-layout.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Application Performance Management (APM) \/ Typical APM layout", 
"snippet" : "See Common UI Features for general information about Unravel's UI. See Resource Metrics for the list of metrics that Unravel collects. When layout varies if the app is on-premise or the cloud and the differences are noted. A black title bar notes the app type, i.e., Spark, Impala, MapReduce, Fragmen...", 
"body" : "See Common UI Features for general information about Unravel's UI. See Resource Metrics for the list of metrics that Unravel collects. When layout varies if the app is on-premise or the cloud and the differences are noted. A black title bar notes the app type, i.e., Spark, Impala, MapReduce, Fragment, etc. and the job ID. On the right side of the title bar are glyphs for adding a comment, and to minimize or close the tile if possible. If the jobs has a parent, i.e., Hive, Pig, there is an arrow with the parent's type. Clicking on it brings up its APM. If it is a running yarn job (MR, Tez, or Spark) there is an action box ( ) Unravel's Intelligence Engine can provide insights into an app and may provide recommendations, suggestions and insights on how to improve the app's run. When there are insights a bar appears immediately beneath the title bar. If Unravel has recommendations the insight bar is orange, otherwise it's blue. For more information about events, see the Event Panel Example The next section contains general job information and Key Performance Indicators (KPIs) (as applicable) Event icon : The number of events the job had. No Events , instead of the box, is noted if there were none. This job has two (2) events, clicking on the icon brings up a panel which contains one (1) or more Tabs, as relevant, i.e., Recommendations, Efficiencies, Application Failure. See Event Panel & Insights . Job icon : The job type and status. The box is color coded to indicate the app's status. Job name : Next to the job name will be an AutoActions glyph ( ) if the job has violated any actions. Hover over it to see a list of the violated actions. A fine-tuning glyph ( ) appears when Unravel has tuning suggestions for the job. Job information : This varies by job type and whether the app is on-premise or in the cloud. KPIs : These vary by job type. The last section, typically divided into two (2), has specific information related to job. Each Application-Specific Manager Section goes into detail about this section. If the job is composed of tasks\/jobs\/stages they appear on the left under Navigation . Clicking a row brings up itdetailed information it in a tile\/panel. The AutoAction\/Event ( ) column notes the number of events associated with the job\/stage. " }, 
{ "title" : "Spark APM", 
"url" : "102156-apms-spark.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Application Performance Management (APM) \/ Spark APM", 
"snippet" : "Overview A Spark app consists of one or more Jobs, which in turn has one or more Stages. Job : corresponds to a Spark action, for example, count, take, foreach. The Spark APM lets you: Quickly see which jobs and stages consumed the most resources. View your app as a RDD execution graph . Drill into ...", 
"body" : "Overview A Spark app consists of one or more Jobs, which in turn has one or more Stages. Job : corresponds to a Spark action, for example, count, take, foreach. The Spark APM lets you: Quickly see which jobs and stages consumed the most resources. View your app as a RDD execution graph . Drill into the source code from the stage tile, Spark stream batch tile, or the execution graph to locate the problems. You can use the APM to analyze an app's behavior to: Resolve inefficiencies, bottlenecks, and reasons for failure within apps. Optimize resource allocation for Spark driver and executors. Detect and fix poor partitioning. Detect and fix inefficient and failed Spark apps. Tune JVM settings for driver and executors. Unravel provides insights into Spark apps and potentially tuning recommendations. There are multiple Spark app types and the Spark APM's information can vary by the app type. Currently, Unravel distinguishes between: Scala, Java, and PySpark SQL-Query Streaming Regardless of the app type and how they are submitted (for example, from Notebooks, Spark shells, or spark-submit), the Spark APMs are similar and there are common tabs\/information across all types. The Spark APM's basic layout A black title bar notes the type of tile (Spark, Job, Stage, etc). On the right side there are actions ( ), comment ( ), and minimize ( ) glyph. For jobs and stages instead of actions there is a , click it to see the job's or stage's parent. A running Spark streaming app lets you toggle the auto refresh so you can examine the app without the APM constantly refreshing. Unravel's Intelligence Engine provides insights into an app and may provide recommendations, suggestions, or insights into how to improve the app's run. When there are insights, a bar appears immediately beneath the title bar. If Unravel has recommendations, the insight bar is orange, otherwise it's blue. For more information about events, see Events and Insights . The next section contains the Key performance indicators (KPIs) and general app information. Event icon: The number of events the Spark app had. If there were none, No Events is noted instead of the box. This Spark app has two events, clicking on the icon brings up a panel which contains one or more sub-tabs, as relevant, e.g., Recommendations, Efficiencies, Application Failure. See the Event Panel examples for more information. Application icon: The app's status and the window type (S-Spark, SJ-Spark Application and so on). The box is color coded based upon its status. Job icon: The job type and status. The box is colored coded to indicate as the app's status. Job Name: Next to the job name is an AutoActions glyph ( ) if the job has violated any actions. Hover over it to see a list of the violated actions. A fine-tuning glyph ( ) appears when Unravel has tuning suggestions for the job. Job Information: Job number, owner, queue, cluster and start\/stop time. In Unravel for Azure Databricks the Job information is more detailed. First line: Run creator, Instance name. Second line: Cluster Id, Cluster Name, Start and End time of the run. Third line: Workspace Id, Workspace Name, Job Id, Run Name, Run Id, Number in job. KPIs: these vary by job type. The last section, divided in half, has specific information related to the app. The sections for a specific Spark Application (for example, Streaming) go into more detail. If the app is composed of tasks\/jobs\/stages they appear on the left side under Navigation or Stream . Clicking a row brings up detailed information about it in a separate tile\/panel. When there is an Auto Action column ( ), the number of events associated with the stage is listed. Click to toggle the display of the job tile. As of Unravel 4.5.2.0, a left-hand tab isn't displayed if there is no data available, e.g., the Conf tab isn't shown if there is no configuration information. The Graphs tab isn't shown if there is no data available. When a graph has less than four data points, the graph is converted to a bar graph. Common tabs Except for the Spark streaming app, the Application Manager view is split into two sections The sections contain the following tabs. A tab is only displayed if there is data. Left tabs Errors: Lists all errors associated with the app. The errors are color coded (fatal , errors , warnings ) and the number for each type noted. The total number of errors is listed at the top. They are grouped by executors with the number and types of errors noted. Errors are categorized by severity, and include keywords and details associated with each. Keywords help to extract important details from the error messages\/log data to help quickly root out the cause of a problem. If there are no errors, then \"No errors found\" is listed. Logs: The critical logs that were collected for this Spark app. No Logs Found is displayed when Unravel is unable to load any log information. Click the log name to see it, this example is an excerpt of the executor-20 log. Conf: The configuration parameters and their values. The tab opens listing all the properties, according to app\/task\/job. The number of properties displayed is noted above the list. You can narrow the list by choosing the configuration type to display; to see the Spark version select metadata. Metadata and driver are selected in this example and the list narrowed from 1042 to two properties. Some properties appear in multiple categories, e.g., spark.executor.extraJavaOptions is listed under Memory, Driver and Executor. You can search by name; searching on YARN displays every property containing the word yarn. Click Reset to display all properties. Right tabs Program: Displays the program's source code if available. Both the Spark Stage and Execution graph link to this tab and display code related to the tab. See Uploading Spark Programs . Task Attempts: Graphically and textually notes the number of tasks and their status. The donut graphs show the percentage of successful (green), failed (orange), and killed (red) tasks. A legend on the right lists the number of each. The following graph on the left shows a job in which all tasks succeeded, while the on the right has all failed tasks. Frequently, the result is a combination. Hovering over the chart tells you the percentage of each. Graphs: There are three types of graphs. Graphs use \"wall clock time\" as opposed to computer usage time. This tab isn't shown when there is no data available. Running Containers: The number of running containers. vCores: Allocated vCores. Memory: Allocated memory. Resource: Graphs the resources the app\/job\/stage consumed. When no resource data is available the tab displays No metrics found. Metrics are displayed after Unravel has processed the app completely and Unravel's application sensors are enabled. By default, the graph displays all executors using the metric availableMemory . Beneath the list notes the page, if more than one, that is being displayed. Click the arrow to page down or up to display a page. Choose the metric for the graph using the pull-down menu. The metrics fall into one of the following categories: OS Memory, CPU, and Java Memory. Click Get Data to see the resource information displayed in JSON. You can toggle whether a resource is displayed by clicking on it; it is greyed out when it's not displayed. You choose one or more series to display using the Select box. Clicking within the box brings up a list of all resources available. Click the resource you want to display. You can choose to display from one to all resources. Only the chosen resources are listed at the bottom of the box. Click the X next to the resource to remove it from the list. This example of the JSON shown when yoyu click Get Data . Common tiles Scala, Java, PySpark, and SQL-Query navigation tab The Navigation tab isn't shown if there is no data available. Navigation: The app's jobs with their relevant KPIs: Status , Start Time , Duration , Tasks , DFS Read , DFS Write , Stages . The jobs are sorted by Start Time in descending order. While the app is running, the Status column displays the job's completion percentage. The Tasks and Stages columns show the (current number completed)\/(total). In the following image, Job ID 1 is 41% complete. This tab updates every 60 seconds. However, the live data that Unravel receives is only updated upon: The start of a Spark Job. The completion of a Spark task, job or stage. This tab isn't shown if there is no information\/data available. Once completed the total number of the Tasks and Stages are shown along with the number skipped. In this example, 39,600 Tasks and 99 Stages were skipped. Click a job row to bring up its details in a Job Block. You can open the job even if it's running. The job block lists the KPIs Duration and # of Stages . It has three tabs, Stages , Gannt , and Metadata . The default view is the Stages tab. The stage table lists ID , Status , Start Time , Duration , Tasks (Complete\/Total), Shuffle Read , Shuffle Write , Input (total data read by the app), and Output (total data written by the app). By default, the stages are sorted by Start Time in descending order. Click a stage to bring up its details in a tile. See Spark Stage . This example is for Job 1 above, which is only 41% complete. Here the # of Stages lists 199, but only 82 completed jobs are listed in the table. As jobs are completed they are added to the table. " }, 
{ "title" : "Actions", 
"url" : "102156-apms-spark.html#UUID-13ac45bc-ab94-831f-8fcc-9797c6b0cf26_N1553170011068", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Application Performance Management (APM) \/ Spark APM \/ Actions", 
"snippet" : "Click in the title bar to bring up the available actions. When a YARN app is running you can kill or move it. After the app has stopped, you can load logs and diagnostics. You can also create or auto tune a Session . Clicking Load Diagnostics brings up a popup containing diagnostic information for t...", 
"body" : "Click in the title bar to bring up the available actions. When a YARN app is running you can kill or move it. After the app has stopped, you can load logs and diagnostics. You can also create or auto tune a Session . Clicking Load Diagnostics brings up a popup containing diagnostic information for the app. Load Logs refreshes the APM and populates\/updates the Logs tab. " }, 
{ "title" : "Spark job", 
"url" : "102156-apms-spark.html#UUID-13ac45bc-ab94-831f-8fcc-9797c6b0cf26_N1553168728356", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Application Performance Management (APM) \/ Spark APM \/ Spark job", 
"snippet" : "A job is created for every Spark action, for example, foreach. A job consists one or more stages. The following job has three stages, two keyby RDD's and one count. You can see the source code associated with the job in the stage tile. The Stages table lists each stage's KPIs so you can quickly see ...", 
"body" : "A job is created for every Spark action, for example, foreach. A job consists one or more stages. The following job has three stages, two keyby RDD's and one count. You can see the source code associated with the job in the stage tile. The Stages table lists each stage's KPIs so you can quickly see which stage consumed the most time. Key performance indicators Duration: The wall-clock time it took to complete the job. # of Stages: Number of stages to the job. It has three tabs: Stages: This tab is the default view. It lists the stages with their KPIs and is initially sorted on Start Time in ascending order. Gannt Chart: This tab again shows all the stages, graphically displaying the time spent in each. The stages are initially displayed in order of execution, i.e., first, second, ..., nth. Metadata: Lists all the attributes and values for the jobs. " }, 
{ "title" : "Spark Stage", 
"url" : "102156-apms-spark.html#UUID-13ac45bc-ab94-831f-8fcc-9797c6b0cf26_N1552611722669", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Application Performance Management (APM) \/ Spark APM \/ Spark Stage", 
"snippet" : "The Stage tile can help you pinpoint problems with the stage, for example, skewing, and see the source code associated with the stage. Key performance indicators Duration: The computer time it took to complete the job. Date IO: Total input and output associated with the stage. It has two tabs, by de...", 
"body" : "The Stage tile can help you pinpoint problems with the stage, for example, skewing, and see the source code associated with the stage. Key performance indicators Duration: The computer time it took to complete the job. Date IO: Total input and output associated with the stage. It has two tabs, by default it opens in the graph tab displaying the task attempts. Graphs Task Attempt: Displays the number of total tasks for the stage and number of total attempts made to run these tasks. The number of tasks is noted on the left side of the bar with the number of attempts on the right. The donut chart graphically displays the successful ( ) and failed ( ) attempts which are also listed in the legend to the right. Hover over a section of the chart to see the percentage of successful and failed task attempts. In the this example, all tasks attempts have succeeded. This tab isn't shown if not information is available. Program Details: Displays the stage's program details. Unravel extracts Source file name and call line. Click the source file or line number to see program code. The Description shows the stage's Stack trace. The first line is always a call to the Spark library. The source file name and line number are extracted from the second line. Time line tab The Time Line tab has two sections: Distribution Charts . Three tabs below the chart: Time Line . Timeline Breakdown . Selected Tasks . Time Line . This tab has two sections: Distribution Charts . Three tabs below the chart: Time Line , Timeline Breakdown , and Selected Tasks . By default, the Time Line tab opens with the Distribution Charts displaying the Map Tasks over ShuffleMap information. You can toggle the chart using the radio buttons (1) to show Disk Bytes Spilled , Memory Bytes Spilled , and Records Read . The lower section opens displaying the Time Line tab, which initially lists all the stages regardless of status. You can choose to filter the stages (2) by Tasks or Killed\/Failed. Hovering over a task brings up a text-box with that stage's details. Timeline Breakdown This is useful to identify bottlenecks. For each executor used in the current stage, multiple metrics are graphed: Scheduler Delay , Executor Deserialization Time , Fetch Wait Time , Executor Computing Time , JVM GC time , Result Serialization Time and Getting Result Time . Using the ratio of Executor Computing Time to the total execution time you can determine where the executor spent most of its time: Performing actual work. Thrashing. Waiting for scheduling. Selected Tasks: A list of tasks, if any, for the stage. " }, 
{ "title" : "Spark Scala, Java, and PySpark", 
"url" : "102156-apms-spark.html#UUID-13ac45bc-ab94-831f-8fcc-9797c6b0cf26_N1553168833366", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Application Performance Management (APM) \/ Spark APM \/ Spark Scala, Java, and PySpark", 
"snippet" : "Key performance indicators Events: is the number, if any, of Unravel insights for the query. See the here for more information. Duration: The \"wall clock\" time it took for the app to complete. Data I\/O: The memory used. Number of Stages: The number of stages in the query. Left tabs Navigation: The a...", 
"body" : "Key performance indicators Events: is the number, if any, of Unravel insights for the query. See the here for more information. Duration: The \"wall clock\" time it took for the app to complete. Data I\/O: The memory used. Number of Stages: The number of stages in the query. Left tabs Navigation: The app's jobs with their relevant KPIs: Status , Start Time , Duration , Partitions\/Tasks ( ), DFS Read , DFS Write , # Stages . See above for more details. Execution: The execution graph shows a RDD DAG of the app. The app's RDDs are represented by the vertices while the edges are the Operations to be applied. Any particular RDD can involve multiple stages so there isn't a one-to-one mapping between a RDD and stages. The graph is \"linked\" to the Program tab. If the source code is available, clicking on a vertex brings up the source line associated with the RDD. If the program tab displays the code it is linked with the DAG. Display the execution and program tab simultaneously, click the vertex to highlight the relevant code. Here we see the corresponding code for vertex 18. We expanded the above area, and vertices 15-19 are shown (the vertex number is noted in the circle). The vertex lists the type of RDD, partitions used, Spark call, and the number of stages which were involved. The RDD represented by vertices 17-16 involved two stages, while 15-16 had five. Hover over the vertex to bring up an information box, containing the RDD description and CallSite (source line) which called the RDD transformation. The execution graph isn't displayed when there are more than 1000 RDDs in the generated DAG. Gantt Chart: Displays the stages using a Gantt Chart. The table is sorted on Start Time in ascending order. Errors , Log and Conf Tabs: For an explanation of these tabs see Errors , Logs , and Conf . Four Tabs Program: When available it displays the program associated with the app. See above for an example of this tab and its relation to the Execution graph. Task Attempts , Graphs , and Resources: For an explanation of these tabs see Task Attempts , Graphs , and Resource . " }, 
{ "title" : "Spark SQL-Query", 
"url" : "102156-apms-spark.html#UUID-13ac45bc-ab94-831f-8fcc-9797c6b0cf26_N1553168860873", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Application Performance Management (APM) \/ Spark APM \/ Spark SQL-Query", 
"snippet" : "Key performance indicators Events: is the number, if any, of Unravel insights for the query. See here for more information. Duration: The \"wall clock\" time it took for the app to complete. Data I\/O: The memory used. # of Stages: The number of stages in the query. Left tabs As of Unravel 4.5.2.0, a l...", 
"body" : "Key performance indicators Events: is the number, if any, of Unravel insights for the query. See here for more information. Duration: The \"wall clock\" time it took for the app to complete. Data I\/O: The memory used. # of Stages: The number of stages in the query. Left tabs As of Unravel 4.5.2.0, a left-hand tab isn't displayed if there is no data available, e.g., the Conf tab isn't shown if there is no configuration information. Navigation: The app's jobs with their relevant KPIs: Status , Start Time , Duration , Partitions\/Tasks ( ), DFS Read , DFS Write , # Stages . See above for more details. Execution: A execution graph of the query. There are times when the DAG is too large to display and it will be noted. See above for more details. Gantt Chart: Displays the stages using a Gantt Chart. For more details see above . Errors , Log , and Conf Tabs: For an explanation of these tabs see Errors , Logs , and Conf . Right tabs Program: This tab connects all the pieces of the SQL query. The table lists all queries with significant KPIs and the top five stages, i.e., the stages with the longest duration. Beneath the list are two tabs, SQL and Program The SQL tab displays the SQL query in either query or plan mode. The Program tab lists the query's corresponding Program. For a particular query, the SQL query text, Program, and stages are linked. By default: The Query table is sorted on the query's duration in descending order. Similarly, the stages are sorted on duration in descending order left to right. The SQL tab is displayed using the query view. The query with the longest duration (first row) is shown. Unravel loads whatever information is available, even if the information is incomplete. Click the Query ID to display its SQL and program. Click the stage to display its Spark Stage Detail. See Spark Stage Details for further information on the stage view. To switch between SQL Query and Plan view, click the plan\/query button in the upper right-hand corner of the SQL window. To copy the SQL query to your clipboard click the Copy button. You don't have to have the query view showing in order to copy it. This screenshot shows the default window, the SQL query for Query ID 4. Scroll down to see the entire SQL Plan. Click Query to return to the program. Task Attempts , Graphs , and Resources : For an explanation of these tabs see Task Attempts , Graphs , and Resource . " }, 
{ "title" : "Spark Streaming", 
"url" : "102156-apms-spark.html#UUID-13ac45bc-ab94-831f-8fcc-9797c6b0cf26_N1552611588233", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Application Performance Management (APM) \/ Spark APM \/ Spark Streaming", 
"snippet" : "You can toggle the UI's refreshing a running app. This lets you examine and drill down in the app without the refreshing interfering. Click to toggle the refresh state. Key performance indicators The KPIs refer to the entire Spark job composed of jobs which in turn have stages. Events: Is the number...", 
"body" : "You can toggle the UI's refreshing a running app. This lets you examine and drill down in the app without the refreshing interfering. Click to toggle the refresh state. Key performance indicators The KPIs refer to the entire Spark job composed of jobs which in turn have stages. Events: Is the number, if any, of Unravel insights for the query. See the here for more information. Duration: The total time to process all stages. (The wall clock time can be calculated from the start and end times are on the left-hand side of the job bar.) Data I\/O: Total data read and written by the query. Number of Jobs: The number of jobs that make up the streaming. Number of Stages: The number of stages that make up the streaming. Unlike other Spark Application Managers this has a Stream tab on the right. The manager opens displaying the Stream tab on the right and the Program on the right. Left tabs As of Unravel 4.5.2.0, a left-hand tab isn't displayed if there is no data available, e.g., the Conf tab isn't shown if there is no configuration information. Stream: Displays the core of a Streaming Application. From here you drill down into the batches, the main processing unit for Spark streaming. The graph displays the number of events\/second (the bars) with a superimposed line graph of the chosen metric. By default, Scheduling Delay is used. You can change the date range to display or the metric used via the Metric pull-down menu (1). You can choose Scheduling Delay , Processing Time or Total Delay . In the following example, the graph is displaying Scheduling Delay Time. It has two sections; by default, they display the entire run over the last 7 days. You can zoom in on a section of graph by pulling the tabs left or right (2). The table lists the Completed Batches relevant to the time period selected. Each batch has its KPI's listed. In the view above, the entire stream time is displayed, therefore all Completed Batches are displayed and in this case there are seven pages. In the example, we have zoomed in on the last two minutes, the table now lists the batches completed in that time period. The tables now contains only one page, versus the seven above. The table lists only the first three batches, but you can page through the table (3). By default, the streams are sorted on start time in ascending order. When you sort the batches, they are sorted across all tables, i.e., if Start Time is switched to descending list, nth batch becomes the first, the n-1 the second, etc. Click a batch to bring up the Spark Stream Batch tile. You can only open one batch job at a time. The batch window lists all the jobs associated with the batch and the batch's metadata. Its title bar notes it's a Spark Stream Batch view and that it's part of a Spark Streaming app. The KPI's, Duration , Processing Delay , Scheduling Delay , and Total Delay refer to this Batch job. It has two tabs, Output Operation and Input . This example is of the batch in the first line of the table above. The Stream Batch has two calls and the first call has two jobs. Since these jobs are run in parallel, the job with the longest time determines the duration of the batch. The description notes the RDD and the call line; clicking on the description displays the associated code in the program window. Click the Job ID to see the Spark Job information. The Input Tab lists all the Topics consumed. Errors , Log , and Conf: For an explanation of these tabs see Errors , Logs , and Conf . Right tabs Program: The program (if uploaded by the user) is shown in this tab. Task Attempts , Graphs , and Resources: For an explanation of these tabs see Task Attempts , Graphs , and Resource . " }, 
{ "title" : "Hive APM", 
"url" : "102157-apms-hive.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Application Performance Management (APM) \/ Hive APM", 
"snippet" : "The Hive APM provides a detailed view into the behavior of Hive queries. Typical users are Hadoop DBAs or app owners (engineers, BI team, analysts). You can use this view to: Resolve inefficiencies, bottlenecks and reasons for failure within apps. By default, the Hive APM opens displaying the Naviga...", 
"body" : "The Hive APM provides a detailed view into the behavior of Hive queries. Typical users are Hadoop DBAs or app owners (engineers, BI team, analysts). You can use this view to: Resolve inefficiencies, bottlenecks and reasons for failure within apps. By default, the Hive APM opens displaying the Navigation and Query tabs. For Hive queries that don't run using a Tez, LLAP, MapReduce, or Spark app, the duration shown by Unravel may be inaccurate because Hive doesn't call the Hive pre and post hooks correctly for these queries. Key performance indicators Events: The number, if any, of Unravel insights for this query. See Event Panel & Insights for more information. Duration: Total time taken by the app to complete execution. Data I\/O: Total data read and written by the app. Number of YARN apps: The number of YARN apps making up the query. Left tabs Navigation: List all the MapReduce jobs associate with the query. Click the job name to bring up job in the MapReduce Application Manager tile. Execution Graph: Shows detailed information about the MapReduce jobs and their relationship to one another. This view helps identify bottlenecks and inefficiencies. The graph provides a quick and intuitive way to understand the MapReduce jobs. Upon opening the tab you immediately see the MR jobs (1) in relation to each other and limited job info: tables used, the job length in absolute and relative value to the whole. Clicking on the job brings up a box with more Table KPIs, a forward path for the Map and Reduce operations, and input paths (should you want to show them). Click a table name to bring up its details. See Reports > Data Insights > Details for description of the tables. Click Close (2) or scroll within the tab to close the box. Click a path point (3) drill deeper. The resulting text box notes the operation type (MapJoin, ReduceSink, etc.), and various key information about the operation. The information displayed is specific to that operation at that time. Gantt Chart: Shows job sequencing using a Gantt chart. Errors: Lists all the job's errors. Like job status, the errors are color coded and the total number of each type (fatal, errors, warnings) is noted. The top line lists the number of all jobs and tasks. The errors are grouped by tasks\/jobs and then by severity. For each job\/task the total and type of errors are noted. Time, keywords (if any) and a brief message is displayed for the error. Keywords extract important details from the errors messages and log data that can help developers and operators quickly \"root cause\" issues. \"No errors found\" is displayed when there are none. Tags: The defined tag keys and values for the app. This example has two tag keys, project and dept with each having one value, group11 and hr respectively. Right tabs Query: Shows the Hive query. See the Hive APM window above for an example Query tab. Click the Copy Query to copy it to the clipboard. Tables: A list of tables accessed by the app. Task Attempts: Displays MapReduce task attempts by success, failed, and killed status. The data displayed is for the entire Hive job. To see the details for a specific MapReduce task click on the job in the Navigation tab. Attempts: Graphs the map and reduce task slot usage over the duration of the job. The wall clock time the job started is listed in the upper left-hand corner. The total map and reduce slot duration times are listed beneath the graph. " }, 
{ "title" : "", 
"url" : "102157-apms-hive.html#UUID-0848892e-58aa-b0e6-9585-7082a03923b3_N1553794396025", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Application Performance Management (APM) \/ Hive APM \/ ", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Hive-on-Spark", 
"url" : "102157-apms-hive.html#UUID-0848892e-58aa-b0e6-9585-7082a03923b3_section-5cd06161c8e77-idm46542969152736", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Application Performance Management (APM) \/ Hive APM \/ Hive-on-Spark", 
"snippet" : "For Hive-on-Spark jobs the Hive APM shows the details of the Spark app. Like the Spark APM, the running and completed Spark Jobs are listed in the Navigation tab. The progress of a running Job is displayed. In this example, Job 2 is 75% complete. Clicking on the job brings up the Spark Job informati...", 
"body" : "For Hive-on-Spark jobs the Hive APM shows the details of the Spark app. Like the Spark APM, the running and completed Spark Jobs are listed in the Navigation tab. The progress of a running Job is displayed. In this example, Job 2 is 75% complete. Clicking on the job brings up the Spark Job information in a new tile, and from there you can drill down into the Job's Stages. See the Spark APM for more information. " }, 
{ "title" : "Impala APM", 
"url" : "102158-apms-impala.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Application Performance Management (APM) \/ Impala APM", 
"snippet" : "The Impala APM provides a detailed view into the behavior of Impala queries. Left tabs Fragments: Displays a table with information about each fragment associated with this query. Click More to expose the Fragments operators and Less to hide them. The coordinator fragment ( ) is always the nth fragm...", 
"body" : "The Impala APM provides a detailed view into the behavior of Impala queries. Left tabs Fragments: Displays a table with information about each fragment associated with this query. Click More to expose the Fragments operators and Less to hide them. The coordinator fragment ( ) is always the nth fragment. This window shows the Fragment and its KPIs. It defaults to the table of the Fragment's Operators with the associated KPIs for the operations. Clicking the operator brings up the operator window. (See Operators for more information.) You can view the Query Plan or the Instance View. Instance View: Lists each instances with its KPIs. Operators: Displays a list of all operators for all fragments. You can search the operator's name. Click the operator to display its details. Scan HDFS details Aggregate details Exchange details Gannt Chart: Charts the fragments and the time spent on each operation. Hover over a section to see the operation and its KPIs. Query plan: Shows the query plan in fragment or operator view. Both the fragment and operator view are shown here. Hover over the operator to get detailed information. Click the button to switch views. Tags: The defined tag keys and values for the app. This example has two tag keys, project and dept with each having one value, group11 and hr respectively. Right tabs Query: Shows the query plan code. Click Query Copy to copy the query. See Impala APM image above for the Query Tab. Mem Usage: Graphs the Memory Usage by peak usage. Notes the maximum memory used on what host and the estimated memory per host. " }, 
{ "title" : "Key performance indicators", 
"url" : "102158-apms-impala.html#UUID-09bc5d2f-4a9e-fdb4-25b5-31a73221d6c8_N1553795187640", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Application Performance Management (APM) \/ Impala APM \/ Key performance indicators", 
"snippet" : "Events: The number, if any, of Unravel insights for this query. See Event Panel & Insights for more information. Duration: Total time taken by the query. Data I\/O: Total data read and written by the query. Number of Fragments: Total number of query fragments. Number of Operators: Total number of ope...", 
"body" : "Events: The number, if any, of Unravel insights for this query. See Event Panel & Insights for more information. Duration: Total time taken by the query. Data I\/O: Total data read and written by the query. Number of Fragments: Total number of query fragments. Number of Operators: Total number of operators in this query. " }, 
{ "title" : "Kafka APM", 
"url" : "102159-apms-kafka.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Application Performance Management (APM) \/ Kafka APM", 
"snippet" : "The Kafka APM provides multi-cluster support for monitoring: Multi-cluster metrics monitoring. Multi-cluster consumer offset\/lag monitoring. See Kafka Insights for a Use Case example of drilling down into lagging or stalled Consumer Groups. Operations > Usage Details > Kafka displays the List of Con...", 
"body" : "The Kafka APM provides multi-cluster support for monitoring: Multi-cluster metrics monitoring. Multi-cluster consumer offset\/lag monitoring. See Kafka Insights for a Use Case example of drilling down into lagging or stalled Consumer Groups. Operations > Usage Details > Kafka displays the List of Configured Kafka Clusters. Click the Cluster Name to bring up the Cluster View . Cluster view This window has three sections: Key performance indicators Metric graphs Table of Kafka topics Key performance indicators # Under Replicated Partitions # Offline Partitions # Controller Bytes In\/sec Bytes Out\/sec Messages in\/sec Metrics This section has three tabs: Metrics : Graphs the metrics for the data across the entire cluster. Broker : Graphs the metric for the data of the broker you select from the broker list. Topic : Graphs the metric data for a specific topic you select from the topic list. Metrics The Metrics and Broker tab graphs all the following Metrics. The Topic tab displays a subset of the Metrics. In addition, subsets of these Metrics are used in the Topic and Consumer Group pages. While the definition of the metric is the same throughout, the breadth of the collected data is not. Therefore, the implication\/interpretation of the metric varies by context. For each metric, we explain: Its definition. Whether it is used in Topic tab or the Topic and Consumer pages' Topic details tab. The data collected for the metric at each point. For some Metrics, the implication or interpretation of data at the point of collection is explained; along with how you can use it to monitor your cluster or improve its performance. In addition to these Metrics the Topic and Consumer pages Partition tab graphs the partition's Consumer lag and offset. Bytes In Per Second The one minute rate of all the bytes flowing into the Kafka network, specifically the Kafka Producers publishing messages. It indicates how much traffic the brokers are receiving from the Producer clients. This metric is also displayed in the Topic tab and the Topic and Consumer group views. For any given data point, for that point in time, the graph displays: Metrics tab: The total number of bytes flowing into the cluster across all topics and brokers. Broker tab: The number of bytes flowing into the selected broker. Topic tab: The number of bytes flowing into a particular topic across all brokers. Topic view: The same as the Topic tab, but each broker is represented by a trend line so you can see the data flowing into individual topics. Consumer group view: Similar to the Topic page. Analysis: The Brokers tab can help you determine if you need to expand your cluster. There might be situations where it could be possible to identify a broker that is receiving more traffic than others which indicates that there is a need for a partition rebalance. Bytes Out Per Second The one minute rate of all the bytes flowing out of the Kafka network, specifically Kafka Consumers consuming messages from the topics. This metric scales differently than Bytes In Per Second , because of the great job Kafka does handling multiple consumers. It possible for the outgoing rate to be two to three times the incoming rate for topics bring consumed. Before Kafka 0.11.0.0, this metric included the internal replica traffic. Therefore, it showed an outbound rate for a topic that was an actively producing client, even when no consumers were consuming from the same. This metric is also displayed in the Topic tab and the Topic and Consumer group pages. These graphs are exactly like Bytes In Per Second except they display the bytes out. Messages In Per Second The number of messages produced per second. While Bytes In Per Second displays the broker traffic in absolute terms of bytes, this is the number (count) of individual messages that flow in, regardless of their size. This metric is also displayed in the Topic tab and the Topic and Consumer group views. For any given data point, the graph displays, for that point in time: Metrics tab: The total number of messages produced in the cluster across all topics and brokers. Broker tab: The number of messages being written into the selected broker. Topic tab: The number of messages flowing into the selected topic. Topic view: The same as the Topic tab. Each topic, is represented by a trend line so you can see the number of message flowing into each broker. Consumer group view: Similar to the Topic page. Analysis: Useful for end users who are more interested in the message count rather than the network throughput. When used in conjunction with Bytes in Per Seconds you can estimate the size of a single message. Like Bytes in Per Seconds it can help identify broker imbalance so you can rebalance your partitions. Total Fetch Requests Per Second The number of consumer fetch requests sent per second. Like Bytes Out Per Second this is an indicator of the rate consumers are uptaking\/requesting messages from the Kafka cluster. This metric is also displayed in the Topic tab and the Topic view. For any given data point in the graph, for that point in time, it displays: Metrics tab: The total number of fetch requests within the cluster across all topics and brokers. Broker tab: The number of fetch requests received by the selected broker across all topics. Topic tab: The number of fetch requests received for a particular topic across all brokers. Topic view: The total number of fetch requests received for that topic. Each topic, is represented by a trend line so you can see the number of fetch requests flowing into each broker. Analysis: Care must be taken when interpreting this metric because the value also includes replica traffic, that is, the fetch requests that get raised by the brokers to keep the topic partitions in sync. Under Replicated Partitions Per Second The count of under replicated partitions existing in a cluster. Metrics tab: The trend line for under replicated partitions for the entire cluster. Broker tab: The trend line for under replicated partitions for the selected broker. Analysis: This is the metric to monitor. It can provide insight into a number of problems on a Kafka cluster ranging from a broker being down to resource exhaustion. For example, a constant number of under replicated partitions for many brokers might indicate that one of the brokers is down. The count across the cluster is equal to the number of partitions on the faulty broker. # Active Controller Trend The active controller trend line. Metrics tab: The trend line indicates the presence of an active controller for the entire cluster. Broker tab: The trend line indicates the duration for which the selected broker was an active controller. Analysis: On the cluster level the value for this metric should always be one, that is, at all times only one broker should be the active controller. Any other value could indicate the cluster might be susceptible to administrative issues such as partition reassignment, etc. On a broker level, except for the active controller, the brokers should return zero. Request Handler Idle Ratio Average Per Minute The request handler idle ratio trend over a time period. It indicates the percentage of the time the request handlers aren't in use. Metrics tab: The trend line is the metric value across the entire cluster averaged over all brokers. Broker tab: The trend line is the metric value for the selected broker. Analysis: Lower numbers indicate increased load on the broker. Anything under 0.2 (20 percent) is a cause for concern. Values under 0.1 (10 percent) usually indicate an active performance problem. Typically, there are two reasons for high thread utilization: There aren't enough threads. The threads are devoting some of their resources to overhead tasks and not servicing the client requests itself. # Partition Count The number of partitions. The count includes both leader and follower replica. It should be basically constant over time. Metrics tab: The trend line indicates the metric's value across the entire cluster averaged over all brokers. Broker tab: The trend line indicates the metric's value for the selected broker. Analysis: Expect this to be basically constant over time. This count includes both leader and follower replica. # Leader Partition Count The number of partition leaders. Metrics tab: The trend line indicates the metric's value across the entire cluster averaged over all brokers. Broker tab: The trend line indicates the metric's value for the selected broker. It represents the number of partitions for which the broker is currently a leader. Analysis: Consider monitoring this metric because it gives you a qualitative distinction between the replicas on a broker in regards to leaders and followers. Even if the replicas are balanced in size or number across the cluster, there might be a broker who has an indiscriminately large\/small number of leaders. This is an indication of imbalance. You can use this metric in conjunction with # Partition Count to calculate the percentage of partitions on broker for the partition the broker is a leader. In well-balanced clusters this should be uniform. For example, if you have a replication factor of two all brokers should be leaders of approximately 50% of their partitions. # Offline Partition Count The number of offline partitions in the cluster indicating the number of partitions in the cluster that have no leader. Metrics tab: The trend line indicates the metric's value across the entire cluster averaged over all brokers. Broker tab: The trend line indicates the metric's value for the selected broker. Analysis: Along with Under Replicated Partitions , this is a critical metric to monitor. Such partitions are inaccessible to clients because produce and fetch requests are sent only to leaders. This could result in issues like producer clients losing messages. One potential reason for the presence of offline partitions is the broker\/brokers hosting the leader replicas are down. Fetch Total Time, 99th Percentile The 99th percentile of the total turnaround time (time from receiving the request to sending a response back) the broker spends processing the fetch request. 99% of all values in a group of fetch request timing are less than this metric's value. Metrics tab: Shows the time value for the entire cluster averaged across all brokers. Broker tab: Shows the time value for the selected broker. Analysis: You can gain insight into how an average request performs and what are the outliers by correlating this metric value with the average value of the metric. Produce Total Time, 99th Percentile The 99th percentile of the total turnaround time (time from receiving the request to sending a response back) the broker spends processing the produce request. 99% of all values in a group of fetch request timing are less than this metric's value. Metrics tab: Shows the time value for the entire cluster averaged across all brokers. Broker tab: Shows the time value for the selected broker. Analysis: You can gain insight into how an average request performs and what are the outliers by correlating this metric value with the average value of the metric. You can use this to set a baseline of sorts. Much like Under Replicated Partitions , a spike in the 99 percentile for Produce requests can alert you to a wide range of performance issues. # Fetch Requests Per Sec Displays the one minute rate for all the fetch requests being received and processed. Like Bytes Out Per Sec , it is an indicator of the rate at which consumers are uptaking\/requesting messages from the Kafka cluster. Metrics tab: Shows the summation of the fetch requests being processed across all brokers per second. Broker tab: Shows the fetch requests being processed by a specific broker per second. Analysis: You must take care when interpreting this metric. In addition to the consumer client fetch request, the value also includes replica traffic, that is, the fetch requests that get raised by the brokers to keep the topic partitions in sync. # Produce Requests Per Sec Displays the one minute rate of all the Produce Requests being received and processed. Like Bytes In Per Sec , it indicates how much traffic the brokers are receiving from the Producer clients. Metrics tab: Shows the summation of the produce requests being processed across all brokers per second. Broker tab: Shows the produce requests being processed by a specific broker per second. Analysis: This metric can be useful in deciding whether to expand your cluster. Additionally, it can be helpful when tracked at the broker level. On a normal Kafka cluster the value across all brokers should be basically the same. It helps you identify a broker that is receiving more traffic than others indicating there is a need for a partition rebalance. Produce Purgatory Size The number of Produce requests sitting in the Producer Request Purgatory. It is a holding pen for requests waiting to be satisfied (Delayed). Of all Kafka request types, it is used only for Produce requests. It tracks the number of requests sitting in purgatory (including both watchers map and expiration queue). Metrics tab: The count for the entire cluster averaged across all brokers. Broker tab: The count for the selected broker. Analysis: You can use this metric as a rough gauge of memory usage. Fetch Purgatory Size The number of Fetch requests sitting in the Fetch Request Purgatory. It is a holding pen for requests waiting to be satisfied (Delayed). Of all Kafka request types, it is used only for Fetch requests. It tracks the number of requests sitting in purgatory (including both watchers map and expiration Metrics tab: The count for the entire cluster averaged across all brokers. Broker tab: The count for the selected broker. Analysis: You can use this metric as a rough gauge of memory usage. Log Flush Latency, 99th Percentile The 99th percentile value of the latency incurred by a log flush, that is, write to disk in milliseconds. 99% of all values in the group are less than the value of the metric. Metrics tab: The time value for the entire cluster averaged across all brokers Broker tab: The time value for the selected broker. Analysis: Log flushes are generally one of the most expensive operations, and can affect the durability, latency and throughput of the Kafka cluster. This trend line depicts what's the log latency like for the cluster and can help in configuring the log flush interval. Metrics tab Broker tab TOPIC tab Kafka topics list consumed by a consumer group (CG) with relevant KPIs. Click the topic to bring up the Topic view and the consumer to bring up the Consumer group view. By default it is sorted on Topic name, this table is sorted on Consumer Groups in order to see the active consumer groups. The Consumer Groups status is clearly indicated below it. See Kafka Insights for a Use Case example of locating of lagging or stalled Consumer Groups. Consumer group page Key Performance Indicators Number of Topics Number of Partitions The Topic list displays the KPIs; when details are available a more info icon is displayed. Click it to bring up the Kafka view for the topic. Below the list are two tabs which display graphs of the Topic and Partition details. By default, the window opens with the Topic Detail graph displayed. Partition Tab You can choose both the Partition and the Metric for the display. By default, the 0 th partition is displayed using the metric offset . The Partition Details ' list is populated if the details are available. Topic page The Kafka View has two tabs, Topic Detail and Partition Detail . Each view has a Consumer Details' list which is populated if the details are available. Kafka Topic Detail By default, the Kafka Topic Detail opens in the Topic Detail view which graphs the KPIs. The KPIs in the title bar, present Kafka Partition Detail You can choose both the Partition and the Metric for the display. By default, the 0 th partition will be displayed on using the metric offset . Unravel insights for Kafka Unravel provides auto-detection of lagging\/stalled Consumer Groups. It lets you drill down into your cluster and determine which consumers, topics, partitions are lagging or stalled. See Kafka Insights for a use case example of drilling down into lagging or stalled Consumer Groups. Unravel determines Consumer status by evaluating the consumer's behavior over a sliding window. For example, we use average lag trend for 10 intervals (of 5 minutes duration each), covering a 50-minute period. Consumer Status is evaluated on several factors during the window for each partition it is consuming. For a topic partition Consumer status is: Stalled : If the Consumer commit offset for the topic partition is not increasing and lag is greater than zero. Lagging : If the Consumer lag for the topic partition is increasing consistently, and an increase in lag from the start of the window to the last value is greater than lag threshold (e.g., 250). The information is distilled down into a status for each partition, and then into a single status for the consumer. A consumer is either in one of the following states: OK : The consumer is working and is current. Warning : The consumer is working, but falling behind. Error : The consumer has stopped or stalled. " }, 
{ "title" : "MapReduce APM", 
"url" : "102160-apms-mapreduce.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Application Performance Management (APM) \/ MapReduce APM", 
"snippet" : "The MapReduce APM provides and easy way to understand the breakdown of the app. You can use this view to: Drill down into MapReduce jobs that make up the app, and Resolve inefficiencies, bottlenecks and reasons for failure within apps. It contains similar sections to the Hive APM and additionally sh...", 
"body" : "The MapReduce APM provides and easy way to understand the breakdown of the app. You can use this view to: Drill down into MapReduce jobs that make up the app, and Resolve inefficiencies, bottlenecks and reasons for failure within apps. It contains similar sections to the Hive APM and additionally shows the timeline view of MapReduce job execution, logs and configuration. Key performance indicators Events: The number, if any, of Unravel insights for this query. See Event Panel & Insights for more information. Duration: Total time taken by the app to complete execution. Data I\/O: Total data read and written by the app. Tabs By default, the MapReduce APM opens in the Graphs > Attempts view. Graphs: Has four sub tabs. Attempts: Number of task attempts are charted in \"wall-clock\" time. The aggregated time of all tasks running in the Map\/Reduce slot duration is listed under the graph. Containers , vCores , and Memory: Graphs utilization of slot containers, vCores, and memory over time. When there is no data available for a graph, the tab isn't shown. When a graph has less than four data points, the graph is converted to a bar graph. Timeline: Displays the details of each MapReduce job by showing the execution of each task on the machine it was executed on. The Timeline tab is divided into two sections: A Distribution chart (which displays either the Map or Reduce tasks) A bottom table which lists either the tasks by stages on servers or the list of tasks and their associated KPIs. The default displays the Map jobs and the timeline. You can change the Distribution Charts by selecting Map or Reduce (1). Whether to display the Timeline or Selected tasks (3). When displaying the timeline you can filter the display by Map, Reduce, Killed\/Failed or All jobs (2). You can choose what timeline\/tasks to display by dragging and highlighting a section of the distribution chart. Metrics: The metrics, their definitions, and values. Logs: Lists the available logs by Map, Reduce and Application Master. Click the tab to see the listing for that type (Map, Reduce, or Application Master). Click an item to see the log. Configuration: The defined parameters and their values. Resource Usage: These graphs are useful for identifying critical resources that caused a performance degradation. Initially all the executors are displayed using the Metric systemCpuLoad. You can choose a different metric to display from the Metric pull down menu. Hover over an executor to bring up the © option and click it to display just the information for that executor. To graph all executors, hover above the list for the Show All option and click it to display all. When no resource data is available the tab displays No metrics found. Metrics are displayed after Unravel has processed the app completely and Unravel's application sensors are enabled. Errors: Lists all the job's errors. Like job status, the errors are color coded and the total number of each type (fatal, errors, warnings) is noted. The top line lists the number of all jobs and tasks. The errors are grouped by tasks\/jobs and then by severity. For each job\/task the total and type of errors are noted. Time, keywords (if any) and a brief message is displayed for the error. Keywords extract important details from the errors messages and log data that can help developers and operators quickly \"root cause\" issues. \"No errors found\" is displayed when there are none. Tags: The defined tag keys and values for the app. This example has two tag keys, project and dept with each having one value, group11 and hr respectively. " }, 
{ "title" : "Tez APM", 
"url" : "102161-apms-tez.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Application Performance Management (APM) \/ Tez APM", 
"snippet" : "The Tez APM provides a detailed view into the behavior of Hive queries as a DAG (Directed Acyclic Graph). To troubleshoot Tez data collection issues, check \/usr\/local\/unravel\/logs\/unravel_ew_1.log . Key performance indicators Events : The number, if any, of Unravel insights for this query. See Event...", 
"body" : "The Tez APM provides a detailed view into the behavior of Hive queries as a DAG (Directed Acyclic Graph). To troubleshoot Tez data collection issues, check \/usr\/local\/unravel\/logs\/unravel_ew_1.log . Key performance indicators Events : The number, if any, of Unravel insights for this query. See Event Panel & Insights for more information. Duration: Total time taken by the query. Data I\/O: Total data read and written by the query. By default the Tez APM opens showing the Navigation and Program Tabs. Left tabs Navigation: Lists the DAG jobs with KPIs, Duration and I\/O. Configuration: List the configuration parameters and their values. Tags: The defined tag keys and values for the app. This example has two tag keys, project and dept with each having one value, group11 and hr respectively. Right tabs Program: Displays the query. Graphs: Has three sub tabs. Containers , vCores , and Memory: Graphs utilization of slot containers, vCores, and memory over time. When there is no data available for a graph, the tab isn't shown. When a graph has less than four data points, the graph is converted to a bar graph. Resources: Graphs the resources consumed. By default, the Resource tab displays the first 10 series using the systemCpuLoad metric. You can select one or more series to display in the Select Box. You can change the default number of series to show (1-n). Clicking on a series name causes the graph to display that series alone. You choose the Metric to graph from the pull down menu. Click Get Data to retrieve the data for that metric, it can be viewed in its Raw form, JSON, or headers. When no resource data is available the tab displays No metrics found. Metrics are displayed after Unravel has processed the app completely and Unravel's application sensors are enabled. " }, 
{ "title" : "DAG detail", 
"url" : "102161-apms-tez.html#UUID-7e1e6bff-7de1-86c5-1def-42248c75d3cd_N1566041809604", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Application Performance Management (APM) \/ Tez APM \/ DAG detail", 
"snippet" : "The DAG detail has six tabs: Query: Displays the query. Graph: Displays the vertices and their relationship to each other. Clicking on a node brings up the task details. Counter: Lists all the relevant counters for the Tez-DAG and their values. Vertex Timeline: Displays the timeline for all tasks. T...", 
"body" : "The DAG detail has six tabs: Query: Displays the query. Graph: Displays the vertices and their relationship to each other. Clicking on a node brings up the task details. Counter: Lists all the relevant counters for the Tez-DAG and their values. Vertex Timeline: Displays the timeline for all tasks. The task time can be displayed in both Wall Clock time and Total Run time as applicable. Hover over the task to display the information in text. All Vertices: List each vertex and their KPIs. The Vertices are searchable by Vertex Name; vertices containing the string will be displayed. All Task: List all tasks, their status (failed, success, etc.), vertex name and other relevant information. The tasks are searchable by Task ID and Vertex name; Tasks containing the string will be displayed. All Task Attempts: List all attempts, their status (failed, success, etc.), vertex name and other relevant information. The task attempts are searchable by Attempt ID, Task ID and Vertex name; Task attempts containing the string will be displayed. Changed Configuration: Lists all relevant parameters and their value. " }, 
{ "title" : "Workflow APM", 
"url" : "102162-apms-workflow.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Application Performance Management (APM) \/ Workflow APM", 
"snippet" : "The Workflow APM provides a comprehensive view to understand workflows and their patterns of execution. It is used by Workflow (Pipelines) owners to Identify anomalies, inefficiencies and bottlenecks in workflow instances. The Workflow Manager helps pipeline owners easily maintain SLAs. (Apps that h...", 
"body" : "The Workflow APM provides a comprehensive view to understand workflows and their patterns of execution. It is used by Workflow (Pipelines) owners to Identify anomalies, inefficiencies and bottlenecks in workflow instances. The Workflow Manager helps pipeline owners easily maintain SLAs. (Apps that have a Workflow parent have a link to the workflow in the Goto column in Applications > Applications .) Key performance indicators Events: The number, if any, of Unravel insights for this query. See Event Panel & Insights for more information. Duration: Total time taken by the query. Data I\/O: Total data read and written by the query. Number of Yarn Apps: The number of apps that make up the workflow. The APM opens showing the Navigation and Compare tabs by default. Left tabs Navigation: Provides an easy way to understand the breakdown of the workflow the apps which comprise the Workflow, e.g., Hive, Spark, MapReduce, Oozie. Click More to display the jobs\/apps which comprise the type. This image shows the second Oozie node, which consists one MapReduce job and three Hive jobs. The Hive jobs comprise one or more tasks, so that too can be expanded. The second Oozie node has been expanded along with the first Hive job within it. You can click a job to see its app manage, for example, you can click the expanded Hive job to bring up the Hive APM. Similarly, you can click the mapreduce job within the Hive job to go directly to it. Click Less to close the list. Execution: Displays the execution graph of the workflow. Click to zoom in, and to zoom in. Click to return to the initial display. Hover over a node within the graph to see a text box which information about the node task. Errors: Lists all the job's errors. Like job status, the errors are color coded and the total number of each type (fatal, errors, warnings) is noted. The top line lists the number of all jobs and tasks. The errors are grouped by tasks\/jobs and then by severity. For each job\/task the total and type of errors are noted. Time, keywords (if any) and a brief message is displayed for the error. Keywords extract important details from the errors messages and log data that can help developers and operators quickly \"root cause\" issues. \"No errors found\" is displayed when there are none. Tags : Tags: The defined tag keys and values for the app. This example has two tag keys, project and dept with each having one value, group11 and hr respectively. Right tabs Compare: Provides a quick way to understand how well a workflow run compares to its other runs. Hovering your pointer graph displays instances top KPIs such as Duration , Data I\/O, Resources , and the number of jobs in that instance. Clicking on the point in the chart brings up the Workflow APM for that instance. The graph Metrics choices are I\/O , MR Jobs , Resource and Events . The Workflow APM above show an example of the compare tab. For Unravel for Azure Databricks This tab compares workflow instances and the metrics available to you are: Duration , Data I\/O, Apps , and Events . Task Attempts: Displays charts for Map Task, Reduce, and Spark Tasks, broken down by success, failed, and killed as appropriate. Attempts: Graphs the attempts over the time interval in Wall Clock time. The Map and Reduce Slot Duration in total computing time is listed beneath the line graph. " }, 
{ "title" : "Athena APM (preview)", 
"url" : "102163-apms-athena.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Application Performance Management (APM) \/ Athena APM (preview)", 
"snippet" : "The Athena APM is currently in preview (beta). The KPI bar contains information about your query. Currently, Unravel does not have events for Athena jobs. Since Athena jobs are retrieved upon completion the job status can only be success, killed, or failed. Next to the status box is your job informa...", 
"body" : "The Athena APM is currently in preview (beta). The KPI bar contains information about your query. Currently, Unravel does not have events for Athena jobs. Since Athena jobs are retrieved upon completion the job status can only be success, killed, or failed. Next to the status box is your job information. App ID User, user type, account ID, database, and start time Output Key performance indicators Duration : Total time taken by the query. Data I\/O : The amount of data scanned by the query. Cost : The cost is calculated based upon Amazon's current pricing . Amazon charges based upon the data scanned for successful and killed apps. There are no costs for failed apps. The Query tab displays your query. " }, 
{ "title" : "Cascading and Pig APM", 
"url" : "102164-apms-cascading-pig.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Application Performance Management (APM) \/ Cascading and Pig APM", 
"snippet" : "The only difference between these two APM's is the job type name; otherwise they are the same. This example uses the Pig APM. By default, the window opens displaying the Navigation and Task Attempts. Key performance indicators Events: The number, if any, of Unravel insights for this query. See Event...", 
"body" : "The only difference between these two APM's is the job type name; otherwise they are the same. This example uses the Pig APM. By default, the window opens displaying the Navigation and Task Attempts. Key performance indicators Events: The number, if any, of Unravel insights for this query. See Event Panel & Insights for more information. Duration: Total time taken by the query. Data I\/O: Total data read and written by the query. Number of YARN Apps: The number of apps that make up the workflow. Left tabs Gantt Chart : Exceptions, errors, and warnings associated with this app. Reusing topic #UUID-30559ede-3b59-31c6-1b76-c72d6f36ecb2 Right tabs Task Attempts: Displays map and reduce task attempts by success, failed, and killed status. The data displayed is for the entire Hive job. To see the details for a specific MapReduce task click the job in the Navigation tab. The Pig APM above shows the Task Attempts. Attempts: Graphs the map and reduce task slot usage over the duration of the job. The wall clock time is noted in the upper left-hand corner. The computer slot usage is shown in this graph. " }, 
{ "title" : "HBase alerts and metrics", 
"url" : "102165-hbase-metrics.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Application Performance Management (APM) \/ HBase alerts and metrics", 
"snippet" : "Alerts Alerts generated and stored along with metrics. Unravel UI plots this information as appropriate. Category Alert Suggested Action Data availability Table offline Run hbase hbck to see if your HBase cluster has corruptions and use -repair flag if required. Check master logs for more informatio...", 
"body" : "Alerts Alerts generated and stored along with metrics. Unravel UI plots this information as appropriate. Category Alert Suggested Action Data availability Table offline Run hbase hbck to see if your HBase cluster has corruptions and use -repair flag if required. Check master logs for more information. Region offline Run hbase hbck to see if your HBase cluster has corruptions and use -repair flag if required. Check master logs for more information. Region in transition beyond threshold period. If a region server is dead, this is common. If not run hbase hbck to see if your HBase cluster has corruptions. Server availability Dead region servers Check region server logs for more information. Performance Region servers with reads > 20% of average Region server hotspotting. Split regions or randomize the keys. Region servers with writes > 20% of average Region server hotspotting. Split regions or randomize the keys. Regions within a table with reads > 20% of average for that table Table hotspotting - Split regions or randomize the keys. Regions within a table with writes > 20% of average for that table Table hotspotting - Split regions or randomize the keys. Regions within a regionserver with reads > 20% of average for that table Region server hotspotting - Split regions or randomize the keys. Regions within a regionserver with writes > 20% of average for that table Region server hotspotting - Split regions or randomize the keys. Load, osload > 20% of average Check for compactions, regions in transition and server logs. Balancer not running Enable Balancer. Number of compactions and length of compaction Disable periodic automatic major compactions by setting - hbase.hregion.majorcompaction to 0 Storage Regionservers with storage (storefilesie sum) > 20% of average Split or randomize the keys. Regions within a table with storage (storefilesie sum) > 20% of average for that table Split or randomize the keys. Temporal e.g. requests > 20% higher for the last 1 hour as compared to the prior 3 hours (just an example) Check master and region server alerts or environment issues which could be slowing down the read\/write. Metrics Master\/Cluster & JMX metrics Metric Description Unit averageLoad Average number of Regions per Region Server. percentage clusterRequests Number of read and write requests across Cluster. count masterActiveTime Master Active Time epoch in milliseconds masterStartTime Master Start Time epoch in milliseconds numDeadRegionServers Number of dead Region Servers. count numRegionServers Number of live Region Servers. count ritCount The number of regions in transition. count ritCountOverThreshold The number of regions that have been in transition longer than a threshold time. seconds ritOldestAge The age of the longest region in transition, in milliseconds. millliseconds OS Metrics (Ambari Only) OS Metrics Description Unit jvm_* jvm metrics number rpc_* rpc metrics number Region server metrics JMX metrics JMX Metrics Description Unit compactionQueueLength Current depth of the compaction request queue. If increasing, we are falling behind with storefile compaction. count hlogFileSize Size of all WAL Files. bytes percentFilesLocal Percent of store file data that can be read from the local DataNode, 0-100. percentage readRequestCount The number of read requests received. count regionCount The number of regions hosted by the regionserver. count slowOPCount The number of operations we thought were slow. OP: delete, get, put, increment, append. count storeFileSize Aggregate size of the store files on disk. bytes writeRequestCount The number of write requests received. count OS Metrics (Ambari Only) OS Metrics Description Unit cpu_user cpu percentage disk.disk_free Amount of free disk space. bytes disk.write_bps Number of bytes written per second to disk. bytes per second disk.read_bps Number of bytes read per second to disk. bytes per second load.load_one load number memory.mem_free Percentage of free memory. percentage network.bytes_in Total number incoming bytes to network. bytes network.bytes_out Total number outgoing bytes to network. bytes Table\/Region Metrics Table and Region Metrics Description Unit tableSize Total table size in the region server. bytes regionCount Number of regions. count averageRegionSize (Table only) Average region size over the region server including memstore and storefile sizes. bytes storeFileSize Size of storefiles being served. bytes readRequestCount Number of read requests this region server has answered. count writeRequestCount Number of mutation requests this region server has answered. count " }, 
{ "title" : "AutoActions", 
"url" : "102166-auto-actions.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ AutoActions", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Overview", 
"url" : "102167-auto-actions-overview.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ AutoActions \/ Overview", 
"snippet" : "Unravel's AutoActions automates the monitoring of your compute cluster by allowing you to define complex actionable rules on different cluster metrics. You can use an AutoAction to alert you to a situation needing manual intervention, for example, resource contention or stuck jobs. Additionally, it ...", 
"body" : "Unravel's AutoActions automates the monitoring of your compute cluster by allowing you to define complex actionable rules on different cluster metrics. You can use an AutoAction to alert you to a situation needing manual intervention, for example, resource contention or stuck jobs. Additionally, it can be set to automatically kill an app or move it to a different queue. The Unravel Server processes AutoActions by: Collecting various metrics from the cluster. Aggregating the collected metrics according to user-defined rules. Detecting rule violations. Applying defined actions for each rule violation. Each rule consists of: A logical expression that is used to aggregate cluster metrics and to evaluate the rule. A rule has two conditions: Prerequisite conditions : The conditions which cause a violation, for example, the number of jobs running, memory used. Defining conditions : Who\/what\/when can cause the violation, for example, user, apps. Actions for Unravel Server to execute whenever it detects a rule violation. Manage > Auto Actions The AutoActions tab provides a quick way to view AutoActions and quickly see their status, along with its defined actions and scope. The tab displays all defined AutoActions separated into an Active and Inactive list. You enable\/disable by clicking the check box on the left. You can edit ( ) or delete ( ) an AutoAction regardless of its status. Click to copy the AutoActions JSON code. At the top are buttons which allow you to define new AutoActions . Hovering over the AutoAction's name gives you the description which was entered when defining the AutoAction. Hovering over action or scope glyph brings up its detail. For example, for the active AutoAction above: Rule description: . Email action: an email is sent to only one person, . Queue scope: is three queues, . The Actions and Scope columns contains all available options. When an option has been set, i.e., no longer using the default setting, it is highlighted. It is possible to set an AutoAction which contains no actions, see quicktest below. Such an AutoAction simply has when it was triggered logs and retains the data. Every AutoAction must have a scope. When the AutoAction has an action or scope defined via an Expert Rule , that action or scope isn't noted in the table. The AutoAction, quicktest below, doesn't note a scope; however one was specified via an Expert Rule. The History of Runs column lists the number of times, if any, the AutoAction was triggered. Click the number to bring up its history. By default, all actions are off. Possible actions are: Send an Email ( ). Kill the App ( ). Move the app to another queue ( ). Send an HTTP post ( ). By default, the various scopes apply to all, i.e., all apps and constantly on. The scopes are: User ( ). Queue ( ). Cluster ( ). App ( ). Time ( ). Sustained Violation (This isn't shown in the AutoActions list). If you haven't defined a particular action or scope, that is, it's using the default, the glyph is gray ( ). When defined the glyph is blue in an active AutoAction ( ), and darkened for a disabled action ( ). Click the History of Runs for detailed information when the AutoAction was triggered. The history notes the time the action was triggered, contains a link a Cluster View (see below), and a link to the offending app. Hover over the app's link to see the app's type and click it to bring the app's APM. Click the run's Link button for the Cluster View ( Operations > Usage Details > Infrastructure ) for that particular run. The Cluster View shows a time slice, ±5 minutes from when the AutoAction was triggered and lists all the apps running during that period. This app table is similar to the app table shown under Applications > Applications . Not all the running apps will have triggered an AutoAction during the time slice. Click in the graph to show the apps running at that point in time. The Notifications column ( ) notes if the app triggered the AutoAction ( ), has tuning suggestions ( ), or both ( ). In the example below, two apps were running at the time; both triggered the action but neither has tuning suggestions. Hovering over brings up a pop-up listing the violations; in this example the first app violated two separate AutoActions. 'Snoozing' AutoActions AutoActions violations can become \"noisy\", i.e., an app continues to violate an AutoAction but the violation adds no new information. 'Snoozing' AutoActions helps you filter out noise by preventing automatic actions from repeating during a specified period, if and only if , it is the same violation context, and the action adds no further information to the violation. " }, 
{ "title" : "Limitations", 
"url" : "102168-auto-actions-limitations.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ AutoActions \/ Limitations", 
"snippet" : "Alerting on running apps Applications of the following types don't provide any means for real-time alerts, i.e. when in the running state. Once the app has finished, the alerts are generated notifying users about policy violations that already occurred. Impala Hive-on-Tez Note : Before 4.5.0.7 no ru...", 
"body" : "Alerting on running apps Applications of the following types don't provide any means for real-time alerts, i.e. when in the running state. Once the app has finished, the alerts are generated notifying users about policy violations that already occurred. Impala Hive-on-Tez Note : Before 4.5.0.7 no running Hive apps alerted in real-time. Running duration versus final duration inconsistency Unravel calculates and publishes internally the current duration for apps of the following types in real-time, that is, when in the running state. Upon the app completion Unravel receives the actual end time and performs the final duration calculation. This can lead to an inconsistency where the duration aggregated and published during the running state is greater than the duration published upon the app's completion. Workflow Missing AutoAction violation badge The badge isn't displayed for following app types. For example, in the cluster view above these apps don't display even when they have violated an AutoAction during the time period shown. Hive Workflow Running and failed Impala apps. Hive on Tez Unsupported Kill action for the following apps Hive Workflow Prior to Unravel v4.5.3.0 Impala apps couldn't be killed. Move to Queue action for the following apps Hive Impala Workflow \"Cloud\" type setups Unravel for EMR and Unravel for HDInsight Kill and move actions for all types of apps. Rules that span multiple clusters. In multi-cluster configurations AutoActions doesn't differentiate between entities of each cluster and setting up a policy targets all monitored clusters. For instance, setting up a rule to target root queue causes it to be monitored on all clusters. Workaround: If the cluster ID is known isolate the policy for the cluster using policy options. Uses the internal Hadoop cluster ID instead of Unravel cluster ID\/name. You must obtain the internal cluster ID in order to specify a Hadoop cluster in the policy options section. It can be obtained from HDFS namenode, where it’s stored in {dfs.namenode.name.dir}\/current\/VERSION. In case of transport message protocol synchronization error, n exceptionally rare occasion AutoAction can be triggered up to 180 seconds after the violation occurs. No data loss is expected. Recent Events & Alerts shows the events across all clusters regardless of the currently selected cluster. Application Master level metrics, such as job metrics and job counters, aren't collected by EMR sensor by default and therefore cann't be used in AA policies. Collection of AM metrics can be enabled manually using “am-polling” option in EMR sensor. In exceptionally rare cases AutoActions can be triggered up to 180 seconds later in case of transport message protocol synchronization error but no data loss is expected. Prior to Unravel v4.5.2.0 Cloud Release, AutoActions aren't supported. All other Cloud platforms, for example, Unravel for Azure Databricks, Quoble, etc. There is no support for AutoActions. AutoActions properties See AutoAction properties general AutoActions and AutoAction daemon properties. " }, 
{ "title" : "Templates", 
"url" : "102169-auto-actions-templates.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ AutoActions \/ Templates", 
"snippet" : "See here for limitations on auto actions. Whether using Create from Template or Build Rule you have five sections. Expert Rule is simply a text box. The sections are: Name and description The name is mandatory and is used by the UI for all auto actions' displays; we recommend using a name which refl...", 
"body" : "See here for limitations on auto actions. Whether using Create from Template or Build Rule you have five sections. Expert Rule is simply a text box. The sections are: Name and description The name is mandatory and is used by the UI for all auto actions' displays; we recommend using a name which reflects the auto action's purpose. The description is optional, but we recommend completing it with a succinct description of the action. When users hover over the action's name, the description is displayed. This example is from Create from Template . The name is already filled with the same name as the template and is highlighted. You can change the name. Ruleset At least one rule type ( User , Queue , Cluster , Apps , and Expert Rule ) must be defined. You can define rules for each rule type using pull down menus for a metric , type or state . The Expert Rule is available only in the Build Rule template. Metric defines a rule \"metric\" \"comparison operator\" \"value\". See Supported Cluster Metrics for a list and definition of available metrics . The comparison operators: >, >=, ==, <, and <=. Value: any valid numeric value. The default value is 0; were you to leave it the auto action would constantly trigger. The Type options are: MapReduce, YARN, Tez, Spark, Impala, Workflow and Hive The State options are: new, new_saving, submitted, accepted, scheduled, allocated, allocatedSaving, launched, running, finishing, finished, killed, failed, undefined, newAny, allocatedAny, pending, and * (all). Multiple rule types are evaluated in conjunction with each other using: Or , And , or Same Or and And work as you would expect. See Same Logical Operator for the definition of Same and its implementation. Using create from template This template has the Ruleset defined as needed to fulfill the template type, for example, Rogue Contention In Queues (allocated memory). The template highlights the fields you can or should change. You cann't delete\/add any of the rule types or add\/delete rules. You can change the metric , comparison , type , and state if available by using the pull down menus. If you change the Metric , Type , or State the template doesn'tperform the task you have selected, for example, Rogue Contention In Queues (allocated memory). The default value for the metric comparison is 0. You must change the value otherwise the auto action constantly triggers. Multiple rules types are Same'd together. See Same Logical Operator for its definition and implementation Using build rule The Ruleset initially lists the type of rules available, User , Queue, Cluster , App , or EXPERT RULE . Click the rule type you want to define. Below Add Queue is selected with the options to add rules for metric , type and state . These options and only these options, are available for every rule type except the Expert Rule template which is a text box. You must define at least one rule for each of the rule type selected. In the example below, Metric and Type are selected for the Queue rule type. You use the pull-down menus to select metric , the comparison operator , type , or state . See above for further information. A second rule , Apps has been added. When multiple rules are selected , you must choose how they are evaluated in conjunction with each other. The default is the Same operator, but you may select Or or And . See Same Logical Operator for the definition of Same and its implementation. Or and And work as you would expect . You can choose up to two rules, e.g., user & user, expert rule & queue, etc. Click Close to delete a rule type and click trash ( ) to delete a specific rule. If you close the rule before saving the auto actions, your settings are lost. Options Define the scope ( User , Queue , Cluster , and Application Name ), the period in which the auto action acts on a violation ( Time ), and how long\/short the violation must occur before the auto action takes action ( Sustained ). When you select an option its default is All, except Time which defaults to always and Sustained Violation which defaults to none. When using Create from Template the required option is already checked and uses the default. Any changes you make may cause the auto action to not perform as expected. As of 4.5.07 when using Create from Template the options sections lists only options that are available for the chosen template. For instance, the Rogue Impala templates only has Queue , Cluster , and Application Name options. Check the box next to the option's name to select it. You can narrow the scope of User , Queue , Cluster , and Application Name by using Only or Except . Only applies the rule to only those apps specified, while Except applies them to all but those specified . Use the Transform to specify the names using a regular expression. The example below is using the Application Name in the Except mode with the app MyApp. You can add more apps by clicking Add Application . Since no regular expression is specified, this option applies to all apps except MyApp. Create from Template defaults to All . The Time sets the time range and time zone during which the auto action can be triggered. The auto action remains active but doesn't trigger outside of the specified time range. The default start and end time is when you defined auto action with the time zone set to America\/Los Angeles. If you don't change the default time the auto action can be triggered for only one minute a day. Enter the time directly or click on the clock ( ) in the time box. Time is entered in 24 hour time. The end time must be later than the start time. Sustained violation specifies a length of time violation must occur before the auto action is triggered. This allows time for the violator to \"self correct\" and lowers the number of false positives The default is zero, i.e., all auto actions are immediately triggered upon violation and the specified action is carried out. You can select minimum or maximum mode. In both cases the auto action must be continually violated. Minimum sustained mode triggers the action only if this violation was continuously detected for at least the specified period. This suppresses triggering of violation actions for “on-offs” and metric spikes. These are normal in multi-tenant cluster environments can return to normal operation on their own. If a violation stops before the minimum time period, the clock is reset for that app. For instance, if the minimum time is one hour and the app violates the auto action for 58 minutes and then returns to normal – no action is taken and the time period for that app resets to 0. Maximum sustained mode triggers the action only if this violation is continuously detected for less than the specified time period. This suppresses the triggering of violations for long-running apps and triggers on auto action rule scope on ad hoc short-lived user apps. Actions Defines the actions to take when the auto action is triggered. As of 4.5.0.7 when using Create from Template the actions section lists only options that are available for the chosen template. For instance, Impala and Workflow templates don't have the option to Move the app or workflow. Build Rule and Create from Template , exception for Impala query, you can specify the following actions: Send an email , HTTP Post , Post to Slack , Move App to Queue , and Kill App . Use Build Rule to enter an Expert Action . See Expert Mode for information on defining an action in JSON. See Auto Actions and Pagerduty for using Unravel's API to receive notifications via Pagerduty. You can't kill a Hive, Impala, or Workflow app. You can choose one or more actions. Check the box to choose that action. If you chose no actions, the UI simply records the violation and saves the data for the cluster view. Shown below are all the possible actions; in Create from Template only actions valid for the template are available. For Send Email you must enter at least one recipient. Add more recipients by clicking Add Recipient . You can also specify to include the owner of the app selecting the Include Owner radio button. For HTTP post you must enter at least one URL. You can add more URLs by clicking Add URL . Post to Slack Unravel provides integration with SlackApp allowing you to post information to one or more Slack channels and users. In order to use this feature you need a Slack: Webhook URL : The incoming webhook URL configured in Slack for the channel or user. You can post to multiple webhooks. Token : The OAuth access token for the SlackApp. See Slack's Incoming Webhooks for further information on creating\/obtaining the above. 4.5.0.0 - 4.5.0.7 Prior to 4.5.0.7 Unravel uses the official Slack API available at https:\/\/slack.com\/api\/chat.postMessage . Using it, you can post simultaneously to: Public Slack channel Private channel Direct message\/IM channel It provides a better integration with Slack Service so you can send a direct message to the owner of Hadoop job's violating the Auto Action. You must generate the token via a Slack service website. For more information on generating the token see: test tokens - generic user icon and \"bot\" username custom bot user token - generic bot icon, with generic \"bot\" username Slack App user token with chat:write:bot - inherits Slack App's icon, with generic \"bot\" username Slack App bot user token - inherits Slack App's icon, with generic \"bot\" username Move app to queue or Kill App . You must enter a queue to move the App to. The Move App and Kill App are mutually exclusive. If you select both, the Kill App takes precedence and Move App ignored. In order for these to be executed the scope must: Have directly caused the rule violation, and Have allocated resources, that is the app is in allocated or running states. Move App is a non-destructive action that shouldn't affect the cluster performance and its availability to the user; however we suggest using it with caution. Kill App is a destructive action. It can affect the cluster performance and its availability to the users. This option is primarily to kill rogue apps that are causing contention of a cluster resources. Use Build Rule to enter an action using JSON. See Expert Rules for examples. " }, 
{ "title" : "Creating AutoActions", 
"url" : "102170-auto-actions-create.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ AutoActions \/ Creating AutoActions", 
"snippet" : "See here for limitations on AutoActions. Select Manage > Auto Actions . The tab displays all defined AutoActions separated into an Active and Inactive list. You can enable\/disable an AutoAction by clicking the check box on the right side of its row. You can edit or delete an AutoAction regardless of...", 
"body" : "See here for limitations on AutoActions. Select Manage > Auto Actions . The tab displays all defined AutoActions separated into an Active and Inactive list. You can enable\/disable an AutoAction by clicking the check box on the right side of its row. You can edit or delete an AutoAction regardless of its status by clicking edit ( ) or delete ( ) on the right side of the row. To create a new AutoAction click either Create from Template , Build Rule , or Expert Rule . Create from Template: Provides a partially completed template designed for the task. The only ruleset options available are relevant to the task being defined. Fields which you need to fill in are highlighted. Build Rule: Provides an empty template, with all the options available. Expert Rule: provides only a text box for defining your AutoAction using JSON. Create an AutoAction. See AutoActions Templates for descriptions and use of the template's sections. Using Create From Template Click Create from Template . All the available templates are displayed. Click the template you wish to use. Resource contention Resource contention in cluster: Monitors overall memory or vCore usage on a cluster and number of pending\/running jobs in order to detect when the cluster is struggling to accommodate all submitted jobs and falls into a resource contention pitfall. Once detected, the specified AutoAction is taken. Resource contention in queue: Monitors overall memory or vCore usage in a queue and number of pending\/running jobs in order to detect a state when the queue is struggling to accommodate all submitted jobs and falls into a resource contention pitfall. Once detected, the specified action is taken. Rogue identification Rogue user: Identifies so-called “rogue” users on a cluster who can potentially affect other users and the cluster as a whole, that is, users submitting jobs that use too much of the cluster resources (memory or vCores). Once the rogue user is detected, the specified action is taken. Rogue app: Identifies so-called “rogue” apps on a cluster which can potentially affect other apps and the cluster as a whole, that is, apps using too much of the cluster resources (memory or vCores). Once the rogue app is detected, the specified action is taken. Rogue Impala query (HDFS Read\/Write) - identifies so-called “rogue” Impala queries on a cluster which can potentially affect other apps and the cluster as a whole, that is, queries using too much of the cluster resources (memory or vCores). Once the rogue app is detected, the specified action is taken. Long-running Jobs Long-running YARN app: Monitors elapsed time of a running YARN app, MapReduce, Spark, Hive, etc., and executes the action if the job runs longer than desired. Long-running Hive query: Tracks Hive jobs by monitoring the total elapsed time of a running Hive query and executes the action if the job runs longer than desired. Long-running workflow: Tracks workflow jobs by monitoring total elapsed time of a running workflow and executes the action if the job runs longer than desired. Long-Impala query: Tracks Impala jobs by monitoring total elapsed time of a running workflow and executes the action if the job runs longer than desired. Complete the template. Note: Changing the predefined ruleset can result in the action not behaving as anticipated. Using Build Rule Click Build Rule and complete the template. Using Expert Rule The AutoActions engine is capable of much more than is available through the templates. Expert Rule is a very powerful mode giving you total access the engine's capabilities. You can create complex rulesets, using JSON, to accommodate almost any cluster monitoring requirements. Before using this mode, you should have clear understanding of AutoActions concepts. See the Expert Rule section for a detailed explanation on how to define an AutoAction. See the Sample AutoActions section for examples of rules and actions. Click Save AutoAction . Your AutoAction is now listed in the Manage > AutoActions tab. " }, 
{ "title" : "Expert rule", 
"url" : "102171-auto-actions-expert.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ AutoActions \/ Expert rule", 
"snippet" : "Overview Expert Rule is a very powerful mode of operation which provides you with greater flexibility than is available through the templates. Using the Expert Rule you can create complex rulesets to accommodate almost any cluster monitoring requirements. Before using this mode, you should have clea...", 
"body" : "Overview Expert Rule is a very powerful mode of operation which provides you with greater flexibility than is available through the templates. Using the Expert Rule you can create complex rulesets to accommodate almost any cluster monitoring requirements. Before using this mode, you should have clear understanding of AutoActions concepts and capabilities, along with JSON, which is used to define the AutoAction. This mode's flexibility and power makes it dangerous and capable of wreaking havoc. Consult with the Unravel team before attempting to use the Expert Rule. Before using the Expert Rule, look at Build Rule to see if you can use that template. It lets you define an expert rule and action while giving you support by defining the header, and providing basic rulesets, options, and actions. See here for the current limitations on AutoActions. In this mode you must specify Prerequisite conditions : boolean conditions that must be met for the Unravel Server to evaluate the AutoAction's defining conditions, for example AutoAction should run from 8.00 and 14.00. Defining conditions : boolean conditions that must be met for the Unravel Server to execute the corresponding action, for example the app can't run more than 50 mapper tasks. Actions : steps to be taken when the prerequisite and defining conditions evaluate to true, for example send an email to admin. When using Expert Rule you must define the Header, Ruleset ( rules ), Options, and Actions ( actions ) using the JSON data format. {\n \/\/ header is required\n 'HEADER' \n\n \/\/ Rules - at least one must be defined. Two or must be joined using an operator.\n \"rules\":[\n { scope } | \"operator\" [ { scope } { scope } ... \n ]\n\n \/\/ Prerequisite Conditions - at least one\n 'OPTIONS - POLICY\/SCOPE'\n \n \/\/ Actions - at least one\n \"actions\":[\n { action } \n ]\n} Header : basic AutoAction information including status (in\/active). Rules : the rules for the scope. You must define at least one rule. Options - Policy\/Scope : who, what, where causes a violation and when. You must specify at least one. Actions : actions to perform when a violation triggers the AutoAction. If none are defined the UI still implements and tracks AutoActions. Defining your AutoAction Header You must define a header. The only item not required is the Description. Attributes Name\/ Definition Possible Value Default Value enabled Whether the AutoAction is active or not. True: active\/enabled. False: inactive\/disabled. True | False - policy_name Value defined by Unravel. AutoActions2 AutoActions2 policy_id Value defined by Unravel. 10 10 instance_id Any unique value. - name_by_user Any unique string. The name is used when the AutoAction is displayed in the UI. - description_by_user Description of the AutoAction. - created_by Value defined by Unravel. admin admin last_edited_by Value defined by Unravel. admin admin created_at Time created. Date and time is in the form of a Epoch\/Unix timestamp. - updated_at Time updated. Date and time is in the form of a Epoch\/Unix timestamp. - \"enabled\": true,\n\"policy_name\": \"AutoActions2\",\n\"policy_id\": 10,\n\"instance_id\": 273132543512,\n\"name_by_user\": \"aa_Sample_Test\",\n\"description_by_user\": \"long running workflow\",\n\"created_by\": \"admin\",\n\"last_edited_by\": \"admin\",\n\"created_at\": 1524220191137,\n\"updated_at\": 1524220265920, Rules: defining conditions Field Name\/ Definition Possible Values Required\/ Required by Default Value scope The rule scope. app, apps, multi_app, by_name, cluster, clusters, multi_cluster, container, containers, multi_ containers queue, queues, multi_queue, user, users, multi_user Note : apps==multi_app, users==multi-user, etc √ - target Application name any valid app name when scope is by_name - metric Metric used for comparison. see supported metrics per type - comparison Comparison operator >, >=, ==, <=, < metric - value Value for comparison. The value form varies by metric. number metric - state Scope state new, new_saving, submitted, accepted, scheduled, allocated, allocated_saving, launched, running, finishing, finished, killed, failed, and * - type Job type mapreduce, yarn, tez, spark, workflow, hive - Logical operators for evaluating multiple rules Operator Condition for a Violation OR At least one rule evaluates to true. AND All rules evaluate to true. SAME All the rules evaluate to true and occur within the same scope. See Same Logical Operator for more details. You must define at least one rule. A Single Rule \"rules\": [\n \/\/ rule\n {\n \"scope\":\"\",\n \/\/ at least one of the following\n \/\/metric\n \"metric\":\"\",\n \"compare\":\"\",\n \"value\":,\n \"state\":\"\",\n \"type\":\"\"\n }\n] Violation occurs when the app is a pending workflow with a duration > 10. \"rules\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"duration\",\n \"compare\":\">\",\n \"value\":10,\n \"state\":\"pending\",\n \"type\":\"workflow\"\n }\n] Violation occurs when the app is a workflow with a duration > 10. Removing state doesn't affect the rule. \"rules\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"duration\",\n \"compare\":\">\",\n \"value\":10,\n \"state\":\"\",\n \"type\":\"workflow\"\n }\n] Violation occurs when the app has a duration > 10. Removing state and type doesn' affect the rule. \"rules\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"duration\",\n \"compare\":\">\",\n \"value\":10,\n \"state\":\"\",\n \"type\":\"\"\n }\n] A Rule Array Two or more rules combined with an operator. \"rules\": [\n {\n \"operator\": [\n \/\/ rule 1\n {\n\n }\n \/\/ rule 2\n {\n }\n \/\/ rule n\n {\n }\n ]\n }\n]\n\n Note Multi_X is equivalent to the plural of X. In this case we could use multi_apps instead of apps. Take the following two rules: \/\/ apps (allocatedMB >=1024)\n{\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n}\n\n\/\/ apps (allocatedVCores > 100)\n{\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n}\n OR example When they are ORed a violation occurs if at least one rule evaluates to true. \"rules\":[\n {\n \"OR\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n } \n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n }\n ]\n }\n] AND example When ANDed a violation occurs if both rules evaluate to true. \"rules\":[\n {\n \"AND\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n } \n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n }\n ]\n }\n] SAME example When SAMEd a violation occurs if both rules evaluate to true and the violations are within the same scope. \"rules\":[\n {\n \"SAME\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n } \n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n }\n ]\n }\n] Using the above example, if My_App only violates rule 1 (allocatedMB), and Your_App only violates rule 2 (allocatedVcores) the AutoAction isn't triggered because the violations occurred in different scopes, i.e., My_App and Your_App. However, if My_App violates both rules (allocatedMB and allocatedVcores), and Your_App only violates rule 2 (allocatedVcores) the AutoAction is triggered for My_App but not Your_App. Given the same ruleset, evaluation becomes more restrictive. OR : the AutoAction is triggered if one or more of conditions is true. AND : the AutoAction is triggered if all the conditions are true. SAME : the AutoAction is triggered if all the conditions are true within a specific scope. Options - policy\/scope: prerequisite conditions Who\/what can cause the violation and when. You must define at least one option - policy\/scope. Field Name\/Definition Required\/ Required by Possible Values Default Value X _mode where X is user, queue, cluster, or app. The mode defines how the rules are applied to type X : 0 - the rules aren't evaluated. 1 - the rules are evaluated for all type X . 2 - the rules are evaluated for only those in X _list or matching  X _transform. 3 - the rules are evaluated for everything but the those in X _list or matching  X _transform. You must define at least one option\/policy. 0, 1, 2, 3 Default: 0 0 X _list A list of X type used when the mode value is 2 or 3. Applicable Only if mode is set to 2 (only) or 3 (except). if_mode is 2 or 3 and X _transform isn't defined empty, single item or comma separated list. - X _transform A list of regex used to generate a list of X when the mode value is 2 or 3. Applicable Only if mode is set to 2 (only) or 3 (except). if X _mode is 2 or 3 and X _list isn't defined empty, single regex or comma separated regex list - Time The daily time the AutoAction is trigger. any time period spanning less than 24 hours. - Sustained Violation Set a minimum or maximum time period for the AutoAction to be triggered. See here for more information any time period less than 24 hours. - Options - policy\/scope rule where X is user, queue, cluster, or app. \"X_mode\": \"\",\n\n\/\/ at least one of the following if X_mode = 2|3\n\"X_list\": \"\" ,\n\"X_mode\": \"\" , Cluster - doesn't apply to any clusters. \"cluster_mode\": 0,\n\"cluster_list\":\"\",\n\"cluster_transform\":\"\", Queue - applies all queues. \"queue_mode\": 1,\n\"queue_list\":\"\",\n\"queue_transform\":\"\", User - applies only the users specified in the list. \"user_mode\": 2,\n\"user_list\": [userA, userB],\n\"user_transform\":\"\", Application Name - applies to all apps except those matching the list. \"app_mode\": 3,\n\"app_list\": [userA, userB],\n\"app_transform\":\"\", User - applies only to the users specified in the list and regex. \"user_mode\": 2,\n\"user_list\": [userA, userB],\n\"user_transform\":\"regex\", Actions: action to implement upon violation You do not have to define any actions, but it defeats the purpose not to. If no actions are defined, the UI keeps track of when the AutoAction was triggered and what triggered it. Both the prerequisite and defining conditions must be met before the AutoAction is triggered. Field Name\/Definition Required\/ Required by Possible Value Default Value action The action to be taken. at least one send_email, http_post, post_in_slack, move_to_queue, kill_app - to Email recipients. send_mail if to_owner not true One or more recipients in a comma separated list. - to_owner Send email to owner. send_mail if to is empty false: do not send email true: send email false urls URLs for Http post http_post One or more URLs in a comma separated list. - token Token generated by slack. post_in_slack Slack token - channels Slack channel. post_in_slack One or more channels in a comma separated list - queue Queue name. move_to_queue The name of a valid queue to move the app to. - Single action \"actions\": [\n {\n \"action\": \"\"\n \/\/ if required action options\n }\n] Multiple actions \"actions\": [\n \/\/ action 1 \n {\n }\n\n \/\/ action n\n {\n }\n] Actions can be Ignored when in conflict Below we specified two actions, move_to_queue and kill_app , correctly; but in conjunction they don't make sense. If we kill the app how can we then move it? Why bother moving the app if we are going to kill it? Effectively only one action can be executed. In this case Unravel gives precedence to kill_app and move_to_queue is ignored. actions\":[\n {\n {\n \"action\":\"move_to_queue\",\n \"queue\":\"sample\"\n }, \n \"action\":\"kill_app\"\n }\n }\n] Actions fail if the required information is invalid or not specified. Below are two actions with invalid information. In send_mail the addresses are invalid and the owner isn't to be notified. The http_post has an invalid URL. Unravel AutoAction's engine sends the email and tries to perform the HTTP post; however both actions will fail. Since only these two actions are specified, no action is effectively taken. However, the UI stills records the trigger and retains the information to populate for Operations > Dashboard AutoActions tile , history of runs and the cluster view for each time the action was triggered. (See AutoActions Overview .) \"actions\":[\n {\n \"action\":\"send_email\",\n \"to\": [aBadEmailAddress.mycompany.com,anotherBadAddress.mycompany.com\n ],\n \"to_owner\":false\n },\n {\n \"action\":\"http_post\",\n \"urls\":[https:\/\/nonexistentURL\n ]\n }\n] Example actions There are five main actions: send_email , http_post , post_in_slack , move_to_queue , and kill_app . send_email, http_post, and post_in_slack allow you to use comma separated lists to specify multiple recipients, Below are two actions with invalid information. In send_mail the addresses are invalid and the owner isn't to be notified. The http_post has an invalid URL. Unravel AutoAction's engine sends the email and tries to perform the HTTP post; however both actions will fail. Since only these two actions are specified, no action is effectively taken. However, the UI stills records the trigger and retains the information to populate for Operations > Dashboard AutoActions tile , history of runs and the cluster view for each time the action was triggered. (See AutoActions Overview .) URLs and channels respectively. See AutoActions Templates for example of send_email , http_post and post_in_slack notifications. Note You must take care when entering information. A specified action fails if you enter the incorrect information, for example bad email address\/URL\/channel, wrong or non-existent queue. Send_email Unlike when using Create from Template or Build Rule , the UI won't notify you when entering an invalid address on the face of it, for example myMailaddress&com. You can automatically send a email to the owner by setting to_owner to true; Unravel handles the rest. You can enter multiple email addresses using a comma separated list. \"actions\":[\n {\n \"action\":\"send_email\",\n \"to\": [\"myMail@mycompany.com,ThisPerson@theircompany.com,TheBoss@mycompany.com\"\n ],\n \"to_owner\":false\n }\n] http_post Just like send_email you won't be notified if your HTTPS is invalid on the face of it, for example userAtTheCompany. You can enter multiple URLs using a comma separated list. \"actions\": [\n {\n \"action\": \"http_post,\n \"to\": [\"https:\/\/test24:3000\/post\/\"\n ]\n }\n] post_in_slack Verify that your token is correct, and the channels are entered correctly. You can enter multiple channels using a comma separated list. \"actions\": [\n {\n \"action\": \"post_in_slack\",\n \"token\": \"xyz\",\n \"channels\": [ \"auto-action-2\"\n ]\n }\n] move_to_queue Be sure to enter an existing and correct queue. This is non-destructive but none-the-less may affect the cluster performance and its availability to the users. \"actions\": [\n {\n \"action\": \"move_to_queue\",\n \"queue\": \"sample\"\n }\n] kill_app This is straight forward, but kill_app is a destructive action and may affect the cluster performance and its availability to the users. \"actions\": [\n {\n \"action\": \"kill_app\"\n }\n] An expert rule example This AutoAction triggers on apps using (memoryMB >= 1024), has (allocatedVcores >100), and which occur within the same scope, except for the apps, myApp, yourApp, and theirApp. Upon triggering a notification is posted to a Slack channel and the app is moved to the slow_queue. {\n \/\/ Header\n \"enabled\":true,\n \"policy_name\":\"AutoActions2\",\n \"policy_id\":10,\n \"instance_id\":273132543512,\n \"name_by_user\":\"aa_Sample_Test\",\n \"description_by_user\":\"long running workflow\",\n \"created_by\":\"admin\",\n \"last_edited_by\":\"admin\",\n \"created_at\":1524220191137,\n \"updated_at\":1524220265920,\n\n \/\/ Defining Conditions \n \"rules\":[\n {\n \"SAME\":[\n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedMB\",\n \"compare\":\">=\",\n \"value\":1024\n } \n {\n \"scope\":\"apps\",\n \"metric\":\"allocatedVCores\",\n \"compare\":\">\",\n \"value\":100\n }\n ]\n }\n ] \n \/\/ Prerequisite Conditions\n \"app_mode\":3,\n \"app_list\":\"myApp, yourApp, theirApp\",\n\n \/\/ Actions\n \"actions\":[\n {\n \"action\":\"post_in_slack\",\n \"token\":\"xyz\",\n \"channels\":[\n \"auto-action-2\"\n ]\n },\n {\n \"action\":\"move_to_queue\",\n \"queue\":\"slow_queue\"\n }\n ]\n} AutoAction examples See Sample AutoActions for more Expert Rule examples. Examples include defining an expert rule or action within a Build Rule template to provide some predefined structure. " }, 
{ "title" : "Same logical operator", 
"url" : "102172-auto-actions-same.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ AutoActions \/ Same logical operator", 
"snippet" : "SAME : logically and 's rules plus adding the further constraint that the rules must be violated within same scope in order to trigger an AutoAction. Example - a rule designed to alert on rogue users. Human-readable form If any user is running more than ten jobs on a cluster and the same user has mo...", 
"body" : "SAME : logically and 's rules plus adding the further constraint that the rules must be violated within same scope in order to trigger an AutoAction. Example - a rule designed to alert on rogue users. Human-readable form If any user is running more than ten jobs on a cluster and the same user has more than five jobs pending then report the user as rogue. More formally (any user has > 10 running apps) SAME (any user has > 5 pending jobs) JSON definition “rules”:[\n “SAME”:[\n {\n “scope”:”users”,\n “metric”:”appCount”,\n “operator”:”>”,\n “value”:10,\n state”:”running”\n },\n {\n “scope”:”users”,\n “metric”:”appCount”,\n “operator”:”>”,\n “value”:5,\n “state”:”pending”\n }\n ]\n] Implementation Internally the back-end uses a clustering technique to implement the SAME operator. AutoActions runs all metric aggregations simultaneously. When the metrics are received and aggregated it then evaluates all rules and expressions. It starts at the evaluation tree's leaf expressions and works its way up to the root expression. Assume the above rule, three users (A, B, and C), and the following conditions user A has 12 running and three pending apps user B has seven running and one pending apps user C has 21 running and 11 pending apps First, the two (2) simple rules are evaluated: does user have more than 10 apps running? User A has 12 → TRUE User B has seven → FALSE User C has 21 → TRUE does user have more than 5 apps pending? User A has three → FALSE User B has one → FALSE User C has 11 → TRUE Second, it applies clustering by scope and for each cluster it counts the number rules triggered. In the back-end code this procedure is called “linking” of rules (see Ruleset.java). Cluster “User A”, link count = 1. User A > 10 running apps? → TRUE User A > five pending apps? → FALSE Cluster “User B”, link count = 0. User B > 10 running apps? → FALSE User B > five pending apps? → FALSE Cluster “User C”, link count = 2. User C > 10 running apps? → TRUE User C > five pending apps? → TRUE Third, all groups with less than the needed number of links (2 in this case) are discarded. If some of the rules were triggered, that rule is reset for the group. Cluster “User A” has a link count = 1 so it's reset and discarded. User A > 10 running apps? → TRUE reset to FALSE User A > 5 pending apps? → FALSE Cluster “User B”, link count = 0 so it's discarded. User B > 10 running apps? → FALSE User B > 5 pending apps? → FALSE Finally, only the users that have triggered all rules remain. Cluster “User C”, link count = 2: User C > 10 running apps? → TRUE User C > 5 pending apps? → TRUE User C meets the criteria for the Rogue User AutoAction, therefore User C triggers the AutoAction and the alert is sent and\/or the actions performed. Comparison to AND Both User A and User C would have triggered the above rule were AND used instead of SAME , that is, ( any user has > 10 running apps) AND ( any user has > 5 pending jobs). To achieve the same result as the above example using AND instead of SAME , you would need to create the following AutoAction rule for each and every user on the cluster: ( Username has > 10 running apps) AND ( Username has > 5 pending apps) " }, 
{ "title" : "Snooze feature", 
"url" : "102173-auto-actions-snooze.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ AutoActions \/ Snooze feature", 
"snippet" : "The snooze function prevents automatic actions from being repeated during a specified period, if and only if, it is the same violation context and the action adds no further information to the violation, i.e., essentially noise. For example, alerts of user A violating rule Y are snoozed as the alert...", 
"body" : "The snooze function prevents automatic actions from being repeated during a specified period, if and only if, it is the same violation context and the action adds no further information to the violation, i.e., essentially noise. For example, alerts of user A violating rule Y are snoozed as the alert adds no new context the user A's violation. App is used in this example, but the AutoAction can be defined for user, queues, etc. An AutoAction specifying a Kill App or Move App action can not be snoozed. Snooze is set the first time the app violates the rule. The AutoAction itself continues to run uninterrupted whether zero or all apps currently covered by the AutoAction are snoozed. The AutoAction takes action for any app not snoozing at the time of violation. If an app is still violating upon awaking, the specified actions are performed and the app is snoozed once again. See here for information on AutoAction's Limitations Example Rule\/Action : if app uses memory > 1 GB send email Two apps : A & B Snooze time : 30 minutes at 20:00 A > 1 GB → email is sent + snooze set (snoozed until 20:30). B < 1 GB → app is not violating so nothing is done. at 20:10 A > 1 GB → snoozing , no action is taken. B > 1 GB → email is sent + snooze set (snoozed until 20:40). at 20:20 A > 1 GB → snoozing , no action is taken. B > 1 GB → snoozing , no action is taken. at 20:30 A > 1G B → app wakes and is still in violation. An email is sent + snooze set (runs until 21.00). >B > 1 GB → snoozing , no action is taken. at 20:40 A > 1 GB → snoozing, no action is taken. >B < 1 GB → app wakes . app not violating so nothing is done. To change the snooze time On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties . # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Search for com.unraveldata.auto.action.default.snooze.period.ms . Set the parameter to the new value and save the file. Here it is set to 2 hours. com.unraveldata.auto.action.default.snooze.period.ms=7200000 Restart the JCS2 daemon. # service unravel_jcs2 restart " }, 
{ "title" : "AutoActions examples", 
"url" : "102174-auto-actions-examples.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ AutoActions \/ AutoActions examples", 
"snippet" : "Limitations on AutoActions can be found here . Information on demos can be found here . Sample JSON rules Unless otherwise noted, all JSON rules are entered into the Rule Box in the Expert Mode template. Alert examples Alert if Hive query duration > 10 minutes. { \"scope\": \"multi_app\", \"user_metric\":...", 
"body" : "Limitations on AutoActions can be found here . Information on demos can be found here . Sample JSON rules Unless otherwise noted, all JSON rules are entered into the Rule Box in the Expert Mode template. Alert examples Alert if Hive query duration > 10 minutes. {\n \"scope\": \"multi_app\",\n \"user_metric\": \"duration\",\n \"type\": \"HIVE\",\n \"state\": \"RUNNING\",\n \"compare\": \">\",\n \"value\": 600000\n} Alert if Tez query duration > 10 minutes. {\n \"scope\": \"multi_app\",\n \"user_metric\": \"duration\",\n \"type\": \"TEZ\",\n \"state\": \"RUNNING\",\n \"compare\": \">\",\n \"value\": 600000\n} Alert if any workflow's duration > 20 minutes. {\n \"scope\": \"multi_app\",\n \"type\": \"WORKFLOW\",\n \"state\": \"RUNNING\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 1200000\n} Alert if workflow named “foo” and duration > 10 minutes. {\n \"scope\":\"by_name\",\n \"target\":\"foo\",\n \"type\":\"WORKFLOW\",\n \"state\":\"RUNNING\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":600000\n} Alert if workflow named “foo” and totalDfsBytesRead > 100 MB and duration > 20 minutes. {\n \"AND\":[\n {\n \"scope\":\"by_name\",\n \"target\":\"foo\",\n \"type\":\"WORKFLOW\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":1200000\n },\n {\n \"scope\":\"by_name\",\n \"target\":\"foo\",\n \"type\":\"WORKFLOW\",\n \"user_metric\":\"totalDfsBytesRead\",\n \"compare\":\">\",\n \"value\":104857600\n }\n ]\n}\n Alert if Hive query in Queue “foo” and duration > 10 minutes. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"state\": \"RUNNING\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 600000\n}\n And select global rule condition Queue only “foo”: Kill App Example When workflow name is “prod_ml_model” and duration > 2h then kill jobs with allocated_vcores >= 20 and queue != ‘sla_queue’ In Rule Box enter: {\n \"scope\": \"by_name\",\n \"target\": \"prod_ml_model\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 7200000\n} In Action Box enter: {\n \"action\": \"kill_app\",\n \"max_vcores\": 20,\n \"not_in_queues\": [\"sla_queue\"],\n \"if_triggered\": false\n} AutoActions rules, predefined templates vs expert mode AutoActions demo package documentation is here . Predefined templates cover a variety of jobs, yet they can lack the specificity or complexity you need for monitoring. For instance, you can use the Rogue Application template to determine if jobs are using too much memory or vCore resources by alerting for jobs using more than 1 TB of memory. However, if you only want to know if only Map Reduce jobs are using > 1 TB, the template won't suffice. For such instances, you need to write your AutoActions using the Expert Mode template with the rules and some actions written in JSON. Below are a variety of AutoActions written using JSON. MapReduce Alert on MapReduce jobs using > 1 TB of memory. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n}\n Alert on MapReduce jobs using > 1000 vCores. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n}\n Alert on MapReduce jobs running more than 1 hour. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"elapsed_time\",\n \"compare\": \">\",\n \"value\": 3600000\n} Alert on MapReduce jobs that may affect any production SLA jobs running on a cluster. Check for MapReduce jobs not in the SLA queue, running between 12 am and 3 am, and using > 1 TB of memory. Use the JSON rule specifying Map Reduce jobs using > 1 TB and set the rule conditions as shown. Alert on ad hoc MapReduce jobs use a majority of cluster resources which may impact the cluster performance. Check for MapReduce Jobs in the “root.adhocd” queue, running between 1 am and 5 am, and using > 1 TB of memory. Use the JSON rule specifying Map Reduce jobs using > 1 TB and set the rule conditions as shown. Spark The JSON rules to alert if a Spark app is grabbing majority of cluster resources are exactly like the Map Reduce rules for except SPARK is used for the \"type\". Alert on only Spark jobs using > 1 TB of memory. {\n \"scope\": \"multi_app\",\n \"type\": \"SPARK\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n}\n Alert on only Spark jobs using > 1000 vCores. {\n \"scope\": \"multi_app\",\n \"type\": \"MAPREDUCE\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n}\n Alert if a Spark SQL query has unbalanced input vs output, which may indicate inefficient or “rogue” queries. Check if any Spark app is generating lots of rows in comparison with input. In this example, ‘outputToInputRowRatio’ > 1000 {\n \"scope\": \"multi_app\",\n \"type\": \"SPARK\",\n \"user_metric\": \"outputToInputRowRatio\",\n \"compare\": \">\",\n \"value\": 1000\n}\n Alert if a Spark SQL has lots of output partitions. Check if any Spark app ‘outputPartitions’ > 10000. {\n \"scope\": \"multi_app\",\n \"type\": \"SPARK\",\n \"user_metric\": \"outputPartitions\",\n \"compare\": \">\",\n \"value\": 10000\n}\n Hive Alert if a Hive query duration is running longer than expected. Check if a Hive query duration > 5 hours. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n}\n Alert if SLA bound query is taking longer than expected. Check if a Hive query started between 1 am and 3 am in queue ‘prod’ runs longer than > 20 minutes. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 1200000\n}\n Set the rule conditions as shown. Check if any Hive query is started between 1 am and 3 am in any queue except ‘prod’. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"metric\": \"app_count\",\n \"compare\": \">\",\n \"value\": 0\n}\n Set the rule conditions as shown. Alert if a Hive query has extensive I\/O, which may affect HDFS and other apps. Check if a Hive query writes out more than 100 GB in total. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"totalDfsBytesWritten\",\n \"compare\": \">\",\n \"value\": 107374182400\n}\n Check if a Hive query reads in more than 100 GB in total. {\n \"scope\": \"multi_app\",\n \"type\": \"HIVE\",\n \"user_metric\": \"totalDfsBytesRead\",\n \"compare\": \">\",\n \"value\": 107374182400\n}\n Detect inefficient and “stuck” Hive queries, that is, alert if a Hive query has not read lots of data but running for a longer time. Check if any Hive query has read less than 10GB in total and its duration is longer than 1 hour. {\n \"SAME\":[\n {\n \"scope\":\"multi_app\",\n \"type\":\"HIVE\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":3600000\n },\n {\n \"scope\":\"multi_app\",\n \"type\":\"HIVE\",\n \"user_metric\":\"totalDfsBytesRead\",\n \"compare\":\"<\",\n \"value\":10485760\n }\n ]\n}\n Tez Alert if a Tez query duration is running longer than expected. Check if a Tez query duration > 5 hours. {\n \"scope\": \"multi_app\",\n \"type\": \"TEZ\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n}\n Alert if SLA bound query is taking longer than expected. Check if a Tez query started between 1 am and 3 am in queue ‘prod’ runs longer than > 20 minutes. {\n \"scope\": \"multi_app\",\n \"type\": \"TEZ\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 1200000\n}\n Set the rule conditions as shown. Check if any Tez query is started between 1 am and 3 am in any queue except ‘prod’. {\n \"scope\": \"multi_app\",\n \"type\": \"TEZ\",\n \"metric\": \"app_count\",\n \"compare\": \">\",\n \"value\": 0\n}\n Set the rule conditions as shown. Alert if a Tez query has extensive I\/O, which may affect HDFS and other apps. Check if a Tez query writes out more than 100 GB in total. {\n \"scope\": \"multi_app\",\n \"type\": \"TEZ\",\n \"user_metric\": \"totalDfsBytesWritten\",\n \"compare\": \">\",\n \"value\": 107374182400\n}\n Check if a Tez query reads in more than 100 GB in total. {\n \"scope\": \"multi_app\",\n \"type\": \"TEZ\",\n \"user_metric\": \"totalDfsBytesRead\",\n \"compare\": \">\",\n \"value\": 107374182400\n}\n Detect inefficient and “stuck” Tez queries. For example, alert if a Tez query has not read lots of data but running for a longer time. Check if any Tez query has read less than 10 GB in total and its duration is longer than 1 hour. {\n \"SAME\":[\n {\n \"scope\":\"multi_app\",\n \"type\":\"TEZ\",\n \"user_metric\":\"duration\",\n \"compare\":\">\",\n \"value\":3600000\n },\n {\n \"scope\":\"multi_app\",\n \"type\":\"TEZ\",\n \"user_metric\":\"totalDfsBytesRead\",\n \"compare\":\"<\",\n \"value\":10485760\n }\n ]\n}\n Workflow Alert if a workflow is taking longer than expected. Check if any workflow is running for longer than 5 hours. {\n \"scope\": \"multi_app\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n} Check if a SLA bound workflow named ‘market_report’ is running for longer than 30 minutes. {\n \"scope\": \"multi_app\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 18000000\n} Alert if a SLA bound workflow is reading more data than expected. Check if workflow named '‘market_report’' and 'totalDfsBytesRead' > 100 GB. {\n \"scope\": \"by_name\",\n \"target\": \"market_report\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"totalDfsBytesRead\",\n \"compare\": \">\",\n \"value\": 107374182400\n} Alert if a SLA bound workflow is taking longer and kill bigger apps which are not run by the SLA user. Check if Workflow named ‘prod_ml_model’ and duration > 2h then kill jobs with allocated_vcores >= 20 and user != ‘sla_user'. {\n \"scope\": \"by_name\",\n \"target\": \"prod_ml_model\",\n \"type\": \"WORKFLOW\",\n \"user_metric\": \"duration\",\n \"compare\": \">\",\n \"value\": 7200000\n} Enter the following code in the Export Mode template's Action box. {\n \"action\": \"kill_app\",\n \"max_vcores\": 20,\n \"not_in_queues\": [\"sla_queue\"],\n \"if_triggered\": false\n} USER User Alert for Rogue User - Any user consuming a major portion of cluster resources. Check for any user where the allocated vCores aggregated over all their apps is > 1000. You can use the Rouge User template, or the JSON rule. {\n \"scope\": \"multi_user\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n} Check for any user where the allocated memory aggregated over all their apps is > 1 TB. You can use the Rouge User template or the JSON rule. {\n \"scope\": \"multi_user\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n} Queue Alert for rogue queue - any queue consuming a major portion of cluster resources. Check for any queue where the allocated vCores aggregated over all its apps for any queue > 1000. {\n \"scope\": \"multi_queue\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n} Check for any queue where the allocated memory aggregated over all its apps is > 1 TB. {\n \"scope\": \"multi_queue\",\n \"metric\": \"allocated_mb\",\n \"compare\": \">\",\n \"value\": 1073741824\n} Applications While apps in quarantine queue continue to run, the queue is preemptable and has a low resource allocation. If any other queue needs resources, it can preempt apps in the quarantine queue. Moving rogue apps to quarantine queue frees resources for other apps. Below we are alerting on vCores; to alert on memory just substitute memory for vCores in the following rules. Alert for rogue app If any app (not SLA bound) is consuming more than certain vCores at midnight, move it to a quarantine queue. You can use the Rogue Application template to specify vCores. Or the Expert Mode template and set JSON rule for vCores as {\n \"scope\": \"multi_app\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": 1000\n} Set Time rule condition as: Set Move app rule as: Any app needing greater than X amount of resources has to be approved, otherwise the app is moved to quarantine queue. You can use the Rogue Application template to specify vCores. Or use the Expert Mode template and set JSON rule as for vCores {\n \"scope\": \"multi_app\",\n \"metric\": \"allocated_vcores\",\n \"compare\": \">\",\n \"value\": [X]\n} Set Queue rule conditions. Set Move app queue action. Related articles Running AutoAction Demos " }, 
{ "title" : "Running AutoAction demos", 
"url" : "102175-auto-actions-demos.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ AutoActions \/ Running AutoAction demos", 
"snippet" : "The Demos program provides you a way to understand and experiment with AutoActions and their triggering. Example AutoActions are included for Map Reduce, Hive, Spark and Workflow jobs. Scripts are provided that trigger a violation in order to demonstrate the Actions in \"action\". Contact Unravel Supp...", 
"body" : "The Demos program provides you a way to understand and experiment with AutoActions and their triggering. Example AutoActions are included for Map Reduce, Hive, Spark and Workflow jobs. Scripts are provided that trigger a violation in order to demonstrate the Actions in \"action\". Contact Unravel Support to in order to download AutoActions demos . Text indicates where you must substitute your particular values for the text. demo-path is the complete path of the auto-actions-demos directory. Unpack and install the AutoAction demos Put the auto-actions-demos.tgz file in the directory Unravel Server host machine where you want to unpack it. Navigate to the directory and unpack the demos. # tar -xvzf auto-actions-demo.tgz Tar creates and unpacks the files into auto-actions-demos directory. The directory should contain the following files. # ls auto-actions-demos\ndemos\/ setup\/ Go to demo-path \/ setup directory. Open .\/settings file and enter the email address to receive violation notifications. Execute the .\/setup-all script. # .\/setup-all The AutoAction rules that include time specification are automatically adjusted to the current time period, for example, from CURRENT_HOUR:00 to CURRENT_HOUR+2:00. After running the script go the Unravel Server UI and select Manage | Auto Actions tab. You should see all the AutoAction demos listed under Active Auto Actions. Each AutoAction is entitled AA-tag , for example, AA-Spark-1c , Map-1b . Executing the demos Go to demo-path \/ demos directory. For each AutoActions rules listed in Manage | Auto Actions there is a corresponding script in the demo's directory. Each script triggers the corresponding AutoAction demo. For example, in the UI the AutoAction named AA-Spark-1c is listed. The corresponding triggering script file is demo-Spark-1c . You should see the following files in the demos directory. # cd demo-path \/setup \n# ls\ndemo-Hive-1a demo-MR-1a demo-MR-2b demo-Spark-1b\ndemo-Hive-2a demo-MR-1b demo-MR-3a demo-Spark-1c\ndemo-Hive-2b demo-MR-1c demo-MR-3b run-all-demos\ndemo-Hive-3a demo-MR-2a demo-Spark-1a scripts\/ Execute .\/demo-tag script to trigger the corresponding AA-tag rule. Each script is designed to simulate violation conditions for the corresponding AutoAction on the target Hadoop cluster, for example, to trigger AA-Spark-1c you run the demo-Spark-1c script. Some AutoAction's demo scripts trigger multiple AutoActions. This side effect can happen when running your own defined AutoActions due to AutoActions having overlapping definitions. Cleaning up demos Run .\/clean-all script remove all the demo AutoAction from the Unravel Server. If you want to run the demos again, simply follow the procedure starting with extracting the files ( step 3 above)to # cd demo-path \/setup \n# .\/clean-all . AutoActions demos list Application and Alert Type Use case Auto Action Triggering Script [empty] Notes MapReduce Alert if a MapReduce job is grabbing majority of cluster resources and may affect other users jobs at any time. Alert if any MapReduce job allocated memory > 20GB. AA-MR-1a Demo-MR-1a Submits to “root.sla” queue. Alert if any MapReduce job allocated vCores > 10. AA-MR-1b Demo-MR-1b Submits to “root.sla” queue. Alert if any MapReduce job is running for longer than 10 minutes. AA-MR-1c Demo-MR-1c Submits to “root.sla” queue. May trigger MR-1b. MapReduce Alert if a MapReduce job may affect any production SLA jobs running on a cluster. Alert if any app is not in the queue ‘sla_queue’ and running between X and Y and allocated memory > 20GB. AA-MR-2a Demo-MR-2a Also triggers MR-1a as well. Alert if any app is not in the queue ‘sla_queue’ and running between X and Y and allocated vCores greater than 10. AA-MR-2b Demo-MR-2b Also triggers MR-2a as well. MapReduce Alert if an ad hoc MapReduce job is grabbing majority of cluster resources and may affect cluster performance. Alert if any MapReduce job allocated vCores > 10 between X and Y in queue ‘root.adhoc’. AA-MR-3a Demo-MR-3a Submits to “root.adhoc” queue. Also triggers MR-1a and MR-2a. Alert if any MapReduce job allocated memory > 20GB between X and Y in queue ‘root.adhoc’. AA-MR-3b Demo-MR-3b Submits to “root.adhoc” queue. Also triggers MR-1b and MR-2b. Spark Alert if a Spark app is grabbing majority of cluster resources and may affect other users jobs at any time. Alert if any Spark app has allocated more than 20GB of memory. AA-Spark-1a Demo-Spark-1a Alert if any Spark app allocated vCores > 8. AA-Spark-1b Demo-Spark-1b Alert if any Spark app is running longer than 10 minutes AA-Spark-1c Demo-Spark-1c Spark Alert if a Spark SQL query has unbalanced input vs output, which may point to an inefficient or “rogue” queries. Alert if any Spark app is generating lots of rows in comparison with input,i.e. ‘outputToInputRowRatio’ > 1000. TBD Hive Alert if a Hive query duration is running longer than expected. Alert if a Hive query duration > 5 minutes. AA-Hive-1a Demo-Hive-1a You can Ctrl-C the query once it triggers the AA. Hive Alert if SLA bound query is taking longer than expected. Alert if a Hive query started between A:00 and B:00 in queue ‘root.prod’ and duration > 10 minutes. AA-Hive-2a Demo-Hive-2a You can Ctrl-C the query once it triggers the AA. Alert if any Hive query is started between A:00 and B:00 in any queue except ‘root.prod’. AA-Hive-2b Demo-Hive-2b Very short query. Hive Alert if a Hive query is writing lots of data. Alert if a Hive query writes out more than 200MB in total. AA-Hive-3a Demo-Hive-3a Alert if a Hive query reads in more than 10GB in total. AA-Hive-3b Demo-Hive-3b Hive Detect inefficient and “stuck” Hive queries. Alert if any Hive query has read less than 10MB in total and its duration is longer than 10 minutes. AA-Hive-4a Demo-Hive-4a Alert if any Hive query in the queue 'root.adhoc' is running for longer than 2 minutes. AA-Hive-4b Demo-Hive-4b Workflow Alert if a workflow is taking longer than expected. Alert if any workflow is running for longer than 10 minutes, might be stuck. AA-WF-1a Demo-WF-1a You can Ctrl-C the query once it triggers the AA. Alert if a SLA bound workflow named ‘market_report’ is running for longer than 5 minutes. AA-WF-1b Demo-WF-1b You can Ctrl-C the query once it triggers the AA. Workflow Alert if a workflow is reading more data than expected. " }, 
{ "title" : "Supported cluster metrics", 
"url" : "102176-auto-actions-metrics.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ AutoActions \/ Supported cluster metrics", 
"snippet" : "AutoActions by default collect metrics from the YARN Resource Manager and MapReduce Application Master for all apps running (or submitted) on the target cluster. MapReduce Application Master (AM) also maintains various counters. Users can use these metrics and counters when defining an AutoActions r...", 
"body" : "AutoActions by default collect metrics from the YARN Resource Manager and MapReduce Application Master for all apps running (or submitted) on the target cluster. MapReduce Application Master (AM) also maintains various counters. Users can use these metrics and counters when defining an AutoActions rule. Additionally, there are Hive\/Workflow and Spark metrics which can used to define rules. Monitoring is performed on most live running apps allowing users to take proactive actions when violations are detected. See here for limitations on AutoActions. Monitoring is only performed on MapReduce AM metrics when the user defines an AutoAction rule that requires the polling\/aggregation of a MapReduce AM metric. Hive\/Workflow metrics Metric Definition duration Total time taken by the app. totalDfsBytesRead Total HDFS bytes read. totalDfsBytesWritten Total HDFS bytes written. MapReduce application master and MapReduce metrics Type Metric Definition elapsedAppTime Time since the app was started. Map mapsCompleted Number of completed maps. mapsPending Number of maps still to be run. mapsRunning Number of running maps. mapsTotal Total number of maps. Map Attempts failedMapAttempts Number of failed map attempts. killedMapAttempts Number of killed map attempts. newMapAttempts Number of new map attempts. runningMapAttempts Number of running map attempts. Reduce reducesCompleted Number of completed reduces. reducesPending Number of reduces still to be run. reducesRunning Number of running reduces. reducesTotal Total number of reduces. Reduce Attempts failedReduceAttempts Number of failed reduce attempts. killedReduceAttempts Number of killed reduce attempts. newReduceAttempts Number of new reduce attempts. runningReduceAttempts Number of running reduce attempts. successfulReduceAttempts Number of successful reduce attempts. For more details see: Apache MapReduce REST Jobs API MapReduce and file system counters Metric Definitions fileBytesRead Amount of data read from local file system. fileBytesWritten Amount of data written to local file system. fileReadOps Number of read operations from local file system. fileLargeReadOps Number of read operations of large files from local file system. fileWriteOps Number of write operations from local file system. hdfsBytesRead Amount of data read from HDFS. hdfsBytesWritten Amount of data written to HDFS. hdfsReadOps Number of read operations from HDFS. hdfsLargeReadOps Number of read operations of large files from HDFS. hdfsWriteOps Number of write operations to HDFS. Job counters Type Metric Definition Map dataLocalMaps Number of map tasks which were launched on the nodes containing required data. mbMillisMaps Total megabyte-seconds taken by all map tasks. millisMaps Total time spent by all map tasks. slotsMillisMaps Total time spent by all executing maps in occupied slots. vcoresMillisMaps Total vCore-seconds taken by all map tasks. Reduce mbMillisReduces Total megabyte-seconds taken by all reduce tasks. millisReduces Total time spent by all reduce tasks. slotsMillisReduces Total time spent by all executing reduces in occupied slots. totalLaunchedReduces Total number of launched reduce tasks. vcoresMillisReduces Total vCore-seconds taken by all reduce tasks. File input\/output format counters Metric Definition bytesRead Amount of data read by every task for every file system. bytesWritten Amount of data written by every task for every file system. For more details see: Apache MapReduce REST Jobs Counters API MapReduce framework counters Type Metric Definition Map failedShuffle Total number of mappers which failed to undergo through shuffle phase. mapInputRecords Total number of records processed by all the mappers. mapOutputBytes Total amount of (uncompressed) data produced by mappers. mapOutputMaterializedBytes Amount of (compressed) data which was actually written to disk. mapOutputRecords Total number of records produced by all the mappers. mergedMapOutputs Total number of mapper output files undergone through shuffle phase. shuffledMaps Total number of mappers which undergone shuffle phase. Reduce reduceInputGroups Total number of unique keys. reduceInputRecords Total number of records processed by all reducers. reduceOutputRecords Total number of records produced by all reducers. reduceShuffleBytes Amount of data processed in shuffle and reduce phase. Records combineInputRecords Total number of records processed by combiners. combineOutputRecords Total number of records produced by combiners. spilledRecords Total number of map and reduce records that were spilled to disk. Time gcTimeMillis Wall clock time spent in Java Garbage Collection. cpuMilliseconds Cumulative CPU time for all tasks. Memory committedHeapBytes Total amount of memory available for JVM. physicalMemoryBytes Total physical memory used by all tasks including spilled data. splitRawBytes Amount of data consumed for meta-data representation during splits. virtualMemoryBytes Total virtual memory used by all tasks. Shuffle errors Metric Definition badId Total number of errors related with the interpretations of IDs from shuffle headers. connection Total number of established network connections. ioError Total number of errors related with reading and writing intermediate data. wrongLength Total number of errors related to compression and decompression of intermediate data. wrongMap Total number of errors related to duplication of the mapper output data. wrongReduce Total number of errors related to the attempts of shuffling data for wrong reducer. Spark metrics In addition to the metric set supported by MapReduce apps, Spark apps can be polled on: Type Metric Definition Join joinInputRowCount The total input rows of the first join of the SQL query, aggregated for all the queries that are part of the app. totalJoinInputRowCount Total number of input rows count for all join operators of all SQL queries that are part of the app. totalJoinOutputRowCount Total number of output rows count for all join operators of all SQL queries that are part of the app. joinOutputRowCount The total output rows of the first join of the SQL query, aggregated for all the queries that are part of the app. Partitions inputPartitions Total number of input partitions for all SQL queries that are part of the app. outputPartitions Total number of output partitions for all SQL queries that are part of the app. Records inputRecords Cumulative number of input records for all SQL queries that are part of the app (collected at stage level). outputRecords Cumulative number of output records for all SQL queries that are part of the app (collected at stage level). outputToInputRecordsRatio OutputRecords \/ inputRecords if inputRecords > 0, else 0. YARN resource manager metrics Metric Definition allocatedMB The sum of memory in MB allocated to the app’s running containers. allocatedVCores The sum of virtual cores allocated to the app’s running containers. appCount Total number of apps. elapsedTime The elapsed time since the app started (in ms). runningContainers The number of containers currently running for the app. memorySeconds The amount of memory the app has allocated (megabyte-seconds). vcoreSeconds The amount of CPU resources the app has allocated (virtual core-seconds). For more details see: Apache MapReduce REST Cluster Applications API " }, 
{ "title" : "Use Cases", 
"url" : "102177-usecases.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Use Cases", 
"snippet" : "Unravel's Web UI can assist you in the detection and resolution of problems. Analyzing Resource Usage Detecting apps using resources inefficiently Identifying rogue apps Detecting resource contention in the cluster HBase End-to-end monitoring of HBase cluster Kafka Best practices for end-to-end moni...", 
"body" : "Unravel's Web UI can assist you in the detection and resolution of problems. Analyzing Resource Usage Detecting apps using resources inefficiently Identifying rogue apps Detecting resource contention in the cluster HBase End-to-end monitoring of HBase cluster Kafka Best practices for end-to-end monitoring of Kafka Kafka detecting lagging or stalled partitions Spark Optimizing the performance of Spark apps How to intelligently monitor Kafka\/Spark Streaming data pipeline Using Unravel to tune Spark data skew and partitioning Using RDD caching to improve a Spark app's performance " }, 
{ "title" : "Detecting resource contention in the cluster", 
"url" : "102178-usecase-resource-content.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Use Cases \/ Detecting resource contention in the cluster", 
"snippet" : "If your cluster has a lot of apps they will contend for resources in the cluster. Unravel Web UI assists you in detecting and resolving resource contention. Click Operations > Usage Details > Resources . In the Cluster VCores or Cluster Memory MB graph, click a spike. Unravel Web UI displays the lis...", 
"body" : "If your cluster has a lot of apps they will contend for resources in the cluster. Unravel Web UI assists you in detecting and resolving resource contention. Click Operations > Usage Details > Resources . In the Cluster VCores or Cluster Memory MB graph, click a spike. Unravel Web UI displays the list of apps running or pending in the cluster at the spike's timestamp, at the bottom of the page. When you see many apps in the Accepted state (not Running ), it means they are waiting for resources. For example, the following screenshot shows that only one Spark app is Running (consuming resources) and four MR apps are Accepted (waiting for resources). Now you can take steps to resolve the problem. Setting Up AutoActions (Alerts) To define an action for Unravel to execute automatically when it detects resource contention in the cluster: Click Manage > AutoActions . Select Resource Contention in Cluster. Specify the rules for triggering this AutoAction, such as a memory threshold, job count threshold, and so on: Select Send Email . " }, 
{ "title" : "Detecting apps using resources inefficiently", 
"url" : "102179-usecase-resource-inefficiency.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Use Cases \/ Detecting apps using resources inefficiently", 
"snippet" : "As a cluster operator, you will want to surface apps that use their resources inefficiently. You can then help developers remediate a app's issue and simultaneously improve your cluster's health. One of the unique aspects of Unravel is its insights into an app's run and the recommendations it provid...", 
"body" : "As a cluster operator, you will want to surface apps that use their resources inefficiently. You can then help developers remediate a app's issue and simultaneously improve your cluster's health. One of the unique aspects of Unravel is its insights into an app's run and the recommendations it provides to improve an app's efficiency. Unravel's architecture deploys sensors that are hooked into running YARN container JVMs. These sensors capture actual memory usage which you can then compare to the amount of YARN memory allocated by developers, (memory allocation per YARN) x (number of YARN containers). For a given app, we can see JVM memory consumption on the Resource tab in the apps APM. Select the metric VmRss (Virtual memory resident set size) for this information. For Spark apps, in the following example, these time series could be compared against spark.executor.memory and spark.driver.memory . Operations > Dashboard contains a list of Inefficient Applications , if any. The list is filtered by App type and then the type of inefficiency ( Event Name ). This information is also available by a Rest API. (See \/apps\/events\/inefficient_apps for an example.) Select the tab for the app type you want to examine and a list of every event ( Event Name ) apps of that type have experienced. The number of apps experiencing the event is also listed. The table is sorted in descending order on # of App Found column. Click the event type you are interested in. A table of all the apps that experienced the event is displayed. The event type is noted in the upper left-hand corner. In this example, UnderutilizedNodeMemoryEvent was selected. This indicates a low utilization of memory resources by the apps. Unravel notes when it has tuning suggestions for the app by a glyph. Sort the list of this column to see all the apps that have recommendations. Click the app to bring up its APM. Although you are examining an app based on the event you selected, an app can have multiple events. In this case the app has four events and four recommendations. Click the event box to see Unravel's recommendation and where the app used resources inefficiently. The Recommendations tab just lists the recommendations for quick access. The Efficiency tab lists all the events the app experienced and the recommendations, if any, for the event. For the following app, Unravel recommends adjusting the spark.executor.memory , spark.default.parallelism , spark.yarn.executor.memoryOverhead , and spark.executor.instance properties. The developer can implement these recommendations as --conf arguments when using the spark-submit command. Each app type has events specific to the app type, and the recommendations are implemented based upon the app type. " }, 
{ "title" : "Identifying rogue apps", 
"url" : "102180-usecase-id-rouge-apps.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Use Cases \/ Identifying rogue apps", 
"snippet" : "Rogue apps can affect cluster health and lead to missed SLAs. Therefore, it is best practice to identify and eliminate them. Symptoms of a cluster with rogue apps include jobs that take too long to run or apps that use too many vCores. Unravel Web UI makes identifying rogue apps easy: Click Operatio...", 
"body" : "Rogue apps can affect cluster health and lead to missed SLAs. Therefore, it is best practice to identify and eliminate them. Symptoms of a cluster with rogue apps include jobs that take too long to run or apps that use too many vCores. Unravel Web UI makes identifying rogue apps easy: Click Operations | Usage Details | Infrastructure . In the Cluster vCores or Cluster Memory MB graph, click on a spike. Unravel's Web UI displays the list of apps running or pending in the cluster at the spike's timestamp at the bottom of the page. Click on the app which has allocated the highest number of vCores. In this example, there is a MapReduce app which has allocated 240 vCores of the cluster. Check the event panel in the app's APM to see Unravel's recommendations for improving the efficiency of this MapReduce app. For example: Set up an AutoAction to proactively alert if a rogue app is occupying the cluster. " }, 
{ "title" : "End-to-end monitoring of HBase databases and clusters", 
"url" : "102181-usecase-monitoring-hbase-databases.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Use Cases \/ End-to-end monitoring of HBase databases and clusters", 
"snippet" : "Running real-time data injection and multiple concurrent workloads on HBase clusters in production are always challenging. There are multiple factors that affect a cluster's performance or health and dealing with them isn't easy. Timely, up-to-date and detailed data is crucial to locating and fixing...", 
"body" : "Running real-time data injection and multiple concurrent workloads on HBase clusters in production are always challenging. There are multiple factors that affect a cluster's performance or health and dealing with them isn't easy. Timely, up-to-date and detailed data is crucial to locating and fixing issues to maintain a cluster's health and performance. Most cluster managers provide high level metrics, which while helpful, aren't enough for understanding cluster and performance issues. Unravel provides detailed data and metrics to help you identify the root causes of cluster and performance issues specifically hot-spotting. This tutorial examines how you can use Unravel's HBase's (APM) to debug issues in your HBase cluster and improve its performance. Cluster health Unravel provides cluster metrics per cluster which provide an overview of HBase clusters in the Operations > Usage Details > HBase tab where all the HBase clusters are listed. Click a cluster name to bring up the cluster's detailed information. The metrics are color coded so you can quickly ascertain your cluster's health and what, if any, issues you need to drill into. Green = Healthy Red = Unhealthy, alert for metrics and investigation required In this case, we examine the HBase metrics and to help youunderstand how use them to monitor your HBase cluster through Unravel. Overall cluster activity Live Region Servers : the number of running region servers. Dead Region Servers : the number of dead region servers. Cluster Requests : the number of read and write requests aggregated across the entire cluster. Average Load : the average number of regions per region server across all Servers Rit Count : the number of regions in transition. RitOver Threshold : the number of regions that have been in transition longer than a threshold time (default: 60 seconds). RitOldestAge : the age, in milliseconds, of the longest region in transition. Region Server refers to the servers (hosts) while region refers to the specific regions on the servers. Dead Region Servers This metric gives you insight into the health of the region servers . In a healthy cluster this should be zero. When the number of dead region servers is one or greater you know something is wrong in the cluster. Dead region servers can cause an imbalance in the cluster. When the server has stopped its regions are then distributed across the running region servers . This consolidation means some region servers handle a larger number of regions and consequently have a corresponding higher number of read, write and store operations. This can result in some servers processing a huge number of operations and while others are idle, causing hot-spotting. Average Load This metric is the average number of regions on each region server . Like Dead Region Servers , this metric helps you to triage imbalances on cluster and optimize the cluster's performance. Below, for the same cluster, you can see the impact on the Average Load when Dead Region Servers is 0 and 4. Dead Region Servers=0 In this case, the Average Load is 2k. Dead Region Servers=4 As the number increased so did the corresponding Average Load which is now 3.23k, an increase of 60%, The next section of the tab contains a region server table which shows the number of regions hosted by the region server . You can see the delta (Average Load - Region Count); a large delta generates an imbalance in cluster and reduces the cluster's performance. You can resolve this issue by either: Moving the regions onto other region servers . Removing regions from a current region server at which point the master immediately deploys it on another available region server . Region server Unravel provides a list of region servers , their metrics and Unravel's insight into server's health for all region servers across the cluster for a specific point of time. For each region server the table lists the Region Server Name and the server metrics Read Request Count , Write Request Count , Store File Size , Percent Files Local , Compaction Queue Length , Region Count , and Health for each server. These metrics and health status are helpful in monitoring activity in your HBase cluster The Health status is color coded and you can see at a glance when the server is in trouble. Hover over the server's health status to see a tool tip listing the hot-spotting notifications with their associated threshold (Average value * 1.2). If any value is above the threshold the region server is hot-spotting and Unravel shows the health as bad. Region server metric graphs Beneath the table are four graphs readRequestCount , writeRequestCount , storeFileSize , and percentFilesLocal . These graphs are for all metrics across the entire cluster. The time period the metrics are graphs over is noted above the graphs. Tables The last item in the Cluster tab is a list of tables. This list is for all the tables across all the region servers across the entire cluster. Unravel then uses these metrics to attempt to detect an imbalance. Averages are calculated within each category and alerts are raised accordingly. Just like with the region servers you can tell at glance the Health of the table. The list is searchable and displays the Table Name , Table Size , Region Count , Average Region Size , Store File Size , Read Request Count , Write Request Count , and finally Health . Hover over the health status to see a tool tip listing the hot-spotting. Bad health indicates that a large amount of store operations from different sources are redirected to this table. In this example, the Store File Size is more than 20% of the threshold. You can use this list to drill down into the tables, and get its details which can be useful to monitor your cluster. Click a table to view its details, which include graphs of metrics over time for the region, a list of the regions using the table, and the apps accessing the table. The following is an example of the graphed metrics, regionCount , readRequestCount , and writeRequestCount . Region Once in the Table view, click the Region tab to see the list of regions accessing the table. The table list shows the Region Name , Region Server Name , and the region metrics Store File Size , Read Request Count , Write Request Count , and Health . These metrics are useful in gauging activity and load in region. The region health is important in deciding whether the region is functioning properly. In case any metric value is crossing the threshold, the status is listed as bad. A bad status indicates you should start investigating to locate and fix hot-spotting. " }, 
{ "title" : "Best practices for end-to-end monitoring of Kafka", 
"url" : "102182-usecase-kafka-monitoring-best-practice.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Use Cases \/ Best practices for end-to-end monitoring of Kafka", 
"snippet" : "It can be difficult to understand what is happening in your Kafka cluster and to successfully root cause\/troubleshoot problems. In order to help you do this, Unravel provides insights and visibility throughout all levels of your cluster. This includes the ability to drill down into apps acting as co...", 
"body" : "It can be difficult to understand what is happening in your Kafka cluster and to successfully root cause\/troubleshoot problems. In order to help you do this, Unravel provides insights and visibility throughout all levels of your cluster. This includes the ability to drill down into apps acting as consumers processing data streams. Unravel's Kafka monitoring and insights are built-in; no configuration is required once you have connected Unravel to your Kafka cluster., In th following examples are some of the best practices for using Unravel to monitor your Kafka environments. It is assumed you have basic knowledge of Kafka concepts and architecture. Your starting point in Unravel is Operations > Usage Details > Kafka . It displays the overall health of the cluster and provides immediate access to your cluster's brokers and topics. Unravel provides six cluster wide KPIs to give you an immediate view of the Kafka cluster's overall health and IO. The first three KPIs provide information on your clusters' partitions and controllers. They are color coded so you can quickly ascertain their health. Green = Healthy Red = Unhealthy, alert for metrics and investigation required The next three KPIs are data IO metrics and are always blue. Beneath the KPIs are three tabs, Metrics , Broker , and Topic . The Metrics and Broker tabs have the same graphs but contain different data, cluster wide and broker specific data respectively. These KPIs and the data graphs help you root cause existing problems or identify potential issues. Immediate access to your cluster's brokers and topics allow you to quickly drill down and locate the problematic areas. Below we examine a few scenarios. Under replicated partitions A healthy cluster has no under replicated partitions. If this KPI is greater than zero, your replication isn't occurring as fast as configured. This under replication: Adds latency as consumers don't receive their needed data until messages are replicated. Suggests you are more vulnerable to data loss if you have a master failure. If you have under replicated partitions, it typically points to a problem with one or more brokers. You should root cause the problem immediately to avoid any data loss. First, you need to understand how often the under replication is occurring. Scroll down in the Metrics tab to locate the Under Replicated Partitions graph to see the under replication over time across the cluster. When you see current or consistent under replication drill down into the brokers. Click the Broker tab for the list of brokers and their KPIs. Brokers having problems will have 1 or greater listed in Under Replicated Partitions column. Click each broker with under replicated partitions and scroll down to its Under Replicated Partitions graph. Once you have identified the problem brokers, examine each of their logs to determine the root cause. Offline partitions This KPI is the total number of offline topic partitions. A healthy cluster doesn't have any offline partitions. Offline partitions can occur if: The brokers with replicas are down. Unclean leader election is disabled and the replicas aren't in sync. In this case, none can be elected leader. (This may be desirable to ensure no messages are lost.) When you have offline partitions you should immediately root cause the problem. Scroll down in the Metrics tab to locate the # Offline Partition Count graph and examine the offline partition number over time. If you have currently have offline partitions, drill down into the brokers. Click the Broker , and check Offline Partitions Count for each broker. Click each broker with offline partitions and examine its # Offline Partition Count graph. It graphs the number of offline partitions over time and you can see whether the partition is currently offline, were briefly offline, or were offline for an extended period of time. After identifying the problem brokers, examine each broker's log to determine the cause of the problem. Topic partition strategy This section provides best practices for using Unravel to evaluate your topic\/broker performance. A common challenge is providing an architecture for the topics and partitions in your cluster which can, and will, support the data velocity coming from producers. Typically, adjustments have tended to be a \"shot in the dark\" because of a lack of relevant data. Unravel provides detailed information to shed light on how your architecture is performing, which, in turn, can help you to re-configure your architecture for better performance. Producer architecture It can be a challenge in deciding how to partition topics on the Kafka level. Producers can choose to send messages via key or use a round-robin strategy when no key has been defined for a message. Choosing the correct number of partitions for your data velocity is important to ensure you have a real-time architecture that is performant. You can use Unravel to get a better understanding on how your current architecture is performing. Then, use the insights to guide you in choosing the number of partitions. Click the Topic tab and scroll down for the list of topics in your cluster. You can quickly identify topics with heavy traffic and where you should examine your partitions' performance. For topics that are experiencing heavy traffic or are in trouble, click its consumer. You can then see the consumer's topic list and the health of each topic. Use this information to help you determine the correct number of partitions. Consumer group architecture You can make changes on the consumer group side to scale according to your topic\/partition architecture. Unravel provides a convenient view for each topic's consumer groups so you can quickly see the health status of each consumer group in your cluster. When the consumer group is a Spark Streaming app Unravel can provide insights for it, thereby providing an end-to-end monitoring solution for your streaming apps. See Kafka Insights for a use case on monitoring consumer groups, and lagging\/stalled partitions or consumer groups. References For a complete description of all the Kafka metrics\/graphs and how to interpret them see here . " }, 
{ "title" : "Log flush latency", 
"url" : "102182-usecase-kafka-monitoring-best-practice.html#UUID-4a776921-7269-80e0-efba-5071d2d22abe_section-5ce0f80adbb44-idm45390842125120", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Use Cases \/ Best practices for end-to-end monitoring of Kafka \/ Log flush latency", 
"snippet" : "Log flush latency is important; it is also related to under replicated partitions. The longer it takes to flush the log to disk, the more the pipeline backs up and the worse the latency and throughput. When latency goes up, even as small an increment as 10ms, end-to-end latency balloons which can le...", 
"body" : "Log flush latency is important; it is also related to under replicated partitions. The longer it takes to flush the log to disk, the more the pipeline backs up and the worse the latency and throughput. When latency goes up, even as small an increment as 10ms, end-to-end latency balloons which can lead to under replicated partitions The Log Flush Latency, 99th Percentile graph lets you keep track of the cluster's latency. At any given point in time 99% of the log latency is less than this value. Scroll down in the Metrics tab to see the cluster's log latency graph. If the latency is fluctuating dramatically, or is consistently high, you need to identify the broker that are contributing to it. Click the Broker tab so you can examine individual brokers. Unlike under replicated partitions, the broker table doesn't include a log flush latency KPI. You must examine each broker individually. Click each broker and then scroll down to its Log Flush Latency, 99th Percentile graph. Problematic brokers have latency graphs with dramatic fluctuations or consistently high latency. After identifying these brokers, examine their logs to gain insight into the problem. If your cluster can keep up with the amount of data coming in, consider adding more brokers. Another possibility is that the node is \"bad\", e.g., the hard drive is going bad; if that is the case you should swap the node out for a healthy one. Regularly monitoring this graph can help you see trends which could lead to problems. " }, 
{ "title" : "Controller health", 
"url" : "102182-usecase-kafka-monitoring-best-practice.html#UUID-4a776921-7269-80e0-efba-5071d2d22abe_section-5ce209fb22fd9-idm45390841858448", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Use Cases \/ Best practices for end-to-end monitoring of Kafka \/ Controller health", 
"snippet" : "The # Controller KPI can indicate potential problems with your cluster. If the # Controller is: 0: There is no active controller. You want to avoid this since data can potentially be lost. 1: There is one active controller. This is the preferred state. During a steady state there should be only one ...", 
"body" : "The # Controller KPI can indicate potential problems with your cluster. If the # Controller is: 0: There is no active controller. You want to avoid this since data can potentially be lost. 1: There is one active controller. This is the preferred state. During a steady state there should be only one active controller per cluster. Greater than 1: This is inconclusive and it's important to find out what is occurring. If this only persists for about a minute, it likely means the active controller switched from one broker to another. If this persists for longer, troubleshoot the cluster for \"split brain\". The # Active Controller Trend graphs all brokers and their status as the active controller. You can swiftly see where your cluster has problems. When the number of active controllers is greater than one for longer a minute or so, you should investigate the brokers' logs. Hover over the graph to see what brokers were active controllers simultaneously. You can view this graph for each broker individually in the Broker tab. When needed, examine the relevant brokers' logs for further insight. In this example there are three points where the controller number is greater than one. The first two incidents were for a brief period, while the third was the active controller being switched from one broker to another. You don't need to investigate the latter, but you might want to investigate the former. " }, 
{ "title" : "Cluster activity", 
"url" : "102182-usecase-kafka-monitoring-best-practice.html#UUID-4a776921-7269-80e0-efba-5071d2d22abe_section-5ce20a0b366f8-idm45690835649824", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Use Cases \/ Best practices for end-to-end monitoring of Kafka \/ Cluster activity", 
"snippet" : "The last three KPIs show cluster activity within the last 24 hours. They are always colored blue because they are counts and therefore neither healthy nor unhealthy. These KPIs are useful in gauging your cluster's activity for the last 24 hours. As with the other KPIs, you can view their information...", 
"body" : "The last three KPIs show cluster activity within the last 24 hours. They are always colored blue because they are counts and therefore neither healthy nor unhealthy. These KPIs are useful in gauging your cluster's activity for the last 24 hours. As with the other KPIs, you can view their information graphically. These metrics can be useful in understanding cluster capacity, and to: Determine if you need additional brokers to keep up with data velocity. Evaluate the performance of topic architecture on your brokers. Evaluate the performance of partition architecture for a topic. These metrics are graphed over time and you can view them across the cluster, by broker, or by topic. Shown here are the Metrics graphs for the KPIs. Click the Broker or Topic tabs to see the metrics across a specific broker or topic. " }, 
{ "title" : "Kafka detecting lagging or stalled partitions", 
"url" : "102183-usecase-kafka-lagged-stalled.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Use Cases \/ Kafka detecting lagging or stalled partitions", 
"snippet" : "Overview The Unravel Intelligence engine provides insights into a cluster's activities through the status of the Consumer Group's (CG). A CG's state is determined on a partition by partition basis, specifically by its commit offset relative to the producer's log commit offset for a particular partit...", 
"body" : "Overview The Unravel Intelligence engine provides insights into a cluster's activities through the status of the Consumer Group's (CG). A CG's state is determined on a partition by partition basis, specifically by its commit offset relative to the producer's log commit offset for a particular partition. The states are: OK : commit offset is in pace with the log end offset. Lagging : the Consumer is lagging behind the Producer. Both the consumer's and log commit offsets are increasing, but the Producer’s is increasing faster. When graphed over time, the Producer's slope is increasing faster than the consumer's slope. Stalled : the Consumer has essentially stopped while the Producer is still active. The consumer's offset isn't increasing while the log commit offset is increasing. Graphically, the slope of the consumer is essentially zero. A Topic's status is set to the lowest status among its Consumer Groups and a Consumer Group's status is set to the lowest status among its partitions. You must drill down from the Cluster in order to determine where Topic\/Consumer Group is lagging or stalled. Use case example Go to Operation | Charts | Kafka to view your currently configured clusters. Click a cluster name to bring up the Cluster View. Use the graph scroll-bar to view graphs of the cluster metrics. The Topic table lists all topics with their consumers and the topic's status. Click within a graph to see what topics were available at that point in time. Click the topic name to examine it. In this case, topic test2 is stalled and has two consumers, demo and test-consumer-group . Consumers with the same name are grouped together into one consumer group. Choosing all clusters from the pull down menu creates tabs for all your clusters so you can easily switch between them. The Topic view opens with Topic Detail tab displaying the brokers KPIs. The Consumer Details table lists active Consumers with its status for that point in time. Its KPIs are across all partitions. Click the graph to see what Consumers were running at that point in time. Here test2 only has one stalled consumer demo ; in turn, demo has one stalled and three lagging partitions. Click the Partition Detail tab to view the Consumer's information per partition. The Consumer Details table now lists the KPIs and status for all consumer groups on the partition displayed. Click within the graph to see what Consumers were running at that point in time on that partition. Partition 0 is initially displayed using the metric offset . As expected from the preceding graph the test-consumer-group is OK on this partition, while demo is lagging. Use the Partition and Metric pull down menu to change partition to display and metric ( Offset or Consumer Lag ) to use. Click more info glyph ( ) in the Go To column to bring up the Consumer Group View. Here Partition 1 idgraphed on the metric Consumer Lag . It's obvious this is a stalled partition both from the steep slope and the status icon. The test-consumer-group is basically a flat-line. The CG view lists the Topics which it is consuming and opens with graphs of its broker KPIs. Just as a Topic can have multiple consumers with varying states, a Consumer Group can be consuming multiple topics with varying degrees of success. In this case, there is only one Topic being consumed and the CG is stalled. Click the Partition Detail tab to see partition. The Partition Details table lists the partitions with its KPIs and status . The window defaults to the graph of partition 0 using the offset metric. In the following image we see partition 1 is stalled, while 0, 2, 3 are lagging. Use the pull down menus to change Metric or Partition used for the graph. The denotes the partition displayed. The graph is shows partition 1 graphed using consumer lag. " }, 
{ "title" : "Optimizing the performance of Spark apps", 
"url" : "102184-usecase-spark-opt.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Use Cases \/ Optimizing the performance of Spark apps", 
"snippet" : "Unravel's Web UImakes it easy for you to identify under-performing Spark apps: click Operations > Dashboard , and scroll down to Inefficient Applications . The following case illustrates the performance of a Spark app before and after tuning it based on Unravel's performance analysis: Before tuning ...", 
"body" : "Unravel's Web UImakes it easy for you to identify under-performing Spark apps: click Operations > Dashboard , and scroll down to Inefficient Applications . The following case illustrates the performance of a Spark app before and after tuning it based on Unravel's performance analysis: Before tuning Before tuning, Unravel Web UI indicated that this app had a running duration of 34 min 11sec: In addition, Unravel Web UI captured details as shown in the following events: Low utilization of memory resources. Low utilization of Spark storage memory. Contention for CPU resources. Opportunity for RDD caching. Save up to 9 minutes by caching at PetFoodAnalysisCaching.scala:129,with StorageLevel.MEMORY_AND_DISK_SER Too few partitions w.r.t to available parallelism. Change executor instances from 2 to 127, partitions from 2 to 289, adjust driver memory (to 1161908224)and yarn overhead (to 819 MB). After tuning After tuning, Unravel's Web UI indicates that this app now has a running duration of 1min 19sec: Unravel Web UI displays these events: Low utilization of memory resources. Low utilization of Spark storage memory. Large idle time for executors. Too few partitions w.r.t to available parallelism. Change executor instances from 127 to 138 " }, 
{ "title" : "Using Unravel to tune Spark data skew and partitioning", 
"url" : "102185-usecase-spark-skew-partitioning.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Use Cases \/ Using Unravel to tune Spark data skew and partitioning", 
"snippet" : "In Spark, it is very important that the RDD partitions are aligned with the number of available tasks. Spark assigns one task per partition and each core can process one task at a time. By default, the number of partitions is set to the total number of cores on all the nodes hosting executors. Too f...", 
"body" : "In Spark, it is very important that the RDD partitions are aligned with the number of available tasks. Spark assigns one task per partition and each core can process one task at a time. By default, the number of partitions is set to the total number of cores on all the nodes hosting executors. Too few partitions leads to less concurrency, processing skew, and improper resource utilization. Too many partitions leads to low throughput and high task scheduling overhead. Tuning a Spark app You need to identify the stages that represent the bottlenecks during the execution. This can easily be done using Unravel's Spark Application Performance Manager (APM). Bring up your app in the Spark APM and select the Gannt Chart tab. Locate the job with the longest duration; click on it to bring the job up in the Spark Job APM. Now locate and click on the stage with the longest duration. The Spark job above took 3 hours 53-minutes to complete with the longest stage taking 43 minutes. This is the stage to examine. Clicking on the stage brings up its information. Click the Timeline tab to show the duration and I\/O of each task. The histogram charts allow you to quickly identify outlier tasks. In this case, there were 200 tasks; 199 tasks took approximately 5-minutes to complete and one approximately 35-40 minutes. This one task accounts for over 82% of the time the stage took to complete. To see the timelines for the first bucket (199 tasks), select it and then the Timeline tab under the histogram. This view shows an understandable view of the skew. In this example, you can see many executors are sitting idle as they wait for the outlier task to complete. Select the outlier bucket and the Timeline tab updates to display the information for this task. In this case the duration of the associated executor is almost equal to the duration of the entire app. The Spark APM's Graphs > Containers graph shows the bursting of containers when the longer executor started. Adding more partitions via repartition() can help distribute the data set among the executors and decrease the skew. Unravel might provide recommendations for optimizations in some cases where the join key or group by key are skewed. In this case, Unravel had three recommendations to improve the app's efficiency. Example In this Spark SQL example two dummy data sources are used, both of which are partitioned. The join operation between customer and order table is on cust_id column which is heavily skewed. Examining the code shows the key, 1000 , has the most number of entries in the orders table. Therefore, one of the reduce partitions contains all the 1000 entries. In such cases there are techniques which can help to avoid skewed processing. Increasing the spark.sql.autoBroadcastJoinThreshold value enables the smaller table “customer” to get broadcasted. Ensuring sufficient driver memory should address this problem. If memory in executors is sufficient, then decreasing the spark.sql.shuffle.partitions to accommodate more data per reduce partitions can help. This helps all the reduce tasks to have approximately the same duration. If possible find out the keys which are skewed and process them separately by using filters. Example: Decrease the spark.sql.shuffle.partitions to accommodate more data per reduce partitions. (Technique #2.) In this example, the spark.sql.shuffle.partitions default is 200. Here, there is lone task which takes more time during shuffle. That means the next stage can’t be started and executors are lying idle. Now change the spark.sql.shuffle.partitions to ten (10). As the shuffle input\/output is well within executor memory sizes, we can safely make this change. import org.apache.spark.sql.SparkSession\n\nobject SkewedDataframeTest {\n\n val INVALID_USAGE =\n s\"\"\"\n |Please enter all cmd line parameters!\n |\n |<numElements> valid numbers are 1 to any positive integer.\n |<skewPercent> valid numbers between 1 to 99\n \"\"\".stripMargin\n\n def main(args: Array[String]) = {\n\n if (args.length < 2) {\n println(INVALID_USAGE)\n System.exit(-1)\n }\n\n val numElements = args(0).toInt\n val skewPercent = args(1).toFloat\n\n val spark = SparkSession.builder().\n enableHiveSupport().getOrCreate()\n\n import spark.implicits._\n\n val numSkewedKeys = (numElements * (skewPercent \/ 100)).toInt\n val numCustomers = numElements \/ 100\n \/\/ Dummy data source which is partitioned\n val df3 = spark.sparkContext.parallelize(0 to numElements, 16).map(i => {\n val join_column = if (i < numSkewedKeys) 1000 else i % 999\n (i, join_column)\n }).toDF(\"order_no\", \"cust_id\").\n repartition(16, $\"order_no\")\n\n df3.createOrReplaceGlobalTempView(\"orders\")\n\n \/\/ Dummy data source which is partitioned\n val df4 = spark.sparkContext.parallelize(0 to numCustomers, 32).\n map(i => (i, i.toString)).\n toDF(\"cust_id\", \"name\").\n repartition(32, $\"cust_id\")\n\n df4.createOrReplaceGlobalTempView(\"customer\")\n\n spark.sql(\"select t1.name, COUNT( t2.order_no) \" +\n \"from global_temp.customer t1, global_temp.orders t2\" +\n \" where t1.cust_id=t2.cust_id \" +\n \" group by t1.name \" +\n \" having COUNT( t2.order_no) >= 50000\").show\n }\n\n} Real-life deployments In product deployments not all skew problems can be solved by configurations and repartitioning. If the data source itself is skewed, then tasks which read from these sources can’t be optimized; modifying the underlying data layout can help. Sometimes, at enterprise level, modification isn't possible as the data source is used from different tools and pipelines. " }, 
{ "title" : "How to intelligently monitor Kafka\/Spark Streaming data pipeline", 
"url" : "102186-usecase-spark-streaming-kafka.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Use Cases \/ How to intelligently monitor Kafka\/Spark Streaming data pipeline", 
"snippet" : "Identifying issues in distributed apps like a Spark streaming app isn't trivial. Unravel helps you connect the dots across streaming apps to identify bottlenecks. Spark streaming is widely used in real-time data processing, especially with Apache Kafka. A typical scenario involves a Kafka producer a...", 
"body" : "Identifying issues in distributed apps like a Spark streaming app isn't trivial. Unravel helps you connect the dots across streaming apps to identify bottlenecks. Spark streaming is widely used in real-time data processing, especially with Apache Kafka. A typical scenario involves a Kafka producer app writing to a Kafka topic. The Spark app then subscribes to the topic and consumes records. The records might be further processed downstream using operations like map and foreachRDD ops or saved into a datastore. The following two scenarios illustrate how you can use Unravel’s APMs to inspect, understand, correlate, and finally debug issues around a Spark streaming app consuming a Kafka topic. They demonstrate how Unravel's APMs can help you perform an end-to-end analysis of the data pipelines and its apps. In turn, this wealth of information helps you effectively and efficiently debug and resolve issues that otherwise would require more time and effort. Producer failing scenario Consider a running Spark app that hasn't processed any data for a significant amount of time. The app's previous iterations successfully processed data and you know for a certainty that current iteration should be processing data. When you notice the Spark app has been running without processing any data, you want to check the app's I\/O. Click it to bring up the app and see its I\/O KPI. When you see the Spark app is consuming from Kafka, you need to examine Kafka side of the process to determine the issue. Click a batch to find the topic it is consuming. Bring up the Kafka Topic details which graphs the Bytes In Per Second , Bytes Out Per Second , Messages In Per Second , and Total Fetch Requests Per Second . In this case, you see Bytes In Per Second and Messages In Per Second graphs show a steep decline; this indicates the Kafka Topic is no longer producing any data. The Spark app isn't consuming data because there is no data in the pipeline to consume. You should notify the owner of the app that writes to this particular topic. Upon notification, they can drill down to determine and then resolve the underlying issues causing that writes to this topic to fail. ‘Slow’ Spark app and offline partitions in the Kafka cluster In this scenario, an app's run has processed significantly less data when compared to what is the expected norm. In the above scenario, the analysis was fairly straightforward. In these types of cases, the underlying root cause might be far more subtle to identify. The Unravel APM can help you to quickly and efficiently root cause such issues. When you see a considerable difference in the data processed by consecutive runs of a Spark App, bring up the APMs for each app. Examine the trend lines to identify a time window when there is a drop in input records for the slow app. The image, the slow app, shows a drop off in consumption. Drill down into the slow app's stream graph to narrow the time period (window) in which the I\/O dropped off. Unravel displays the batches that were running during the problematic time period. Inspect the time interval to see the batches' activity. In this example no input records were processed during the suspect interval. You can dig further into the issue by selecting a batch and examining it's input tab. In the following case, you can infer that no new offsets have been read based upon the Input Source 's Description ,“Offsets 1343963 to Offset 1343963”. You can further debug the problem on the Kafka side by viewing the Kafka cluster monitor. Navigate to Operations > Usage Details > Kafka . At a glance, the cluster's KPIs convey important and pertinent information. Here, the first two metrics show that the cluster is facing issues which need to be resolved as quickly as possible. # of Under Replicated Partitions is a broker level metric of the count of partitions for which the broker is the leader replica and the follower replicas that have yet not caught up. In this case there are 131 under replicated partitions. # of offline partitions is a broker level metric provided by the cluster's controlling broker. It is a count of the partitions that currently have no leader. Such partitions aren't available for producing\/consumption. In this case there are two offline partitions. You can select the Broker tab to see broker table. Here the table hints that the broker, Ignite1.kafka2 is facing some issue; therefore, the broker's status should be checked. Broker Ignite1.kafka2 has two offline partitions. The broker table shows that it is\/was the controller for the cluster. We could have inferred this because only the controller can have offline partitions. Examining the table further, we see that Ignite.kafka1 also has an active controller of one. The Kafka KPI # of Controller lists the Active Controller as one which should indicate a healthy cluster. The fact that there are two brokers listed as being one active controller indicates the cluster is in an inconsistent state. You can further corroborate the hypothesis by checking the status of the consumer group for the Kafka topic the Spark App is consuming from. In this example, consumer groups' status indicates it's currently stalled. The topic the stalled group is consuming is tweetSentiment-1000 . To drill down into the consumer groups topic, click the TOPIC tab in the Kafka Cluster manager and search for the topic. In this case, the topic's trend lines for the time range the Spark app's consumption dropped off show a sharp decrease in the Bytes In Per Second and Bytes Out Per Second . This decrease explains why the Spark app isn't processing any records. To view the consumer groups lag and offset consumption trends, click the consumer groups listed for the topic. In this example, the topic's consumer groups, stream-consumer-for-tweetSentimet1000 , log end offset trend line is a constant (flat) line that shows no new offsets have been consumed with the passage of time. This further supports our hypothesis that something is wrong with the Kafka cluster and especially broker Ignite.kafka2 . These are but two examples of how Unravel helps you to identify, analyze, and debug Spark Streaming apps consuming from Kafka topics. Unravel's APMs collates, consolidates, and correlates information from various stages in the data pipeline (Spark and Kafka), thereby allowing you to troubleshoot apps without ever having to leave Unravel. " }, 
{ "title" : "Using RDD caching to improve a Spark app's performance", 
"url" : "102187-usecase-spark-rdd-caching.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Use Cases \/ Using RDD caching to improve a Spark app's performance", 
"snippet" : "Due to Spark’s lazy evaluation model, there are often cases where a Spark app recomputes the same RDD several times during the lifetime of the app. Spark provides a facility cache() , which when inserted into your code, caches RDDs in executor memory. This can result in improving performance and dec...", 
"body" : "Due to Spark’s lazy evaluation model, there are often cases where a Spark app recomputes the same RDD several times during the lifetime of the app. Spark provides a facility cache() , which when inserted into your code, caches RDDs in executor memory. This can result in improving performance and decreasing overall execution time, especially when a given RDD is re-computed multiple times over the app's lifetime. Unravel's insights and recommendations tells you when you can take advantage of cache() to improve your app's performance. Above is an example, where Unravel is recommending RDD caching. But, more than that, Unravel also recommends where you should insert cache() into your code. In this case, Unravel recommends inserting it before line 102. The Spark APM than can show you the context of where in the code you should place it. Unravel Insights makes it easy for you to understand where and when to use cache() in order to improve your app's performance. This is just one of the events (insights) that Unravel’s ML-driven approach generates to help you tune your Spark apps. See our other use cases for further examples of how Unravel can save you a challenging and time consuming analysis with its it insights and recommendations to improve your app's performance. " }, 
{ "title" : "Miscellaneous", 
"url" : "102188-uguide-misc.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Miscellaneous", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Some keywords and error message", 
"url" : "102189-resource-metrics-keywords-error-messages.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Miscellaneous \/ Some keywords and error message", 
"snippet" : "Commonly searched keywords\/terms and error messages organized by job type. Spark keywords Spark key term Explanation Deploy mode Specifies where the driver runs. In “cluster” mode the driver runs on the cluster. In “client” mode the driver runs on the edge node, outside of the cluster. Driver The pr...", 
"body" : "Commonly searched keywords\/terms and error messages organized by job type. Spark keywords Spark key term Explanation Deploy mode Specifies where the driver runs. In “cluster” mode the driver runs on the cluster. In “client” mode the driver runs on the edge node, outside of the cluster. Driver The process that coordinates the application execution. Executor The process launched by the application on a worker node. Resilient Distributed Dataset (RDD) Fault tolerant distributed dataset. spark.default.parallelism Default number of partitions. spark.dynamicAllocation.enabled Enables dynamic allocation in Spark. spark.executor.memory Related to executor memory. spark.io.compression.codec Codec used to compress RDDs, the event log file, and broadcast variables. spark.shuffle.service.enabled Enables the external shuffle service to preserve shuffle files even when executors are removed. It is required by dynamic allocation. spark.shuffle.spill.compress Specifies whether to compress the shuffle files. spark.sql.shuffle.partitions Number of SparkSQL partitions. spark.yarn.executor.memoryOverhead YARN memory overhead. SparkContext Main Spark entry point; used to create RDDs, accumulators, and broadcast variables. SparkConf Spark configuration object. SQLContext Main Spark SQL entry point. StreamingContext Main Spark Streaming entry point. Spark error messages Spark error messages Explanation Container killed by YARN for exceeding memory limits. The amount of off-heap memory was insufficent at container level. \"spark.yarn.executor.memoryOverhead\" should be increased to a larger value. java.io.IOException: Connection reset by peer Connection reset by peer. Generally occurs in the driver logs when some of the executors fail or are shutdown unexpectedly. java.lang.OutOfMemoryError Out of memory error, insufficient Java heap space at executor or driver levels. org.apache.hadoop.mapred.InvalidInputException Input path does not exist. org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Buffer overflow. org.apache.spark.sql.catalyst.errors.package$TreeNodeException Exception observed when executing a SparkSQL query on non existing data. MapReduce\/Hive keywords Key Term Explanation hive.exec.parallel Determines whether to execute jobs in parallel. hive.exec.reducers.bytes.per.reducer The size per reducer. io.sort.mb The total amount of buffer memory to use while sorting files, in megabytes. io.sort.record.percent The percentage of io.sort.mb dedicated to tracking record boundaries. mapreduce.input.fileinputformat.split.maxsize Buffer overflow. mapreduce.input.fileinputformat.split.minsize Maximum chunk size map input should be split into. mapreduce.job.reduces Default number of reduce tasks per job. mapreduce.map.cpu.vcores Number of virtual cores to request from the scheduler for each map task. mapreduce.map.java.opts JVM heap size for each map task. mapreduce.map.memory.mb The amount of memory to request from the scheduler for each map task. mapreduce.reduce.cpu.vcores Number of virtual cores to request from the scheduler for each reduce task. mapreduce.reduce.java.opts JVM heap size for each reduce task. mapreduce.reduce.memory.mb The amount of memory to request from the scheduler for each reduce task. " }, 
{ "title" : "Resource metrics", 
"url" : "102190-resource-metrics.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ User guide \/ Miscellaneous \/ Resource metrics", 
"snippet" : "The table below lists the resource metrics collected by Unravel. The availability of a particular metric is dependent on the underlying OS and JVM GC algorithm in use. Metric Unit Description allocatedBytes bytes Accumulated number of allocated bytes. availableMemory bytes An estimate of memory avai...", 
"body" : "The table below lists the resource metrics collected by Unravel. The availability of a particular metric is dependent on the underlying OS and JVM GC algorithm in use. Metric Unit Description allocatedBytes bytes Accumulated number of allocated bytes. availableMemory bytes An estimate of memory available for launching new processes. avgFullGcInterval nanoseconds (duration) Average interval between two subsequent full GCs. Might not be available for particular GC algorithms. avgMinorInterval nanoseconds (duration) Average interval between two subsequent minor GCs. Might not be available for particular GC algorithms. blockingRatio percent Estimated percentage of CPU time spent in kernel blocking operations. committedHeap bytes The committed heap size. committedNonHeap bytes The committed non-heap size. committedVirtualMemory bytes The committed virtual memory in the operating system. currentThreadCpuTime nanoseconds (duration) Current thread CPU time elapsed since the start of the measurement. Might not be available on some operating systems. currentThreadUserTime nanoseconds (duration) Current thread user time elapsed since the start of the measurement. Might not be available on some operating systems. edenPeakUsage bytes Maximum memory usage in the eden space. freePhysicalMemory bytes The free physical memory in the operating system. freeSwap bytes The free swap size. fullGcCount count Number of full GC runs. fullGcTime nanoseconds (duration) Accumulated time spent in full GC. gcEdenSurvivedAvg bytes Average number of bytes moved from eden to survivor space. Might not be available for particular GC algorithms. gcLoad percent Percentage of CPU time spent in GC. gcOldLiveAvg bytes Average number of bytes alive in the old generation. Might not be available for particular GC algorithms. gcSurvivorPromotedAvg bytes Average number of bytes moved from survivor to old space. Might not be available for particular GC algorithms. gcYoungLiveAvg bytes Average number of bytes alive in the young generation (eden + survivor spaces). It might not be available for particular GC algorithms. initHeap bytes The initial heap size. initNonHeap bytes The initial non-heap size. maxHeap bytes The maximum heap size. maxNonHeap bytes The maximum non-heap size. minorGcCount COUNT The number of minor GC runs. minorGcTime nanoseconds (duration) The accumulated time spent in minor GC. oldPeakUsage bytes The maximum memory usage in the old space. processCpuLoad PERCENT Average process CPU load for the last minute (all cores). snapshotTs milliseconds (timestamp) The time the metric was read. startTs milliseconds (timestamp) The time when the collection process started. survivorPeakUsage bytes Maximum memory usage in the survivor space. systemCpuLoad PERCENT Average system CPU load for the last minute (all cores). totalPhysicalMemory bytes The total physical memory in the operating system. totalSwap bytes The total swap size. usedHeap bytes The used heap size. usedNonHeap bytes The used non-heap size. vmRss bytes The resident set size of the complete process tree. vmRssDir bytes The resident set size of the process. " }, 
{ "title" : "Advanced topics", 
"url" : "102191-adv.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics", 
"snippet" : "Advanced instrumentation Advanced installation Configurations Cloud Creating an AWS RDS CloudWatch Alarm for free storage space HBase Configuring HBase Hive Connecting to a Hive Metastore Configuring a Hive Metastore Configuring Hive Metastore access for an Oracle database Configuring Hive Metastore...", 
"body" : "Advanced instrumentation Advanced installation Configurations Cloud Creating an AWS RDS CloudWatch Alarm for free storage space HBase Configuring HBase Hive Connecting to a Hive Metastore Configuring a Hive Metastore Configuring Hive Metastore access for an Oracle database Configuring Hive Metastore read-only user permissions Configuring Microsoft JDBC to connect to Hive or HiverServer2 Obtaining Hive Metastore details Kafka Connecting to a Kafka cluster Kafka security Kerberos kerberos-changing-keytab Miscellaneous OnDemand Configuring migration and forecasting reports Configuring OnDemand Configuring small files report and files report Triggering an import of FSImage Secure UI Access Adding SSL and TLS to Unravel Web UI Enabling LDAP authentication for Unravel UI Enabling SAML authentication for Unravel Web UI Enabling TLS to Unravel Web UI directly Spark Configure Spark properties for Spark worker daemon @ Unravel Configuring notebooks for Spark Enable\/Disable live monitoring of Spark streaming apps Unravel Admins Adding more admins to Unravel Adding read-only admins to Unravel Changing Unravel admin's password User Interface Defining a custom banner Defining a custom Web UI port Disabling browser telemetry Disabling support\/comments panel Restricting direct access to Unravel UI Specifying a cluster ID or name Workflow Workflow Airflow Workflow Oozie Miscellaneous Backup and archive metric database Cluster wide report Starting, stopping, and configuring AutoAction daemon MySQL - Only applicable for Unravel v4.3.x and earlier Moving MySQL to a custom location Partitioning MySQL and migrating data REST API Unravel REST endpoints Swagger content Roles and Role Based Access Control Roles and Role Based Access Control RBAC roles Configuring RBAC general properties Configuring LDAP or SAML RBAC properties Example RBAC configurations Configuring RBAC general properties RBAC UI Tagging Tagging What is tagging? Tagging applications Tagging a Hive on Tez query Tagging workflows Unravel Properties Unravel properties, their definitions, and defaults. " }, 
{ "title" : "Advanced instrumentation options", 
"url" : "102192-instrument.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Advanced instrumentation options", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Enabling the JVM sensor", 
"url" : "102193-instrument-jvm.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Advanced instrumentation options \/ Enabling the JVM sensor", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Enabling the JVM sensor on HDP cluster-wide for MapReduce2 (MR)", 
"url" : "102194-instrument-jvm-hdp.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Advanced instrumentation options \/ Enabling the JVM sensor \/ Enabling the JVM sensor on HDP cluster-wide for MapReduce2 (MR)", 
"snippet" : "unravel-host must be a fully qualified DNS or an IP address. Upon completion you must restart of all affected HDFS, MAPREDUCE2, YARN and HIVE services in Ambari UI. In AWU, on the left-hand side, click MapReduce2 | Configs | Advanced tab, and select Advanced mapred-site . Search for MR AppMaster Jav...", 
"body" : "unravel-host must be a fully qualified DNS or an IP address. Upon completion you must restart of all affected HDFS, MAPREDUCE2, YARN and HIVE services in Ambari UI. In AWU, on the left-hand side, click MapReduce2 | Configs | Advanced tab, and select Advanced mapred-site . Search for MR AppMaster Java Heap Size property and concatenate by copying and pasting the below property for current.yarn.app.mapreduce.am.command-opts MR JVM sensor. Be sure to leave a white space in-between the current and the following new property. -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= unravel-host :4043 On the top notification banner, click Save . In AWU, on the left-hand side, click MapReduce2 , next click Configs , go to the Advanced tab, and select\/click Custom mapred-site . Inside Custom mapred-site , click Add Property for MR JVM sensor as follows and use the bulk property add mode. On the top notification banner, click Save . You can manually edit \/etc\/hadoop\/conf\/mapred-site.xml without using AWU. -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= unravel-host :4043 Propagate the Unravel resource metrics sensor JAR onto all the servers in the cluster. If you have already run the unravel_hdp_setup.sh script to distribute sensor; you can skip the following steps. Open a ssh session to the Unravel gateway host server and use guided steps to unzip the Unravel MR JARs. With root or sudo access, change directory to \/usr\/local\/unravel-agent and run the command:s below cd \/usr\/local\/unravel-agent\ncurl http:\/\/localhost:3000\/hh\/unravel-agent-pack-bin.zip -o unravel-agent-pack-bin.zip\nunzip -d jars unravel-agent-pack-bin.zip Ensure you have already installed unzip , curl , and opened Unravel port 3000. Create a .tar file of the \/usr\/local\/unravel-agent directory, and propagate to all the servers in the HDP cluster. cd \/usr\/local\/\ntar -cvf unravel-agent.tar .\/unravel-agent Copy the unravel-agent.tar file to all your servers, and untar to \/usr\/local directory because when you untar unravel-agent directory will appear. Restart of all affected HDFS, MAPREDUCE2, YARN and HIVE services in Ambari UI. " }, 
{ "title" : "Adding additional Spark instrumentation", 
"url" : "102195-instrument-spark.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Advanced instrumentation options \/ Adding additional Spark instrumentation", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Monitoring individual Spark apps", 
"url" : "102196-instrument-individual-spark.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Advanced instrumentation options \/ Adding additional Spark instrumentation \/ Monitoring individual Spark apps", 
"snippet" : "This topic explains how to set up per-appmonitoring of Spark (also called \"dev mode\"). This is different from cluster-wide monitoring . To monitor individual Spark apps, you must submit them through spark-submit . The information here applies to Spark versions 1.3.x through 2.0.x. unravel-host must ...", 
"body" : "This topic explains how to set up per-appmonitoring of Spark (also called \"dev mode\"). This is different from cluster-wide monitoring . To monitor individual Spark apps, you must submit them through spark-submit . The information here applies to Spark versions 1.3.x through 2.0.x. unravel-host must be a fully qualified domain name or IP address. Get Unravel's Spark sensor. The sensor is included in the Unravel Server RPM installation. After installing the Unravel Server RPM on unravel-host , obtain the sensor either from the file system on the Unravel Server host ( \/usr\/local\/unravel\/webapps\/ROOT\/hh\/unravel-agent-pack-bin.zip ), or from http:\/\/ unravel-host :3000\/hh\/unravel-agent-pack-bin.zip . If you run Spark apps in YARN-cluster mode (default): Put the sensor on the host node(s) from which you will run spark-submit by first creating a destination directory that is readable by all users. We suggest that unravel-sensor-path be \/usr\/local\/unravel-spark . If spark-submit is used from a single client node: mkdir unravel-sensor-path \ncd unravel-sensor-path \nwget http:\/\/ unravel-host :3000\/hh\/unravel-agent-pack-bin.zip If spark-submit is used from multiple client nodes, copy the sensor .zip file to HDFS instead of copying it to every client node, and set UNRAVEL_SENSOR_PATH accordingly. For example, copy it to hdfs:\/\/\/tmp : mkdir unravel-sensor-path \ncd unravel-sensor-path \nwget http:\/\/ unravel-host :3000\/hh\/unravel-agent-pack-bin.zip\ncd unravel-sensor-path \nhdfs fs -copyFromLocal unravel-agent-pack-bin.zip \/tmp\nset UNRAVEL_SENSOR_PATH=\"hdfs:\/\/\/tmp\" Define spark.driver.extraJavaOptions and spark.executor.extraJavaOptions as part of your spark-submit command. Substitute your local values for: unravel-sensor-path : Parent directory of the Unravel Sensor .zip file, unravel-agent-pack-bin.zip . If you put this file on HDFS, unravel-sensor-path is the parent directory on HDFS. unravel-host-ip-port : IP address and port of the unravel_lr service in the format ip:port . The default port is 4043. Sample value: 10.0.0.142:4043 . spark-event-log-dir : Location of the event log directory on HDFS, S3, or local file system. If a remote address is used, include the name node IP address and port. spark-sample-jar-path : Absolute path to the jar file used in the spark-submit command. spark-version : Spark version to be instrumented. Valid options are 1.3 for Spark 1.3.x, 1.5 for Spark 1.5.x, 1.6 for Spark 1.6.x, and 2.0 for Spark 2.0.x. export UNRAVEL_SENSOR_PATH= unravel-sensor-path \nexport UNRAVEL_SERVER_IP_PORT= unravel-host-ip-port \nexport SPARK_EVENT_LOG_DIR= spark-event-log-dir \nexport PATH_TO_SPARK_EXAMPLE_JAR= spark-sample-jar-path \nexport SPARK_VERSION= spark-version \n\nexport ENABLED_SENSOR_FOR_DRIVER=\"spark.driver.extraJavaOptions=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=spark-$SPARK_VERSION,config=driver\"\n\nexport ENABLED_SENSOR_FOR_EXECUTOR=\"spark.executor.extraJavaOptions=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=spark-$SPARK_VERSION,config=executor\"\n\n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --master yarn-cluster \\\n --archives $UNRAVEL_SENSOR_PATH\/unravel-agent-pack-bin.zip \\\n --conf \"$ENABLED_SENSOR_FOR_DRIVER\" \\\n --conf \"$ENABLED_SENSOR_FOR_EXECUTOR\" \\\n --conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n --conf \"spark.eventLog.dir=${SPARK_EVENT_LOG_DIR}\" \\\n --conf \"spark.eventLog.enabled=true\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR If you run Spark apps in YARN-client mode: To intercept Spark apps running in yarn-client mode, you need to unzip the Unravel Sensor .zip file on the client node at a location readable by all users, referred to as unzipped-archive-dest below. We suggest \/usr\/local\/unravel-spark . Important Please keep the original unravel-agent-pack-bin.zip file inside unzipped-archive-dest If you use multiple hosts as clients, on each client. mkdir unzipped-archive-dest \ncd unzipped-archive-dest \nwget http:\/\/ UNRAVEL_HOST_IP :3000\/hh\/unravel-agent-pack-bin.zip\nunzip unravel-agent-pack-bin.zip Define spark.executor.extraJavaOptions as part of your spark-submit command. To use the example below, substitute your local values for: unzipped-archive-dest : directory of the unzipped Unravel Sensor files. unravel-host-ip-port : IP address and port of the unravel_lr service in the format ip:port . Port is 4043 by default. Sample value: 10.0.0.142:4043 . spark-event-log-dir : Location of the event log directory on HDFS, S3, or local file system. If a remote address is used, include the namenode IP address and port. spark-sample-jar-path : Absolute path to the jar file used in the spark-submit command. spark-version : Spark version to be instrumented. Valid options are 1.3 for Spark 1.3.x, 1.5 for Spark 1.5.x, 1.6 for Spark 1.6.x, and 2.0 for Spark 2.0.x. export UNZIPPED_ARCHIVE_DEST= unzipped-archive-dest \nexport UNRAVEL_SERVER_IP_PORT= unravel-host-ip-port \nexport SPARK_EVENT_LOG_DIR= spark-event-log-dir \nexport PATH_TO_SPARK_EXAMPLE_JAR= spark-sample-jar-path \nexport SPARK_VERSION= spark-version \n\nexport ENABLED_SENSOR_FOR_EXECUTOR=\"spark.executor.extraJavaOptions=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=spark-$SPARK_VERSION,config=executor\"\n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --master yarn-client \\\n --archives $UNZIPPED_ARCHIVE_DEST\/unravel-agent-pack-bin.zip \\\n --driver-java-options \"-javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=config=driver,libs=spark-$SPARK_VERSION\" \\\n --conf \"$ENABLED_SENSOR_FOR_EXECUTOR\" \\\n --conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n --conf \"spark.eventLog.dir=${SPARK_EVENT_LOG_DIR}\" \\\n --conf \"spark.eventLog.enabled=true\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR " }, 
{ "title" : "Uploading Spark programs to Unravel", 
"url" : "102197-instrument-spark-upload.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Advanced instrumentation options \/ Adding additional Spark instrumentation \/ Uploading Spark programs to Unravel", 
"snippet" : "Unravel UI displays Spark programs you upload if they're submitted as Java, Scala or Python source code, not as JVM byte code. You can upload Spark programs in either either by uploading individual source files or by uploading a .zip file. Uploading individual source files Upload Spark source files ...", 
"body" : "Unravel UI displays Spark programs you upload if they're submitted as Java, Scala or Python source code, not as JVM byte code. You can upload Spark programs in either either by uploading individual source files or by uploading a .zip file. Uploading individual source files Upload Spark source files and specify their location on the spark-submit command. Examples In yarn-client mode , upload the source files to any local directory accessible to the application's driver, and specify their path with --conf \"spark.unravel.program.dir=$PROGRAM_DIR\" on the spark-submit command: export PROGRAM_DIR= fully-qualified-path-to-local-file-directory \nexport PATH_TO_SPARK_EXAMPLE_JAR= fully-qualified-jar-path \n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --files { comma-separated-list-of-source-files } \\\n --conf \"spark.unravel.program.dir=$PROGRAM_DIR\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR The default value of spark.unravel.program.dir is the current directory (the application's home directory). In yarn-cluster mode , upload the source files to the application's home directory, and specify their path with --files comma-separated-list-of-source-files on the spark-submit command: export PROGRAM_DIR= fully-qualified-path-to-local-file-directory \nexport PATH_TO_SPARK_EXAMPLE_JAR= fully-qualified-jar-path \n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --files { comma-separated-list-of-source-files } \\\n $PATH_TO_SPARK_EXAMPLE_JAR Uploading an archive Package all relevant source files into a zip archive. Keep the archive small by including only the relevant driver source files. Examples In yarn-client mode , upload the zip archive to any local directory accessible to the application's driver, and specify its path with --conf \"spark.unravel.program.zip=$SRC_ZIP_PATH\" on the spark-submit command: export PROGRAM_DIR= fully-qualified-zip-path \nexport SRC_ZIP_PATH=$PROGRAM_DIR\/ src-zip-name \nexport PATH_TO_SPARK_EXAMPLE_JAR= fully-qualified-jar-path \n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --conf \"spark.unravel.program.zip=$SRC_ZIP_PATH\" \\\n $PATH_TO_SPARK_EXAMPLE_JAR In yarn-cluster mode , upload the zip archive to the application's home directory, and specify its path with --files $SRC_ZIP_PATH and its filename with --conf \"spark.unravel.program.zip= src-zip-name \" on the spark-submit command: export PROGRAM_DIR= fully-qualified-zip-path \nexport SRC_ZIP_PATH=$PROGRAM_DIR\/ src-zip-name \nexport PATH_TO_SPARK_EXAMPLE_JAR= fully-qualified-jar-path \n\nspark-submit \\\n --class org.apache.spark.examples.sql.RDDRelation \\\n --files $SRC_ZIP_PATH \\\n --conf \"spark.unravel.program.zip= src-zip-name \" \\\n $PATH_TO_SPARK_EXAMPLE_JAR Unravel searches for source files in this order: spark.unravel.program.dir (Option 1) Application home directory (Option 1) Zip archive provided as spark.unravel.program.zip (Option 2) After the Spark application has completed, you can see the Spark program(s) in Unravel UI under Spark Application Manger Program tab . When you click an RDD node, Unravel UI highlights the line of code corresponding to the execution graph of that RDD node. For example, in the screenshot below, Unravel UI highlights the MapPartitionsRDD node at line 324 of QueryDriver.scala . " }, 
{ "title" : "Integrating with Informatica Big Data Management", 
"url" : "102198-instrument-informatica.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Advanced instrumentation options \/ Integrating with Informatica Big Data Management", 
"snippet" : "Informatica Big Data Management (BDM) suite enables enterprises to deploy advanced data management capabilities, including data ingestion, data quality, data masking, and stream processing. BDM provides a graphical user interface for generating ETL mappings (jobs) and workflows for various framework...", 
"body" : "Informatica Big Data Management (BDM) suite enables enterprises to deploy advanced data management capabilities, including data ingestion, data quality, data masking, and stream processing. BDM provides a graphical user interface for generating ETL mappings (jobs) and workflows for various frameworks, such as Spark, Hive on MapReduce, or Hive on Tez. You install BDM on an edge node in your Hadoop cluster, and use it to create jobs and workflows that ingest data into your cluster. The BDM interface allows you to construct jobs graphically, by connecting data sources to mappings to data destinations; BDM creates Hive or Spark queries for you, which removes the need for you to know Hive or Spark programming. However, while a non-programmatic interface is convenient, the jobs BDM generates might need tweaking to improve their performance or to meet your SLAs. Optimizing these BDM-generated data workflows can be complex and a resource drain for many customers. This is where Unravel comes in. The Unravel UI shows more information for each BDM job or workflow than YARN or your cluster manager does. This topic explains how to connect Unravel to your BDM jobs and workflows, so that Unravel can provide operational insights, recommendations, and automation to maximize their performance. You've already installed Informatica BDM on an edge node in your cluster. You've already installed Unravel Server on an edge node in your cluster. Your cluster is on-premises CDH\/HDP\/MapR, or cloud-based Amazon EMR or Microsoft Azure HDInsight. " }, 
{ "title" : "Setting properties in each mapping and workflow", 
"url" : "102198-instrument-informatica.html#UUID-d89e6ec3-9970-f88d-ab5c-7254ae415a71_id_AmazonAthena-IntegratingAthenawithUnravel", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Advanced instrumentation options \/ Integrating with Informatica Big Data Management \/ Setting properties in each mapping and workflow", 
"snippet" : "These steps enable Unravel UI to display your BDM workflows and jobs. For each BDM mapping (job): Log into the BDM graphical user interface. Select a mapping. Open the mapping's Properties view. In the Parameters tab, create two parameters: Create a parameter named UTCTimeStamp of Type string, with ...", 
"body" : "These steps enable Unravel UI to display your BDM workflows and jobs. For each BDM mapping (job): Log into the BDM graphical user interface. Select a mapping. Open the mapping's Properties view. In the Parameters tab, create two parameters: Create a parameter named UTCTimeStamp of Type string, with Precision set to 1000, Scale set to 0 and Default Value set to Default. Create a parameter named workflowName of Type string, with Precision set to 1000, Scale set to 0 and Default Value set to Default. Set up workflow tags . In the Run-time tab, map the two parameters you created to Unravel's workflow properties: Expand the Hadoop label and edit Runtime Properties : For Hive-on-Spark, create the Unravel property spark.unravel.workflow.name ; for Hive-on-MR or Hive-on-Tez, create unravel.workflow.name . Set the Unavel property's value to the BDM job's workflowName parameter. For Hive-on-Spark, create the Unravel property spark.unravel.workflow.utctimestamp ; for Hive-on-MR or Hive-on-Tez, create unravel.workflow.utctimestamp . Set the Unavel property's value to the BDM job's UTCTimeStamp parameter. For each BDM workflow: Log into the BDM graphical user interface. Select a workflow. Open the workflow's Properties view. In the Input tab, set values for the two parameters you created in each mapping: Click Mapping Parameter Inputs | Unravel . Set UTCTimeStamp to the system variable sys.InstanceID . Set workflowName to the system variable sys.WorkflowName . " }, 
{ "title" : "Enabling additional instrumentation from BDM", 
"url" : "102198-instrument-informatica.html#UUID-d89e6ec3-9970-f88d-ab5c-7254ae415a71_id_AmazonAthena-CreateaTrailinAWSCloudTrail", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Advanced instrumentation options \/ Integrating with Informatica Big Data Management \/ Enabling additional instrumentation from BDM", 
"snippet" : "To get even more instrumentation from BDM, follow these steps on the BDM host --in other words, the edge node that's running BDM. Log into your BDM console as administrator. In the BDM console, select Manage | Connections . Put Unravel's Hive Hook JAR in AUX_CLASSPATH. In Domain Navigator , expand |...", 
"body" : "To get even more instrumentation from BDM, follow these steps on the BDM host --in other words, the edge node that's running BDM. Log into your BDM console as administrator. In the BDM console, select Manage | Connections . Put Unravel's Hive Hook JAR in AUX_CLASSPATH. In Domain Navigator , expand | Domain | ClusterConfigurations | HADOOP_hdp_cco . Edit Hive Pushdown Configuration : Set Engine Type to Hive or Tez . Set Advanced Properties to: AUX_CLASSPATH=${AUX_CLASSPATH}:\/usr\/local\/unravel_client\/unravel-hive-1.2.0-hook.jar Deploy Unravel's Spark JAR. In Domain Navigator , expand | Domain | ClusterConfigurations | HADOOP_hdp_cco . Under Spark Configuration , set Spark Event Log Directory to hdfs:\/\/\/spark2-history\/ Under Spark Configuration , append Unravel's Spark sensors to the following properties in Advanced Properties : spark.unravel.server.hostport= unravel-host :4043\n\nspark.driver.extraJavaOptions=-javaagent: path-to-btrace-agent.jar =config=driver,libs=spark-version\n\nspark.executor.extraJavaOptions=-javaagent: path-to-btrace-agent.jar =config=executor,libs=spark-versionspark.eventLog.enabled=true Connect to the Hive metastore to enable Unravel's Hive hooks. In Domain Navigator , expand | Domain | ClusterConfigurations | your deployment, such as HDP_CCO for HDP| hive_site_xml , and click Edit . In the Edit hive_site_xml dialog, set the following properties: com.unraveldata.hive.hook.tcp = true com.unraveldata.hive.hdfs.dir = \/user\/unravel\/HOOK_RESULT_DIR com.unraveldata.host = unravel-host hive.exec.pre.hooks = com.unraveldata.dataflow.hive.hook.unravel hive.exec.post.hooks = com.unraveldata.dataflow.hive.hook.unravel hive.exec.failure.hooks = com.unraveldata.dataflow.hive.hook.unravel In Domain Navigator , expand | Domain | ClusterConfigurations | your deployment, such as HDP_CCO for HDP| tez_site_xml , and click Edit . In tez.am.launch.cmd-opts , add a space and append this line to the existing values: -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr,config=tez -Dunravel.server.hostport= unravel-host :4043 In tez.task.launch.cmd-opts , add a space and append this line to the existing values: -javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=mr,config=tez -Dunravel.server.hostport= unravel-host :4043 " }, 
{ "title" : "Viewing BDM jobs and workflows in Unravel UI", 
"url" : "102198-instrument-informatica.html#UUID-d89e6ec3-9970-f88d-ab5c-7254ae415a71_id_AmazonAthena-ViewAthenaQueriesinUnravelUI", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Advanced instrumentation options \/ Integrating with Informatica Big Data Management \/ Viewing BDM jobs and workflows in Unravel UI", 
"snippet" : "In Unravel UI, look at Applications | Workflows . For more information, see the User Guide ....", 
"body" : "In Unravel UI, look at Applications | Workflows . For more information, see the User Guide . " }, 
{ "title" : "Monitoring individual Hive queries", 
"url" : "102199-instrument-individual-hive.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Advanced instrumentation options \/ Monitoring individual Hive queries", 
"snippet" : "The Unravel JVM sensor is a prepackaged distribution of JVM agent which enables collection of additional information, including resource usage metrics. The sensor binary is distributed as unravel-agent-pack-bin.zip . unravel-host is your Unravel gateway server. Port 4043 is where Unravel LR server i...", 
"body" : "The Unravel JVM sensor is a prepackaged distribution of JVM agent which enables collection of additional information, including resource usage metrics. The sensor binary is distributed as unravel-agent-pack-bin.zip . unravel-host is your Unravel gateway server. Port 4043 is where Unravel LR server is running. When you enable this sensor, it uses the standard MapReduce profiling extension. You can copy and paste the following configuration snippets for a quick bootstrap Option A: Installing the MapReduce JVM sensor on CDH Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5;\nset mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: set mapreduce.task.profile.params=- javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= unravel-host :4043; Enable the JVM agent for application master: set yarn.app.mapreduce.am.command-opts=- javaagent:\/opt\/cloudera\/parcels\/UNRAVEL_SENSOR\/lib\/java\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= unravel-host :4043; Option B: Installing the MapReduce JVM sensor on Cloudera Manager for Hive See Optional - Configure YARN - MapReduce (MR) JVM Sensor Cluster-Wide in Step 2: Install Unravel Sensor and Configure Impala . Option C: Installing the MapReduce JVM sensor on HDFS Change your *init file as follows: Set the path to the JVM sensor archive. set mapreduce.job.cache.archives=path_in_hdfs\/unravel-agent-pack-bin.zip; Enable profiling: set mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5; \nset mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: set mapreduce.task.profile.params=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= unravel-host :4043; Enable the JVM agent for application master: set yarn.app.mapreduce.am.command-opts=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= unravel-host :4043; Option D: Installing the MapReduce JVM sensor on the local file system Add the JVM sensor archive to the PATH environment variable. Enable profiling: set mapreduce.task.profile=true;\nset mapreduce.task.profile=true;\nset mapreduce.task.profile=true; Select map and reduce profiles: set mapreduce.task.profile.maps=0-5; set mapreduce.task.profile.reduces=0-5;\nset mapreduce.task.profile.maps=0-5; set mapreduce.task.profile.reduces=0-5;\nset mapreduce.task.profile.maps=0-5; set mapreduce.task.profile.reduces=0-5;\nset mapreduce.task.profile.maps=0-5; set mapreduce.task.profile.reduces=0-5; Enable the JVM agent for map and reduce tasks: set mapreduce.task.profile.params=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= unravel-host :4043; Enable the JVM agent for application master: set yarn.app.mapreduce.am.command-opts=-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=mr -Dunravel.server.hostport= unravel-host :4043; " }, 
{ "title" : "Advanced installation options", 
"url" : "102200-install-extra.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Advanced installation options", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Modifying the behavior of unravel_es", 
"url" : "102202-install-unravel-es.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Advanced installation options \/ Modifying the behavior of unravel_es", 
"snippet" : "On the Unravel host, edit the unravel_es daemon's configuration files, as described below. Available options vary by release and platform. For Amazon EMR Unravel 4.5.2.1-4.5.2.2 Property\/ Property File Description Default am-polling \/usr\/local\/unravel_es\/etc\/unravel_es.properties Enables \"applicatio...", 
"body" : "On the Unravel host, edit the unravel_es daemon's configuration files, as described below. Available options vary by release and platform. For Amazon EMR Unravel 4.5.2.1-4.5.2.2 Property\/ Property File Description Default am-polling \/usr\/local\/unravel_es\/etc\/unravel_es.properties Enables \"application master\" metrics polling for auto actions. Valid values: true , false . false enable-aa \/usr\/local\/unravel_es\/etc\/unravel_es.properties Enables the auto action feature. Valid values: true , false . true hive-id-cache \/usr\/local\/unravel_es\/etc\/unravel_es.properties Maximum number of jobs you expect to have on the cluster. 1000 metrics-factor \/usr\/local\/unravel_es\/etc\/unravel_es.properties Specifies the interval at which Unravel sensors push data from the EMR cluster nodes to Unravel Server. interval is in units of 5 seconds. In other words, a value of 1 means 5 seconds, 2 means 10 seconds, and so on. 1 com.unraveldata.kerberos.keytab.path \/usr\/local\/unravel_es\/etc\/unravel.properties Kerberos settings Path to the kerberos keytab file that will be used to run kinit . \/etc\/hadoop.keytab com.unraveldata.kerberos.principal \/usr\/local\/unravel_es\/etc\/unravel.properties Kerberos settings Kerberos principal name that will be used to run kinit .. hadoop\/ master-node-ip @ realm yarn.resourcemanager.webapp.password \/usr\/local\/unravel_es\/etc\/unravel.properties YARN resource manager web UI password. a yarn.resourcemanager.webapp.username \/usr\/local\/unravel_es\/etc\/unravel.properties YARN resource manager web UI username. a Property\/ Property File Description Default group-id \/usr\/local\/unravel\/etc\/unravel_ctl Run-as groupname for Unravel daemons. hadoop user-id \/usr\/local\/unravel\/etc\/unravel_ctl Run-as username for Unravel daemons. For a non-secured cluster, the default username is hdfs . For a kerberized cluster, the default is unravel . Unravel 4.5.2.0 Property\/ Property File Description Default am-polling \/usr\/local\/unravel_es\/etc\/unravel_es.properties Enables \"application master\" metrics polling for auto actions. Valid values: true , false . false enable-aa \/usr\/local\/unravel_es\/etc\/unravel_es.properties Enables the auto action feature. Valid values: true , false . true hive-id-cache \/usr\/local\/unravel_es\/etc\/unravel_es.properties Maximum number of jobs you expect to have on the cluster. 1000 metrics-factor \/usr\/local\/unravel_es\/etc\/unravel_es.properties Specifies the interval at which Unravel sensors push data from the EMR cluster nodes to Unravel Server. interval is in units of 5 seconds. In other words, a value of 1 means 5 seconds, 2 means 10 seconds, and so on. 1 For Azure HDInsight Unravel 4.5.2.1-4.5.2.2 Property\/ Property File Description Default am-polling \/usr\/local\/unravel_es\/etc\/unravel_es.properties Enables \"application master\" metrics polling for auto actions. Valid values: true , false . false enable-aa \/usr\/local\/unravel_es\/etc\/unravel_es.properties Enables the auto action feature. Valid values: true , false . true hive-id-cache \/usr\/local\/unravel_es\/etc\/unravel_es.properties Maximum number of jobs you expect to have on the cluster. 1000 com.unraveldata.kerberos.keytab.path \/usr\/local\/unravel_es\/etc\/unravel.properties Kerberos settings Path to the kerberos keytab file that will be used to run kinit . \/etc\/security\/keytabs\/ambari.server.keytab com.unraveldata.kerberos.principal \/usr\/local\/unravel_es\/etc\/unravel.properties Kerberos settings Kerberos principal name that will be used to run kinit . ambari-server- cluster-name @ realm yarn.resourcemanager.webapp.password \/usr\/local\/unravel_es\/etc\/unravel.properties YARN resource manager web UI password. a yarn.resourcemanager.webapp.username \/usr\/local\/unravel_es\/etc\/unravel.properties YARN resource manager web UI username. a Property\/ Property File Description Default group-id \/usr\/local\/unravel\/etc\/unravel_ctl Run-as groupname for Unravel daemons. unravel user-id \/usr\/local\/unravel\/etc\/unravel_ctl Run-as username for Unravel daemons. unravel Unravel 4.5.2.0 Property\/ Property File Description Default am-polling \/usr\/local\/unravel_es\/etc\/unravel_es.properties Enables \"application master\" metrics polling for auto actions. Valid values: true , false . false enable-aa \/usr\/local\/unravel_es\/etc\/unravel_es.properties Enables the auto action feature. Valid values: true , false . true hive-id-cache \/usr\/local\/unravel_es\/etc\/unravel_es.properties Maximum number of jobs you expect to have on the cluster. 1000 For Google Cloud Dataproc Unravel 4.5.2.2 Property\/Property Files Description Default am-polling \/usr\/local\/unravel_es\/etc\/unravel_es.properties Enables \"application master\" metrics polling for auto actions. Valid values: true , false . false enable-aa \/usr\/local\/unravel_es\/etc\/unravel_es.properties Enables the auto action feature. Valid values: true , false . true hive-id-cache \/usr\/local\/unravel_es\/etc\/unravel_es.properties Maximum number of jobs you expect to have on the cluster. 1000 metrics-factor \/usr\/local\/unravel_es\/etc\/unravel_es.properties Specifies the interval at which Unravel sensors push data from the EMR cluster nodes to Unravel Server. interval is in units of 5 seconds. In other words, a value of 1 means 5 seconds, 2 means 10 seconds, and so on. 1 com.unraveldata.kerberos.keytab.path \/usr\/local\/unravel_es\/etc\/unravel.properties Kerberos settings Path to the kerberos keytab file that will be used to run kinit . \/etc\/hadoop.keytab com.unraveldata.kerberos.principal \/usr\/local\/unravel_es\/etc\/unravel.properties Kerberos settings Kerberos principal name that will be used to run kinit .. hadoop\/ master-node-ip @ realm yarn.resourcemanager.webapp.password \/usr\/local\/unravel_es\/etc\/unravel.properties YARN resource manager web UI password. a yarn.resourcemanager.webapp.username \/usr\/local\/unravel_es\/etc\/unravel.properties YARN resource manager web UI username. a Property\/Property File Description Default group-id \/usr\/local\/unravel\/etc\/unravel_ctl Run-as groupname for Unravel daemons. hadoop user-id \/usr\/local\/unravel\/etc\/unravel_ctl Run-as username for Unravel daemons. For a non-secured cluster, the default username is hdfs . For a kerberized cluster, the default is unravel . " }, 
{ "title" : "Deploying Unravel on security-enhanced Linux", 
"url" : "102203-install-selinux.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Advanced installation options \/ Deploying Unravel on security-enhanced Linux", 
"snippet" : "This topic explains how to deploy Unravel over Security Enhanced Linux (SELinux). SELinux allows you to set access control through SELinux policies. SELinux modes Enforcing : The SELinux polices and rules are strictly enforced and applied over the subjects and object. All production systems have SEL...", 
"body" : "This topic explains how to deploy Unravel over Security Enhanced Linux (SELinux). SELinux allows you to set access control through SELinux policies. SELinux modes Enforcing : The SELinux polices and rules are strictly enforced and applied over the subjects and object. All production systems have SELinux enabled in enforcing mode. The policies are enforced whenever any violations or errors are detected and the violations\/errors are logged. Permissive : The policies and rules of SELinux are applied over the subjects and objects but are not enforced. All violations and errors based on the SELinux policy are ignored and logged into the log files. If the SELinux policy prevents a specific service from accessing a specific folder, this mode allows access but logs a denial message. This mode provides enough debugging information to fine tune the SELinux Policy so it runs smoothly in enforcing mode. Disabled : No policies are enforced. SELinux policies Unravel currently only supports the targeted policy. Prerequisites Enable SELinux on Unravel Node running Linux. In \/etc\/sysconfig\/selinux , specify the following settings: Set the mode to enforcing . This is SELinux's default; whenever the system reboots it starts SELinux in this mode. For instructions on changing the mode while running, see . SELINUX=enforcing Use the default policy, targeted . SELINUXTYPE=targeted Reboot the system to make changes take effect. getenforce\nenforcing Verify the SELinux mode setting after reboot. Installing the Unravel RPM on a SELinux-enabled node Install Unravel in permissive mode or enforcing mode. You can install Unravel in either mode. However, installing Unravel in enforcing mode is highly discouraged since SELinux issues a warning regarding uncertainty of functionality. Installing in permissive mode (recommended) Set mode to permissive and verify setting. setenforce 0\ngetenforce\npermissive Install the Unravel RPM. sudo rpm -Uvv unravel- version .x86_64.rpm 2 > \/tmp\/rpm.txt\nsudo \/usr\/local\/unravel\/install_bin\/await_fixups.sh SELinux may generate similar alerts during the installation process depending on the environment. But this should not hinder with the installation process. sealert -a \/var\/log\/audit\/audit.log\nAlert 1: SELinux is preventing \/usr\/bin\/bash from using the rlimitinh access on a process.\nAlert 2: SELinux is preventing \/usr\/bin\/python2.7 from using the rlimitinh access on a process. Installing in enforcing mode (highly discouraged) When Unravel is installed in enforcing mode, SELinux issues a warning regarding uncertainty of functionality. Execute getenforce command to check if Unravel SELinux node is in enforcing mode. If it is not, go to Step b above. getenforce\nenforcing Install Unravel using rpm . sudo rpm -Uvv unravel- version .x86_64.rpm 2 > \/tmp\/rpm.txt\nsudo \/usr\/local\/unravel\/install_bin\/await_fixups.sh The rpm installation sets SELINUX to permissive and issues a security warning: -----RPM installation log\n+ setenforce Permissive\n+ echo\n+ tee_echo '[CREATE_B1: SECURITY: WARNING] Setting selinux to be temporarily Permissive; after a reboot it might revert to Enforced and Unravel functionality might be an issue.'\n+ tee -a \/tmp\/rpm_upgrade.log\n++ date '+%Y-%m-%d %H:%M:%S'\n + echo '[2019-01-28 06:33:17] [CREATE_B1: SECURITY: WARNING] Setting selinux to be temporarily Permissive; after a reboot it might revert to Enforced and Unravel functionality might be an issue.' \n+ echo\n+ FILE_CACHE_HEADROOM_MB=2000\n----- getenforce\npermissive SELinux generates two alerts like the ones below. Similiar alerts are generated throughout the installation process. sealert -a \/var\/log\/audit\/audit.log\nAlert 1: SELinux is preventing \/usr\/bin\/bash from using the rlimitinh access on a process.\nAlert 2: SELinux is preventing \/usr\/bin\/python2.7 from using the rlimitinh access on a process. Switch to user. There should be no alerts at this stage. Set SELINUX to enforcing and verify it's been set. setenforce 1\ngetenforce\nenforcing Run the script switch_to_user.sh where user and group depend on your environment. For more information, see switch_to_user . sudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh user group Start Unravel services after RPM installation. Run the following command to make sure all services start up successfully. sudo \/etc\/init.d\/unravel_all.sh start SELinux generates two alerts. Similar alerts are generated throughout the installation process. sealert -a \/var\/log\/audit\/audit.log\nAlert 1: SELinux is preventing \/usr\/bin\/bash from using the rlimitinh access on a process.\nAlert 2: SELinux is preventing \/usr\/bin\/python2.7 from using the rlimitinh access on a process. Verify that SELinux is set to enforcing . getenforce\nenforcing If getenforce returned permissive , execute the following commands to set SELINUX to enforcing mode. sudo \/etc\/init.d\/unravel_all.sh stop\nsetenforce 0\nsudo \/etc\/init.d\/unravel_all.sh start Configure Unravel Server and install sensors. Substitute your fully qualified domain name or your host's IP for UNRAVEL_HOST . There should be no alerts generated at this stage. python \/usr\/local\/unravel\/install_bin\/cluster-setup-scripts\/unravel_cdh_setup.py --spark-version 2.3.0 --unravel-server UNRAVEL_HOST --cm-server {UNRAVEL_HOST} --all Troubleshooting Run these commands to check for alerts, denials, or policy violations which might happen after an installation or an operation\/job submission to see if any violations have occurred. To view any Unravel specific alerts: sealert -a \/var\/log\/audit\/audit.log | grep unravel To view any system specific alerts: sealert -a \/var\/log\/audit\/audit.log Installing and using SELinux tools yum install setroubleshoot setools\nyum install policycoreutils policycoreutils-python selinux-policy selinux-policy-targeted libselinux-utils setroubleshoot setools setools-console These tools help you get more information about the policy and analyze the avc log file generated by SELinux. Use seinfo to identify the loaded SELinux Policy. Your output should look similar to the sample below. seinfo\nStatistics for policy file: \/sys\/fs\/selinux\/policy\nPolicy Version & Type: v.28 (binary, mls)\n\nClasses: 94 Permissions: 262 \nSensitivities 1 Categories: 1024\nTypes: 4747 Attributes: 251\nUsers: 8 Roles: 14\nBooleans: 307 Cond. Expr.: 56\nAllow: 101746 Neverallow: 0\nAuditallow: 155 Dontaudit: 8846\nType_trans: 17759 Type_change: 74\nType_member: 35 Role allow: 39\nRole_trans: 416 Range_trans: 5697\nConstraints: 109 Validatetrans: 0\nInitial SIDs: 27 Fs_use: 29\nGenfscon: 105 Portcon: 602\nNetifcon: 0 Nodecon: 0\nPermissives: 6 Polcap: 2 Use semodule to log even the trivial violations logged by SELinux. semodule -DB Use sealert to see alerts. Enter the following command to see all the alerts generated by SELinux sealert -a \/var\/log\/audit\/audit.log Enter the following command to see Unravel specific alerts sealert -a \/var\/log\/audit\/audit.log | grep unravel For debugging (in other words, if you're testing in enforcement mode), run the following commands: Log all trivial violations logged by SELinux. semodule -DB Set the audit log file to 0 so you get to know of access violations happening during the testing of enforcement mode. > \/var\/log\/audit\/audit.log Installing MySQL in enforcing mode If the datadir is changed, for example to \/srv\/unravel\/db_data as described in MySQL installation , Unravel SELinux node throws alerts upon installation and configuration of MySQL. If you see these alerts, you must create a new policy to handle the change. A sample policy is below. During Configure and Start MySQL Server configuration The following alert is thrown when starting mysqld daemon (Step 5) after setting datadir=\/srv\/unravel\/db_data . Alert : If you believe that mysqld should be allowed read access on the plugin.frm file by default During Configure Unravel to Connect My SQL Server The following alert is thrown while creating the database (Step 1). Alert : If you believe that mysqld should be allowed create access on the ibdata1 file by default. The following alert is thrown when creating the schema for Unravel (Step 3). Alert : If you believe that mysqld should be allowed remove_name access on the edge-4.lower-test directory by default. Sample policy module my-mysqld 1.0;\nrequire {\ntype mysqld_safe_t;\ntype var_t;\ntype mysqld_t;\nclass process siginh;\nclass dir { add_name create remove_name write };\nclass file { create getattr lock open read rename unlink write };\n}\n#============= mysqld_safe_t ==============\n#!!!! This avc is allowed in the current policy\nallow mysqld_safe_t mysqld_t:process siginh;\n#============= mysqld_t ==============\n#!!!! This avc is allowed in the current policy\nallow mysqld_t var_t:dir { add_name create remove_name write };\nallow mysqld_t var_t:file rename;\n#!!!! This avc is allowed in the current policy\nallow mysqld_t var_t:file { create getattr lock open read unlink write }; " }, 
{ "title" : "Working with modes", 
"url" : "102203-install-selinux.html#UUID-f021ff3b-8370-f113-75d8-bf48a7e7498a_N1558744556203", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Advanced installation options \/ Deploying Unravel on security-enhanced Linux \/ Working with modes", 
"snippet" : "Determining which mode SELinux is running in Retrieve the current SELinux mode. Output is permissive or enforcing , depending on the setting. getenforce Switching modes You can switch modes on the fly using the setenforce command. When Unravel is restarted SELinux returns to the default mode set in ...", 
"body" : "Determining which mode SELinux is running in Retrieve the current SELinux mode. Output is permissive or enforcing , depending on the setting. getenforce Switching modes You can switch modes on the fly using the setenforce command. When Unravel is restarted SELinux returns to the default mode set in \/etc\/sysconfig\/selinux . To set permissive mode setenforce 0 To set enforcement mode setenforce 1 " }, 
{ "title" : "Enabling multiple daemons for high-volume data", 
"url" : "102204-install-multiple-daemons.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Advanced installation options \/ Enabling multiple daemons for high-volume data", 
"snippet" : "These instructions apply to single-host Unravel deployments only; for multi-host deployments, please contact Unravel Support . Based upon your workload run one set of commands. For 10000-20000 jobs per day: sudo chkconfig --add unravel_ew_2 sudo chkconfig --add unravel_jcw2_2 sudo chkconfig --add un...", 
"body" : "These instructions apply to single-host Unravel deployments only; for multi-host deployments, please contact Unravel Support . Based upon your workload run one set of commands. For 10000-20000 jobs per day: sudo chkconfig --add unravel_ew_2 \nsudo chkconfig --add unravel_jcw2_2\nsudo chkconfig --add unravel_sw_2 \nsudo chkconfig --add unravel_ma_2 For 20000-30000 jobs per day: sudo chkconfig --add unravel_ew_3 \nsudo chkconfig --add unravel_jcw2_3\nsudo chkconfig --add unravel_sw_3\nsudo chkconfig --add unravel_ma_3 For more than than 30000 jobs per day: sudo chkconfig --add unravel_ew_4 \nsudo chkconfig --add unravel_jcw2_4\nsudo chkconfig --add unravel_sw_4\nsudo chkconfig --add unravel_ma_4 Start the new daemons. sudo \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "Running verification scripts and benchmarks", 
"url" : "102206-install-verification-benchmarks.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Advanced installation options \/ Running verification scripts and benchmarks", 
"snippet" : "This topic explains how to run verification tests and benchmarks after you install or upgrade Unravel Server....", 
"body" : "This topic explains how to run verification tests and benchmarks after you install or upgrade Unravel Server. " }, 
{ "title" : "Why run verification tests or benchmarks?", 
"url" : "102206-install-verification-benchmarks.html#UUID-3f13b3cb-53ef-ca8d-d7c0-3f154afc7a57_id_RunningVerificationScriptsandBenchmarks-WhyRunVerificationTestsorBenchmarks", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Advanced installation options \/ Running verification scripts and benchmarks \/ Why run verification tests or benchmarks?", 
"snippet" : "Verification tests highlight the value of Unravelâ€™s application performance management\/analysis. Benchmarks verify that Unravel features are working correctly....", 
"body" : "Verification tests highlight the value of Unravelâ€™s application performance management\/analysis. Benchmarks verify that Unravel features are working correctly. " }, 
{ "title" : "Running verification tests", 
"url" : "102206-install-verification-benchmarks.html#UUID-3f13b3cb-53ef-ca8d-d7c0-3f154afc7a57_id_RunningVerificationScriptsandBenchmarks-RunningVerificationTestsSmokeTests", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Advanced installation options \/ Running verification scripts and benchmarks \/ Running verification tests", 
"snippet" : "Unravel provides verification tests for Spark jobs only. Follow the instructions in the section that matches your deployment. CDH On your Unravel Server host, run the spark_test_via_parcel.sh script. This script runs a Spark app. Itâ€™s a good way to verify that Unravel Server captures the data (eve...", 
"body" : "Unravel provides verification tests for Spark jobs only. Follow the instructions in the section that matches your deployment. CDH On your Unravel Server host, run the spark_test_via_parcel.sh script. This script runs a Spark app. Itâ€™s a good way to verify that Unravel Server captures the data (events) generated by the Spark app, after the sensor jars have been deployed to the cluster via the parcel, i.e., Installing the Unravel Parcel on CDH+CM . You should be able to see the data generated by this Spark app on Unravel Web UI. Substitute the host name or LAN IP address of Unravel Server for unravel-host \/usr\/local\/unravel\/install_bin\/spark_test_via_parcel.sh --unravel-server unravel-host Note: You can run this script before configuring the Gateway Automatic Deployment of Spark Instrumentation which instruments the Spark configuration file \"spark-defaults.conf\". After you configure the Gateway Automatic Deployment of Spark Instrumentation which instruments \"spark-defaults.conf\", run a SparkPI job on your Unravel Server host to verify that the sensor is installed and configured correctly: \/opt\/cloudera\/parcels\/CDH\/lib\/spark\/bin\/spark-submit --class org.apache.spark.examples.SparkPi --deploy-mode client --master yarn \/opt\/cloudera\/parcels\/CDH\/lib\/spark\/examples\/lib\/spark-examples-*.jar 1000 HDP After you install Unravel Sensor for Spark , run a SparkPI job on your Unravel Server host to verify that the sensor is installed and configured correctly: \/usr\/bin\/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --num-executors 1 --driver-memory 512m --executor-memory 512m --executor-cores 1 \/usr\/hdp\/current\/spark-client\/lib\/spark-examples*.jar 10 MapR After you install Unravel Sensor for Spark , run a SparkPI job on your Unravel Server host to verify that the sensor is installed and configured correctly: \/opt\/mapr\/spark\/spark-1.6.1\/bin\/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --num-executors 1 --driver-memory 512m --executor-memory 512m --executor-cores 1 \/opt\/mapr\/spark\/spark-1.6.1\/lib\/spark-examples*.jar 10 " }, 
{ "title" : "Running benchmarks", 
"url" : "102206-install-verification-benchmarks.html#UUID-3f13b3cb-53ef-ca8d-d7c0-3f154afc7a57_id_RunningVerificationScriptsandBenchmarks-RunningBenchmarks", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Advanced installation options \/ Running verification scripts and benchmarks \/ Running benchmarks", 
"snippet" : "We provide sample Spark apps that you can download from preview.unraveldata.com . These apps are useful for verifying that an upgrade is successful. Please follow the instructions below for the app you want. Spark Available benchmark packages Package name Location Benchmarks 1.6.x https:\/\/preview.un...", 
"body" : "We provide sample Spark apps that you can download from preview.unraveldata.com . These apps are useful for verifying that an upgrade is successful. Please follow the instructions below for the app you want. Spark Available benchmark packages Package name Location Benchmarks 1.6.x https:\/\/preview.unraveldata.com\/img\/spark-benchmarks1.tgz Benchmarks 2.0.x https:\/\/preview.unraveldata.com\/img\/demo-benchmarks-for-spark-2.0.tgz The .tgz file includes everything needed to run the benchmarks including both the datasets and scripts. Executing the benchmarks Go to the directory where you want to download and unpack the benchmark package. Download the file, where location is full pathname of the benchmark (see above) and package-name is the package name. curl location -o package-name Once downloaded, run md5sum on package-name to ensure it's intact. md5sum package-name Confirm that the output of md5sum is exactly as shown below for the package you just unpacked. ff8e56b4d5abfb0fb9f9e4a624eeb771 Md5sum for spark-benchmarks1.tgz\n71198901cedeadd7f8ebcf1bb1fd9779 Md5sum for demo-benchmarks-for-spark-2.0.tgz Uncompress the package. tar -zxvf package-name After unpacking, navigate to the created directory, demo_dir . cd demo_dir \nls\nbenchmarks\/ data\/ The benchmarks folder includes the jar of the Spark examples, the source files, and the scripts used to execute the examples. ls benchmarks\nREADME recommendations.png src\/\nlib\/ scripts\/ tpch-query-instances\/ lib contains compiled jar of all examples. scripts contains all the scripts needed to run the example. There are two scripts for each example: .\/example*.sh is the initial execution and .\/example*-after.sh is the re-execution of the same example after applying Unravelâ€™s recommendations: src contains the source files for the driver program. tpch-query-instances contains the queries for a TPC-H benchmark. Navigate to the data folder which contains the datasets used by the examples. cd data\nls data\nDATA.BIG.2G\/ tpch10g\/ Upload the datasets (requiring 12 GB size) hdfs dfs -put tpch10g\/ \/tmp\/\nhdfs dfs -put DATA.BIG.2G\/ \/tmp\/ Execute the first benchmark script, where script-number is the number of the script you wish to execute. .\/example script-number .sh After the run, Unravel recommendations are shown in the UI, on the application page. Once the example script is issued, the application metadata is displayed. Use the app id listed in the metadata to locate the app in Unravel UI. Recommendations are deployment specific so you need to edit the Spark properties in the example script-number -after.sh scripts as suggested in the Recommendations tab of Unravel UI. The categories of recommendations and insights are: actionable recommendations (examples 1, 2 , and 5)* Spark SQL (example 3) error view and insights for failed applications (example4) and recommendations for caching (example 5) *if running Benchmarks 2.0.x, example 6 is an actionable recommendation. Sample Spark recommendations Execute the edited *-after script, that includes the Spark configuration properties as suggested in the Recommendations tab of the Unravel UI. $ .\/example $ -after.sh After running the sample, check whether the re-execution of the script improved the performance or resource efficiency of the application. You can also check the Program and the Execution Graph tabs in Unravel UI. Click an RDD in the Execution Graph to see the corresponding line of code in the app. Example 5 In order to run this script you must enable insights for caching, which are disabled by default as it consumes additional heap from the memory allocation of the Spark worker daemon. You should enable insights for caching only if you expect that caching will improve performance of your Spark application. Add the following property to \/usr\/local\/unravel\/etc\/unravel.properties on the Unravel server node: com.unraveldata.spark.events.enableCaching=true Restart the spark worker daemon. sudo \/etc\/init.d\/unravel_sw_1 restart Repeat step 9 - 12. Once you have completed running example5.sh and example5-after.sh , reset the caching insight option to false. com.unraveldata.spark.events.enableCaching=false Restart the spark worker daemon. $ sudo \/etc\/init.d\/unravel_sw_1 restart Benchmarks for 1.6.x Description Demonstrates example1 A Scala-based application which generates its input and applies multiple transformations to the generated data. How Unravel helps select the number of partitions and container sizes for best performance, e.g., increasing the number of partitions and reducing per-container memory resources. example2 A Scala-based application which generates its input and applies multiple transformations to the generated data. How Unravel helps select the container sizes for best performance, in other words, reducing per-container memory resources. example3 A Scala program containing a SparkSQL query. The program runs TPC-H Query #9 on a 10GB database. How Unravel helps select the number of executors for best performance when dynamic allocation is disabled. For example, increasing the number of executors. example4 A Scala-based application. This application generates its input and applies multiple transformations to the generated data. How Unravel helps to root-cause a failed application. For example, failure-related insights and Error View when the application runs out of memory. example5 A Scala-based application. The application runs on an input of 2GB and applies multiple join and co-group transformations on the input data. Certain RDDs are evaluated multiple times. Pre-requirement : Add the property com.unraveldata.spark.events.enableCaching=true to unravel.properties file to enable caching. This property is disabled by default as it consumes additional heap from the memory allocation of the Spark worker daemon. Enable it only if caching related insights are considered for tuning performance of Spark applications. Unravelâ€™s insights for caching by showing the caching opportunities within the application, i.e., where in the program to use persist() to cache the corresponding RDD. In this example, dynamic allocation is disabled. Benchmarks for 2.0.x Example Demonstrates example1 see example1 in Benchmarks for Spark 1.6.x example2 A Scala-based application which generates its input and applies multiple transformations to the generated data including the coalesce transformation which reduces the level of parallelism to a suboptimal value. How Unravel helps select the number of partitions and container sizes for best performance of a Spark application. For example, by increasing the number of partitions. example3 - example5 see example3 - example5 in Benchmarks for Spark 1.6.x example6 A Scala-based Spark application which generates its input and applies multiple transformations to the generated data. How Unravel helps select the container sizes for best performance of a Spark application, e.g., reducing the memory requirements per executor. " }, 
{ "title" : "Uninstalling Unravel Server", 
"url" : "102207-install-uninstall.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Advanced installation options \/ Uninstalling Unravel Server", 
"snippet" : "Save a copy of \/usr\/local\/unravel\/etc\/unravel.properties . Disable and uninstall all sensors on all nodes in your cluster: BTrace and Hive Hook JARs, etc. On Unravel Server, uninstall the Unravel RPM. The Unravel Server RPM has the service name unravel . That means you can uninstall it with the rpm ...", 
"body" : "Save a copy of \/usr\/local\/unravel\/etc\/unravel.properties . Disable and uninstall all sensors on all nodes in your cluster: BTrace and Hive Hook JARs, etc. On Unravel Server, uninstall the Unravel RPM. The Unravel Server RPM has the service name unravel . That means you can uninstall it with the rpm command like this: sudo rpm -e unravel\n Delete the bundled database and all its data. sudo \/bin\/rm -rf \/usr\/local\/unravel \/srv\/unravel\/* \/etc\/unravel_ctl " }, 
{ "title" : "Configurations", 
"url" : "102208-config.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations", 
"snippet" : "Cloud Creating an AWS RDS CloudWatch Alarm for free storage space HBase Configuring HBase Hive Connecting to a Hive Metastore Configuring a Hive Metastore Configuring Hive Metastore access for an Oracle database Configuring Hive Metastore read-only user permissions Configuring Microsoft JDBC to conn...", 
"body" : "Cloud Creating an AWS RDS CloudWatch Alarm for free storage space HBase Configuring HBase Hive Connecting to a Hive Metastore Configuring a Hive Metastore Configuring Hive Metastore access for an Oracle database Configuring Hive Metastore read-only user permissions Configuring Microsoft JDBC to connect to Hive or HiverServer2 Obtaining Hive Metastore details Kafka Connecting to a Kafka cluster Kafka security Kerberos kerberos-changing-keytab Miscellaneous OnDemand Configuring migration and forecasting reports Configuring OnDemand Configuring small files report and files report Triggering an import of FSImage Secure UI Access Adding SSL and TLS to Unravel Web UI Enabling LDAP authentication for Unravel UI Enabling SAML authentication for Unravel Web UI Enabling TLS to Unravel Web UI directly Spark Configure Spark properties for Spark worker daemon @ Unravel Configuring notebooks for Spark Enable\/Disable live monitoring of Spark streaming apps Unravel Admins Adding more admins to Unravel Adding read-only admins to Unravel Changing Unravel admin's password User Interface Defining a custom banner Defining a custom Web UI port Disabling browser telemetry Disabling support\/comments panel Restricting direct access to Unravel UI Specifying a cluster ID or name Workflow Workflow Airflow Workflow Oozie " }, 
{ "title" : "Cloud", 
"url" : "102209-cloud.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Cloud", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Creating an AWS RDS CloudWatch Alarm for Free Storage Space", 
"url" : "102210-cloud-creating-aws-rds-cloudwatch-alarm-freestorage.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Cloud \/ Creating an AWS RDS CloudWatch Alarm for Free Storage Space", 
"snippet" : "This guide is to configure an AWS RDS CloudWatch Alarm for Disk FreeStorageSpace Metrics as part of RDS monitoring. Go to AWS CloudWatch. Select on the left-hand corner tab for Alarms . Click Create Alarm . On the right-hand section under RDS Metrics , click Per-Database Metrics . Under the column D...", 
"body" : "This guide is to configure an AWS RDS CloudWatch Alarm for Disk FreeStorageSpace Metrics as part of RDS monitoring. Go to AWS CloudWatch. Select on the left-hand corner tab for Alarms . Click Create Alarm . On the right-hand section under RDS Metrics , click Per-Database Metrics . Under the column DBInstanceIdentifier , select the database you wish to monitor for free storage space and click Next when you are done. In Alarm Threshold panel, specify the following: Name - for this Database Metrics (e.g. RDS_FreeStorageSpace_for_MySQL-A) Description - describe what the above database metrics name you entered (e.g. Disk space monitor of RDS MySQL-A) Add free storage of 20% left to alert contact under Whenever FreeStorageSpace is <= 20 I have suggested adding 20% of free storage space left, however, you can tune this to be lower. Add 10 for 10 consecutive period(s). Under Actions , add Send notifications to , which is your SNS topic. This SNS topic should already be set up before you add it. Click Create Alarm to create the alarm metrics for RDS monitoring on storage space. The UI displays the alarm you just created in Alarms but includes the error INSUFFICIENT DATA if the alarm hasn't been triggered yet. When it is triggered, an ALARM appears under Alarms tab and the system sends mail alerts will be distributed as defined in the SNS topic. Click Create Alarm to create the Alarm metrics for RDS monitoring on storage space. " }, 
{ "title" : "HBase", 
"url" : "102211-hbase.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ HBase", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Configuring HBase", 
"url" : "102212-hbase-configuring.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ HBase \/ Configuring HBase", 
"snippet" : "To access Rest API the user must have at least read-only access. Cloudera Manager and Ambari supports creating a local user via their administration page. See HBase Configuration properties for information on the below properties and example values. Edit \/usr\/local\/unravel\/etc\/unravel.properties and...", 
"body" : "To access Rest API the user must have at least read-only access. Cloudera Manager and Ambari supports creating a local user via their administration page. See HBase Configuration properties for information on the below properties and example values. Edit \/usr\/local\/unravel\/etc\/unravel.properties and define the following properties. If a property is not found then add it; be sure to substitute your local value for highlighted text . On-Prem clusters managed by Ambari or Cloudera Manager You must define these properties; source_type is AMBARI or CDH . \ncom.unraveldata.hbase.source.type= source_type \ncom.unraveldata.hbase.rest.url= Ambari_or_Cloudera_base_url \ncom.unraveldata.hbase.rest.user= hbase_rest_username \ncom.unraveldata.hbase.rest.pwd= hbase_rest_password \ncom.unraveldata.hbase.clusters= comma_separated_cluster_names \n You can optionally define these properties. com.unraveldata.hbase.service.name= comma_separated_clustername_and_servicename \ncom.unraveldata.hbase.rest.ssl.enabled= true_or_false \ncom.unraveldata.hbase.master.port= Port# \ncom.unraveldata.hbase.regionserver.port= Port# \ncom.unraveldata.hbase.metric.poll.interval= Seconds \ncom.unraveldata.hbase.http.conn.timeout= Seconds \ncom.unraveldata.hbase.http.poll.parallelism= Number \n Clusters running Java Management Extension (JMX), including Amazon EMR and Azure HDI. You must define these properties. com.unraveldata.hbase.source.type=JMX\ncom.unraveldata.hbase.clusters= comma_separated_cluster_names \ncom.unraveldata.hbase. cluster_name .node.http.apis= comma_separated_base_node_http_api \n You can optionally define these properties. com.unraveldata.hbase.metric.poll.interval= Seconds \ncom.unraveldata.hbase.http.conn.timeout= Seconds \ncom.unraveldata.hbase.http.read.timeout= Seconds \ncom.unraveldata.hbase.http.poll.parallelism= Number \ncom.unraveldata.hbase.alert.average.threshold= Number \n Restart the HBase service unravel_us_1 . # sudo service unravel_us_1 restart Verify unravel_us_1 is running. # sudo service unravel_us_1 status unravel_us_1 is running Verify the metrics are collected in Elasticsearch. A separate index file is created for each week with an alias hb-search, for example, hb-20180813_19. Troubleshooting If unravel_us_1 is not running check the logs for any errors. usr\/local\/unravel\/logs\/unravel_us_1.out \/usr\/local\/unravel\/logs\/unravel_us_1.log If hb-* index is not created or no data in Elasticsearch, verify the following daemons are running. unravel_us_1 (Step 1 above) unravel_s_1 (elastic service) unravel_hl (hitdocloader sevice) unravel_k_1 (kafka service) Resources Service name : unravel_us_1 Service logs : \/usr\/local\/unravel\/logs\/unravel_us_1.log , \/usr\/local\/unravel\/logs\/unravel_us_1.out Configuration file : \/usr\/local\/unravel\/etc\/unravel.properties Elasticsearch : Template file : \/usr\/local\/unravel\/etc\/template_hbase_metrics.json Index name : hb-* " }, 
{ "title" : "Connecting to the Hive Metastore", 
"url" : "102213-hive-metastore.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Connecting to the Hive Metastore", 
"snippet" : "Connecting to the Hive Metastore enables Unravel Server to analyze table usage in conjunction with Hive query information, which results in the population of the Data Insights page in Unravel UI. Unravel Server collects information through the Hive API, which is similar to the beeline CLI and uses t...", 
"body" : "Connecting to the Hive Metastore enables Unravel Server to analyze table usage in conjunction with Hive query information, which results in the population of the Data Insights page in Unravel UI. Unravel Server collects information through the Hive API, which is similar to the beeline CLI and uses the JDBC database connection protocol. You have two options for connecting Unravel Server to the Hive Metastore: Get your cluster's username and password for Hive or HiveServer2 , and set Unravel Server to use those credentials . Define a read-only user for Hive or HiveServer2 , and set Unravel server to use those credentials . Configuring Microsoft JDBC to connect to Hive or HiverServer2 , and set Unravel server to use those credentials . " }, 
{ "title" : "Configuring a Hive Metastore", 
"url" : "102214-hive-metastore-config.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Connecting to the Hive Metastore \/ Configuring a Hive Metastore", 
"snippet" : "In order for the Reports > Data Insights tab to populate correctly you must configure the Hive Metastore Access parameters. You also have the option to configure the JDBC parameters managing JDBC pooling. Download the right JDBC connector JAR for your database: If you're using MySQL as the Hive Meta...", 
"body" : "In order for the Reports > Data Insights tab to populate correctly you must configure the Hive Metastore Access parameters. You also have the option to configure the JDBC parameters managing JDBC pooling. Download the right JDBC connector JAR for your database: If you're using MySQL as the Hive Metastore database, download the MySQL JDBC connector JAR from MySQL Download ConnectorJ . If you're using Oracle as the Hive Metastore database, download the Oracle JDBC connector JARs ojdbc7.jar and ojdbc6.jar from Oracle Database 12c Release 1 JDBC Driver Downloads . Make sure the unravel account has read permission. In \/usr\/local\/unravel\/etc\/unravel.properties , edit the following properties: Property\/Description Set by user Unit Default javax.jdo.option.ConnectionDriverName JDBC Driver class name for the data store containing the metadata. Examples: MySQL: com.mysql.jdbc.Driver Oracle: oracle.jdbc.driver.OracleDriver Microsoft: com.microsoft.sqlserver.jdbc.SQLServerDriver Required string - javax.jdo.option.ConnectionPassword Password used to access the data store. Required string - javax.jdo.option.ConnectionUserName Username used to access the data store. Required string - javax.jdo.option.ConnectionURL JDBC connection string for the data store containing the metadata of the form: jdbc: DB_Driver :\/\/ HOST : PORT \/hive Example: Oracle: jdbc:oracle:thin:@prodHost:1521:ORCL Microsoft: jdbc:sqlserver:\/\/ jdbc_url Requited string (URL) - To locate the values for the above properties see Obtaining Hive Metastore details . In other words, javax.jdo.option.ConnectionURL=jdbc: drivertype :\/\/ hive-metastore-database-host \/hive\njavax.jdo.option.ConnectionDriverName= driverclass \njavax.jdo.option.ConnectionPassword= hive-metastore-database-password \njavax.jdo.option.ConnectionUserName= hive-metastore-database-user For example, javax.jdo.option.ConnectionURL=jdbc:mysql:\/\/congo.unraveldata.com\/hive\njavax.jdo.option.ConnectionDriverName=com.mysql.jdbc.Driver \njavax.jdo.option.ConnectionUserName=hive \njavax.jdo.option.ConnectionPassword=hadoop You may optionally configure the following properties to manage the Hive Metastore JDBC connection pooling. Unravel uses the c3p0 library to manage the pooling. Property\/Description Set by user Unit Default com.unraveldata.metastore.db.c3p0.acquireRetryAttempts Controls how many times c3p0 tries to obtain an connection from the database before giving up. For MapR you must set this value to 0. count 30 com.unraveldata.metastore.db.c3p0.acquireRetryDelay Controls how much waiting time is between each retry attempts in milliseconds. For MapR you must set this value to 0. ms 1000 com.unraveldata.metastore.db.c3p0.breakafteracquirefailure Allows you to mark data source as broken and permanently be closed if a connection cannot be obtained from database. The default value is false . boolean false com.unraveldata.metastore.db.c3p0.maxconnectionage The maximum number of seconds any connections were forced to be released from the pool. If the default value (0) is used the connections will never be released. sec 0 com.unraveldata.metastore.db.c3p0.maxidletimeexcessconnections The number of seconds that connections are permitted to remain idle in the pool before being released. If the default value (0) is used the connections will never be released. sec 0 com.unraveldata.metastore.db.c3p0.maxpoolsize The maximum connections in the connection pool. count 5 com.unraveldata.metastore.db.c3p0.idleconnectiontestperiod   0 com.unraveldata.metastore.databasePattern   string dname* com.unraveldata.print.metastore.stats   boolean false com.unraveldata.metastore.use.jdbc Enables read-only access to retrieve data from HiveMetastore with simple JDBC calls. true : read-only access for HiveMetastore data retrieval. false : read-write access for HiveMetastore data retrieval. boolean false Restart Unravel Server. sudo \/etc\/init.d\/unravel_all.sh restart Go to Unravel UI's Applications > Applications page to confirm that Hive queries are displayed. Approximately twenty-four hours after configuration the Reports > Data Insights > Details page displays a list of your Hive Metastore tables along with their KPIs and other details. Reference c3p0 project page " }, 
{ "title" : "Configuring access for an Oracle database", 
"url" : "102215-hive-metastore-oracle-config.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Connecting to the Hive Metastore \/ Configuring access for an Oracle database", 
"snippet" : "Due to restrictions for accessing an Oracle database you need to create synonyms and select access for the Unravel user. Complete the following steps to allow access for a read only user. Substitute your hive database owner ID for HIVEUSER and the user id that unravel is using for unravelhive databa...", 
"body" : "Due to restrictions for accessing an Oracle database you need to create synonyms and select access for the Unravel user. Complete the following steps to allow access for a read only user. Substitute your hive database owner ID for HIVEUSER and the user id that unravel is using for unravelhive database. As the Oracle DBA Run the following queries to output SQL scripts to grant read access to hive tables and create synonyms. Select 'grant select on '||table_name||' to unravelhive;' from SYS.ALL_TABLES where OWNER = HIVEUSER Select 'create synonym '||table_name||' for = HIVEUSER ||table_name||';' from SYS.ALL_TABLES where OWNER == HIVEUSER Create a database account for Unravel to use called unravelhive . grant connect to unravelhive ;\ngrant resource to unravelhive ;\ngrant create synonym to unravelhive ; Open a SQL*Plus connection then: As the Hive database user Set the SQL*Plus formatting settings as shown: set pagesize 200;\nset linesize 200;\n\ncolumn table_name format a30;\ncolumn owner format a30; As Hive database user ( HIVEUSER ), run Query 1 output; it should look similar to this example. grant select on TXNS to unravelhive ;\ngrant select on TXN_COMPONENTS to unravelhive ;\ngrant select on COMPLETED_TXN_COMPONENTS to unravelhive ;\ngrant select on NEXT_TXN_ID to unravelhive ;\n\ngrant select on HIVE_LOCKS to unravelhive ;\ngrant select on NEXT_LOCK_ID to unravelhive ;\ngrant select on COMPACTION_QUEUE to unravelhive ;\ngrant select on NEXT_COMPACTION_QUEUE_ID to unravelhive ;\ngrant select on METASTORE_DB_PROPERTIES to unravelhive ;\ngrant select on SEQUENCE_TABLE to unravelhive ;\ngrant select on NUCLEUS_TABLES to unravelhive ;\ngrant select on PART_COL_PRIVS to unravelhive ;\ngrant select on CDS to unravelhive ;\ngrant select on COLUMNS_V2 to unravelhive ;\ngrant select on PARTITION_KEY_VALS to unravelhive ;\ngrant select on DBS to unravelhive ;\ngrant select on PARTITION_PARAMS to unravelhive ;\ngrant select on SERDES to unravelhive ;\ngrant select on TYPES to unravelhive ;\ngrant select on PARTITION_KEYS to unravelhive ;\ngrant select on ROLES to unravelhive ;\ngrant select on PARTITIONS to unravelhive ;\ngrant select on INDEX_PARAMS to unravelhive ;\ngrant select on TBL_COL_;S to unravelhive ;\ngrant select on IDXS to unravelhive ;\ngrant select on BUCKETING_COLS to unravelhive ;\ngrant select on TYPE_FIELDS to unravelhive ;\ngrant select on SD_PARAMS to unravelhive ;\ngrant select on GLOBAL_PRIVS to unravelhive ;\ngrant select on SDS to unravelhive ;\ngrant select on TABLE_PARAMS to unravelhive ;\ngrant select on SORT_COLS to unravelhive ;\ngrant select on TBL_PRIVS to unravelhive ;\ngrant select on DATABASE_PARAMS to unravelhive ;\ngrant select on ROLE_MAP to unravelhive ;\ngrant select on SERDE_PARAMS to unravelhive ;\ngrant select on PART_PRIVS to unravelhive ;\ngrant select on DB_PRIVS to unravelhive ;\ngrant select on TBLS to unravelhive ;\ngrant select on PARTITION_EVENTS to unravelhive ;\ngrant select on SKEWED_STRING_LIST to unravelhive ;\ngrant select on SKEWED_STRING_LIST_VALUES to unravelhive ;\ngrant select on SKEWED_COL_NAMES to unravelhive ;\ngrant select on SKEWED_COL_VALUE_LOC_MAP to unravelhive ;\ngrant select on MASTER_KEYS to unravelhive ;\ngrant select on DELEGATION_TOKENS to unravelhive ;\ngrant select on SKEWED_VALUES to unravelhive ;\ngrant select on TAB_COL_STATS to unravelhive ;\ngrant select on VERSION to unravelhive ;\ngrant select on PART_COL_STATS to unravelhive ;\ngrant select on FUNCS to unravelhive ;\ngrant select on FUNC_RU to unravelhive ;\ngrant select on NOTIFICATION_LOG to unravelhive ;\ngrant select on NOTIFICATION_SEQUENCE to unravelhive ; As Unravel user ( UNRAVELHIVE ) Run Query 2 output ; it should look similar to this example. \ncreate synonym TXNS for HIVEUSER .TXNS;\ncreate synonym TXN_COMPONENTS for HIVEUSER .TXN_COMPONENTS;\ncreate synonym COMPLETED_TXN_COMPONENTS for HIVEUSER .COMPLETED_TXN_COMPONENTS;\ncreate synonym NEXT_TXN_ID for HIVEUSER .NEXT_TXN_ID;\ncreate synonym HIVE_LOCKS for HIVEUSER .HIVE_LOCKS;\ncreate synonym NEXT_LOCK_ID for HIVEUSER .NEXT_LOCK_ID;\ncreate synonym COMPACTION_QUEUE for HIVEUSER .COMPACTION_QUEUE;\ncreate synonym NEXT_COMPACTION_QUEUE_ID for HIVEUSER .NEXT_COMPACTION_QUEUE_ID;\ncreate synonym METASTORE_DB_PROPERTIES for HIVEUSER .METASTORE_DB_PROPERTIES;\ncreate synonym SEQUENCE_TABLE for HIVEUSER .SEQUENCE_TABLE;\ncreate synonym NUCLEUS_TABLES for HIVEUSER .NUCLEUS_TABLES;\ncreate synonym PART_COL_PRIVS for HIVEUSER .PART_COL_PRIVS;\ncreate synonym CDS for HIVEUSER .CDS;\ncreate synonym COLUMNS_V2 for HIVEUSER .COLUMNS_V2;\ncreate synonym PARTITION_KEY_VALS for HIVEUSER .PARTITION_KEY_VALS;\ncreate synonym DBS for HIVEUSER .DBS;\ncreate synonym PARTITION_PARAMS for HIVEUSER .PARTITION_PARAMS;\ncreate synonym SERDES for HIVEUSER .SERDES;\ncreate synonym TYPES for HIVEUSER .TYPES;create synonym PARTITION_KEYS for HIVEUSER .PARTITION_KEYS;\ncreate synonym ROLES for HIVEUSER .ROLES;\ncreate synonym PARTITIONS for HIVEUSER .PARTITIONS;\ncreate synonym INDEX_PARAMS for HIVEUSER .INDEX_PARAMS;\ncreate synonym TBL_COL_PRIVS for HIVEUSER .TBL_COL_PRIVS;\ncreate synonym IDXS for HIVEUSER .IDXS;\ncreate synonym BUCKETING_COLS for HIVEUSER .BUCKETING_COLS;\ncreate synonym TYPE_FIELDS for HIVEUSER .TYPE_FIELDS;\ncreate synonym SD_PARAMS for HIVEUSER .SD_PARAMS;\ncreate synonym GLOBAL_PRIVS for HIVEUSER .GLOBAL_PRIVS;\ncreate synonym SDS for HIVEUSER .SDS;\ncreate synonym TABLE_PARAMS for HIVEUSER .TABLE_PARAMS;\ncreate synonym SORT_COLS for HIVEUSER .SORT_COLS;\ncreate synonym TBL_PRIVS for HIVEUSER .TBL_PRIVS;\ncreate synonym DATABASE_PARAMS for HIVEUSER .DATABASE_PARAMS;\ncreate synonym ROLE_MAP for HIVEUSER .ROLE_MAP;\ncreate synonym SERDE_PARAMS for HIVEUSER .SERDE_PARAMS;\ncreate synonym PART_PRIVS for HIVEUSER .PART_PRIVS;\ncreate synonym DB_PRIVS for HIVEUSER .DB_PRIVS;\ncreate synonym TBLS for HIVEUSER .TBLS;\ncreate synonym PARTITION_EVENTS for HIVEUSER .PARTITION_EVENTS;\ncreate synonym SKEWED_STRING_LIST for HIVEUSER .SKEWED_STRING_LIST;\ncreate synonym SKEWED_STRING_LIST_VALUES for HIVEUSER .SKEWED_STRING_LIST_VALUES;\ncreate synonym SKEWED_COL_NAMES for HIVEUSER .SKEWED_COL_NAMES;\ncreate synonym SKEWED_COL_VALUE_LOC_MAP for HIVEUSER .SKEWED_COL_VALUE_LOC_MAP;\ncreate synonym MASTER_KEYS for HIVEUSER .MASTER_KEYS;\ncreate synonym DELEGATION_TOKENS for HIVEUSER .DELEGATION_TOKENS;\ncreate synonym SKEWED_VALUES for HIVEUSER .SKEWED_VALUES;\ncreate synonym TAB_COL_STATS for HIVEUSER .TAB_COL_STATS;\ncreate synonym VERSION for HIVEUSER .VERSION;create synonym PART_COL_STATS for HIVEUSER .PART_COL_STATS;\ncreate synonym FUNCS for HIVEUSER .FUNCS;\ncreate synonym FUNC_RU for HIVEUSER .FUNC_RU;create synonym NOTIFICATION_LOG for HIVEUSER .NOTIFICATION_LOG;\ncreate synonym NOTIFICATION_SEQUENCE for HIVEUSER .NOTIFICATION_SEQUENCE; " }, 
{ "title" : "Configuring Microsoft JDBC to connect to Hive Metastore", 
"url" : "102216-hive-configure-usoft-jdbc.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Connecting to the Hive Metastore \/ Configuring Microsoft JDBC to connect to Hive Metastore", 
"snippet" : "Download Microsoft JDBC Driver 7.0 for SQL Server JAR from Microsoft here to Unravel node. Extract the downloaded file. tar -xvzf sqljdbc_ version _enu.tar.gz Create the following directories if they do not exist. mkdir -p \/usr\/local\/unravel\/share\/java Copy mssql-jdbc-7.0.0.jre8.jar to the following...", 
"body" : "Download Microsoft JDBC Driver 7.0 for SQL Server JAR from Microsoft here to Unravel node. Extract the downloaded file. tar -xvzf sqljdbc_ version _enu.tar.gz Create the following directories if they do not exist. mkdir -p \/usr\/local\/unravel\/share\/java\n Copy mssql-jdbc-7.0.0.jre8.jar to the following locations. sudo cp sqljdbc_7.0\/enu\/mssql-jdbc-7.0.0.jre8.jar \/usr\/local\/unravel\/share\/java\n In \/usr\/local\/unravel\/etc\/unravel.properties , edit the following properties: Property\/Description Set by user Unit Default javax.jdo.option.ConnectionDriverName JDBC Driver class name for the data store containing the metadata. Examples: MySQL: com.mysql.jdbc.Driver Oracle: oracle.jdbc.driver.OracleDriver Microsoft: com.microsoft.sqlserver.jdbc.SQLServerDriver Required string - javax.jdo.option.ConnectionPassword Password used to access the data store. Required string - javax.jdo.option.ConnectionUserName Username used to access the data store. Required string - javax.jdo.option.ConnectionURL JDBC connection string for the data store containing the metadata of the form: jdbc: DB_Driver :\/\/ HOST : PORT \/hive Example: Oracle: jdbc:oracle:thin:@prodHost:1521:ORCL Microsoft: jdbc:sqlserver:\/\/ jdbc_url Requited string (URL) - To locate the values for the above properties see Obtaining Hive Metastore details . For example, javax.jdo.option.ConnectionURL=jdbc:sqlserver:\/\/ jdbc_url \njavax.jdo.option.ConnectionDriverName=com.microsoft.sqlserver.jdbc.SQLServerDriver\njavax.jdo.option.ConnectionPassword= hive-metastore-database-password \njavax.jdo.option.ConnectionUserName= hive-metastore-database-user Restart Unravel Server. sudo \/etc\/init.d\/unravel_all.sh restart " }, 
{ "title" : "Configuring Hive Metastore read-only user permissions", 
"url" : "102217-hive-metastore-permissions.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Connecting to the Hive Metastore \/ Configuring Hive Metastore read-only user permissions", 
"snippet" : "Unravel daemons need READ permission on the Hive metastore. To define a read-only Hive metastore user, follow these steps. Get details about the location of the Hive Metastore. In CDH, use the following Cloudera Manager API to get the Hive metastore database name and port. By default, in CDH and HDP...", 
"body" : "Unravel daemons need READ permission on the Hive metastore. To define a read-only Hive metastore user, follow these steps. Get details about the location of the Hive Metastore. In CDH, use the following Cloudera Manager API to get the Hive metastore database name and port. By default, in CDH and HDP, the hive metastore database name is hive . For TLS-secured Cloudera Manager curl -k -u admin:admin \"https:\/\/ cloudera-manager-host :7183\/api\/v11\/clusters\/ cluster-name \/services\/hive\/config\" For non-TLS secured Cloudera Manager curl -k -u admin:admin \"http:\/\/ cloudera-manager-host :7183\/api\/v11\/clusters\/ cluster-name \/services\/hive\/config\" or curl -k -u admin:admin \"http:\/\/ cloudera-manager-host :7180\/api\/v12\/cm\/deployment\" Look for hive_metastore_database_host , hive_metastore_database_port , hive_metastore_database_user and hive_metastore_database_password in the JSON response body. For HDP, contact your cluster administrator. For MapR, contact your cluster administrator. Connect to the Hive metastore using the normal conversational interface for your underlying database (MySQL, psql, Oracle, and so on) as an administrator or root user that can create new users and grant privileges. For PostgreSQL, run the psql command as admin user cloudera-scm on Cloudera embedded PostgreSQL server within the Cloudera Manager node. psql -U cloudera-scm -p 7432 -h localhost -d postgres Create a new database user, such as unravelka , grant the user READ access to the database, grant that user SELECT privileges on all tables in the Hive database, and give that user access from the Unravel server host. For MySQL, run the following command to create a new user unravelka and grant it SELECT privilege on the Hive metastore database: GRANT SELECT ON hive.* to 'unravelk'@'%' identified by 'unravelk_password';\nflush privileges; For PostgreSQL, run the following commands: CREATE USER unravelk WITH ENCRYPTED PASSWORD 'unravelk_password';\n\nGRANT CONNECT ON DATABASE hive TO unravelk;\nGRANT USAGE ON SCHEMA public TO unravelk;\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO unravelk;\nGRANT SELECT ON ALL SEQUENCES IN SCHEMA public TO unravelk;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO unravelk;\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT USAGE ON SEQUENCES TO unravelk; The default PostgreSQL admin password created by Cloudera Manager is stored in \/var\/lib\/cloudera-scm-server-db\/data\/generated_password.txt . For Oracle, see . As the new user, use the conversational interface (MySQL, psql, Oracle, and so on) from Unravel Server to verify that the new user has access. In \/usr\/local\/unravel\/etc\/unravel.properties , update Hive metastore access information with the read-only username and the values you retrieved in step 1. Property\/Description Set by user Unit Default javax.jdo.option.ConnectionDriverName JDBC Driver class name for the data store containing the metadata. Examples: MySQL: com.mysql.jdbc.Driver Oracle: oracle.jdbc.driver.OracleDriver Microsoft: com.microsoft.sqlserver.jdbc.SQLServerDriver Required string - javax.jdo.option.ConnectionPassword Password used to access the data store. Required string - javax.jdo.option.ConnectionUserName Username used to access the data store. Required string - javax.jdo.option.ConnectionURL JDBC connection string for the data store containing the metadata of the form: jdbc: DB_Driver :\/\/ HOST : PORT \/hive Example: Oracle: jdbc:oracle:thin:@prodHost:1521:ORCL Microsoft: jdbc:sqlserver:\/\/ jdbc_url Requited string (URL) - For example, javax.jdo.option.ConnectionURL=jdbc:mysql:\/\/congo.unraveldata.com\/hive javax.jdo.option.ConnectionDriverName=com.mysql.jdbc.Driver \njavax.jdo.option.ConnectionUserName=unravelka\njavax.jdo.option.ConnectionPassword=hadoop If you are using Microsoft JDBC Driver 7.0 for SQL Server set the properties as follows. javax.jdo.option.ConnectionURL=jdbc:sqlserver:\/\/ jdbc_url \njavax.jdo.option.ConnectionDriverName=com.microsoft.sqlserver.jdbc.SQLServerDriver\njavax.jdo.option.ConnectionUserName= username \njavax.jdo.option.ConnectionPassword= password Update \/usr\/local\/unravel\/etc\/unravel.properties to use simple JDBC calls to get read-only details from Hive Metastore. com.unraveldata.metastore.use.jdbc=true Restart Unravel server. sudo \/etc\/init.d\/unravel_all.sh restart Go to Unravel UI's Applications > Applications page to confirm that Hive queries are displayed. Approximately twenty-four hours after configuration the Reports > Data Insights > Details page displays a list of your Hive Metastore tables along with their KPIs and other details. " }, 
{ "title" : "Obtaining Hive Metastore details", 
"url" : "102218-hive-obtain-metastore-details.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Connecting to the Hive Metastore \/ Obtaining Hive Metastore details", 
"snippet" : "This topic explains how to find the connection URL, driver name, username, and password for Hive or HiveServer2. You need these four settings to connect Unravel Server to the Hive Metastore. For CDH From CDH version 5.5+, send the Cloudera Manager REST API request http:\/\/ cloudera-manager-hostname-o...", 
"body" : "This topic explains how to find the connection URL, driver name, username, and password for Hive or HiveServer2. You need these four settings to connect Unravel Server to the Hive Metastore. For CDH From CDH version 5.5+, send the Cloudera Manager REST API request http:\/\/ cloudera-manager-hostname-or-ip :7180\/api\/v12\/cm\/deployment . Search the JSON response body for metastore . Copy the connection URL, driver name, username, and password. For HDP To get the four required settings, contact your cluster's administrator. For MapR To get the four required settings, contact your cluster's administrator. " }, 
{ "title" : "For Azure HDInsight", 
"url" : "102218-hive-obtain-metastore-details.html#UUID-ea496a06-727f-5cd8-bf46-511c4fcbe0cb_section-5d4232c7a2158-idm45764247840320", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Connecting to the Hive Metastore \/ Obtaining Hive Metastore details \/ For Azure HDInsight", 
"snippet" : "You can get 3 of the 4 properties from Ambari, or get all 4 from the Azure portal: From Ambari... Log into the Azure portal. Click HDInsight clusters , and select your cluster. In the cluster's Overview window, click the link in the URL field. This gets you to the Ambari UI. In Ambari, click Hive | ...", 
"body" : "You can get 3 of the 4 properties from Ambari, or get all 4 from the Azure portal: From Ambari... Log into the Azure portal. Click HDInsight clusters , and select your cluster. In the cluster's Overview window, click the link in the URL field. This gets you to the Ambari UI. In Ambari, click Hive | Configs | Advanced . Get the four required settings: For connection URL, use the value in the Database URL field. In the screenshot, this is jdbc:sqlserver:\/\/ldnu62r6x6.database.windows.net;databaseName=v364574cc3cfd51438fb29a723243a9a71bhivemetastore;trustServerCertificate=false;encrypt=true;hostNameInCertificate=*.database.windows.net; For driver name, use the value in the JDBC Driver Class field. In the screenshot, this is com.microsoft.sqlserver.jdbc.SQLServerDriver For username, use the value in Database Username field. In the screenshot, this is v364574cc3cfd51438fb29a723243a9a71bhivemetastoreLogin@ldnu62r6x6.database.windows.net The password is encrypted; Ambari doesn't show it. Obtain the password from Azure instead. From the Azure portal... From the Azure portal, find out the IP address to use for an SSH session to your cluster. Open an SSH session to that IP address. In \/etc\/hive\/conf\/hive-site.xml , search for these values, and copy them to use as-is: javax.jdo.option.ConnectionURL javax.jdo.option.ConnectionDriverName javax.jdo.option.ConnectionUserName To get the Hive password, run this command at the Linux prompt: sudo java -cp '\/var\/lib\/ambari-agent\/cred\/lib\/*' org.apache.ambari.server.credentialapi.CredentialUtil  get javax.jdo.option.ConnectionPassword -provider 'jceks:\/\/file\/var\/lib\/ambari-agent\/cred\/conf\/hive_metastore\/hive-site.jceks' 2>\/dev\/null " }, 
{ "title" : "For Amazon EMR", 
"url" : "102218-hive-obtain-metastore-details.html#UUID-ea496a06-727f-5cd8-bf46-511c4fcbe0cb_section-5d4232f689459-idm45315535889040", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Connecting to the Hive Metastore \/ Obtaining Hive Metastore details \/ For Amazon EMR", 
"snippet" : "Open an SSH session to the cluster, open the Hive configuration file, \/etc\/hive\/conf\/hive-site.xml , and search for the four settings. For example, on EMR version 4.7.2, you'd see something like this: <property> <name>javax.jdo.option.ConnectionURL<\/name> <value>jdbc:mysql:\/\/ip-xx-xx-xx-xx:3306\/hive...", 
"body" : "Open an SSH session to the cluster, open the Hive configuration file, \/etc\/hive\/conf\/hive-site.xml , and search for the four settings. For example, on EMR version 4.7.2, you'd see something like this: <property>\n <name>javax.jdo.option.ConnectionURL<\/name>\n <value>jdbc:mysql:\/\/ip-xx-xx-xx-xx:3306\/hive?createDatabaseIfNotExist=true<\/value>\n <description>username to use against metastore database<\/description>\n<\/property>\n\n<property>\n <name>javax.jdo.option.ConnectionUserName<\/name>\n <value>hive<\/value>\n <description>username to use against metastore database<\/description>\n<\/property>\n\n<property>\n <name>javax.jdo.option.ConnectionPassword<\/name>\n <value>xxxxxxxxxxx<\/value>\n <description>password to use against metastore database<\/description>\n<\/property> " }, 
{ "title" : "Kafka", 
"url" : "102219-kafka.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Kafka", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Connecting to a Kafka cluster", 
"url" : "102220-kafka-connecting.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Kafka \/ Connecting to a Kafka cluster", 
"snippet" : "To connect Unravel Server to a Kafka cluster, change the configuration of the Kafka cluster and add some properties to unravel.properties . Change the configuration of the Kafka cluster. Export an available port for JMX_PORT. You need to export one port for each Kafka server. export JMX_PORT= jmx-po...", 
"body" : "To connect Unravel Server to a Kafka cluster, change the configuration of the Kafka cluster and add some properties to unravel.properties . Change the configuration of the Kafka cluster. Export an available port for JMX_PORT. You need to export one port for each Kafka server. export JMX_PORT= jmx-port The default JMX port for Kafka in CDH is 9393. In HDP you would export this parameter in Ambari Web UI under Advanced kafka-env | kafka-env template . Enable remote access for JMX monitoring by appending the following lines to KAFKA_JMX_OPTS in kafka_run_class.sh : -Dcom.sun.management.jmxremote.rmi.port=$JMX_PORT\n-Dcom.sun.management.jmxremote.port=$JMX_PORT\n-Djava.rmi.server.hostname= public-hostname \n-Djava.net.preferIPv4Stack=true Not required for HDP. Verify the configuration changes on the Kafka cluster. Restart the Kafka broker. In \/usr\/local\/unravel\/etc\/unravel.properties , add properties to monitor the Kafka cluster. The Unravel daemon, unravel_km , relies on a list of Kafka servers to query and discover the entire cluster. You need to add this list of Kafka servers to \/usr\/local\/unravel\/etc\/unravel.properties . For complete descriptions of properties, see Kafka Monitoring Properties . com.unraveldata.ext.kafka.clusters= cluster-list \ncom.unraveldata.ext.kafka. cluster-id .bootstrap_server= boot-servers \ncom.unraveldata.ext.kafka. cluster-id .jmx_servers= jmx-servers \ncom.unraveldata.ext.kafka. cluster-id .jmx. jmx-server-id .host= jmx-hosts \ncom.unraveldata.ext.kafka. cluster-id .jmx. jmx-server-id .port= jmx-port For example, com.unraveldata.ext.kafka.clusters=c1,c2 \ncom.unraveldata.ext.kafka.c1.bootstrap_servers=localhost:9092,localhost:9093 \ncom.unraveldata.ext.kafka.c2.bootstrap_servers=localhost:9192,localhost:9193 \ncom.unraveldata.ext.kafka.c1.jmx_servers=kafka-test1,kafka-test2 \ncom.unraveldata.ext.kafka.c2.jmx_servers=kafka-test1,kafka-test2 \ncom.unraveldata.ext.kafka.c1.jmx.kafka-test1.host=localhost \ncom.unraveldata.ext.kafka.c1.jmx.kafka-test2.host=localhost \ncom.unraveldata.ext.kafka.c2.jmx.kafka-test1.host=localhost \ncom.unraveldata.ext.kafka.c2.jmx.kafka-test2.host=localhost \ncom.unraveldata.ext.kafka.c1.jmx.kafka-test1.port=5005 \ncom.unraveldata.ext.kafka.c1.jmx.kafka-test2.port=5010 \ncom.unraveldata.ext.kafka.c2.jmx.kafka-test1.port=5105 \ncom.unraveldata.ext.kafka.c2.jmx.kafka-test2.port=5110 " }, 
{ "title" : "Kafka security", 
"url" : "102221-kafka-security.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Kafka \/ Kafka security", 
"snippet" : "You can improve the Kafka cluster security by having Kafka authenticate connections to brokers from client using either SSL or SASL. SSL + Kerberos for Kafka clients Prerequisite : Kafka brokers are configured with SSL and Kerberos. Please refer to your Hadoop providers documentation for configuring...", 
"body" : "You can improve the Kafka cluster security by having Kafka authenticate connections to brokers from client using either SSL or SASL. SSL + Kerberos for Kafka clients Prerequisite : Kafka brokers are configured with SSL and Kerberos. Please refer to your Hadoop providers documentation for configuring SSL and Kerberos for Kafka brokers. SSL+Kerberos is supported by new Kafka consumers and producers. The configuration is same for consumer and producer. Replace items in red with values specific\/relevant to your environment. For multiple Kafka clients Each cluster must have a separate consumerConfig.properties files. Open \/usr\/local\/unravel\/unravel.properties . Search for com.unraveldata.ext.kafka.clusters . The property should be defined with a comma separated list. If there is only one cluster name see above . com.unraveldata.ext.kafka.clusters= ClusterName1 , ClusterName2 , ClusterName3 Create a file named consumerConfig ClusterName .properties for each cluster. ssl.protocol = TLSv1 sasl.mechanism = GSSAPI security.protocol = SASL_SSL \nsasl.kerberos.service.name = kafka \nssl.truststore.location = \/usr\/java\/jdk1.7.0_67-cloudera\/jre\/lib\/security\/jssecacerts1 \nssl.truststore.password = changeit \nssl.truststore.type = JKS \nssl.keystore.location = \/opt\/cloudera\/security\/jks\/server.keystore.jks \nssl.keystore.password = password ssl.keystore.type = JKS \nssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1 sasl.jaas.config = com.sun.security.auth.module.Krb5LoginModule required \\\nuseKeyTab=true \\ keyTab=\"\/etc\/keytabs\/kafka.keytab\" \\\n principal=\"kafka\/edge-1.uddev.unraveldata.com@UDDEV.UNRAVELDATA.COM\" Copy\/move each file to \/usr\/local\/unravel\/etc . Edit \/usr\/local\/unravel\/unravel.properties . For each cluster add the following property. com.unraveldata.ext.kafka.ClusterName.consumer.config=\/usr\/local\/unravel\/etc\/consumerConfigClusterName.properties Restart the Kafka monitor daemon unravel_km . service unravel_km restart Kafka authorizations Unravel consumes messages to topic __consumer_offsets using consumer group UnravelOffsetConsumer . Sentry authorization The following privilege must be granted using sentry: HOST=*->CONSUMERGROUP=UnravelOffsetConsumer→action=read\nHOST=*->CONSUMERGROUP=UnravelOffsetConsumer→action=write\nHOST=*->CONSUMERGROUP=UnravelOffsetConsumer→action=describe\nHOST=*->TOPIC=__consumer_offsets→action=read\nHOST=*->TOPIC=__consumer_offsets→action=write\nHOST=*->TOPIC=__consumer_offsets->action=describe For further details see Using Kafka with Sentry Authorization in the Cloudera Distribution of Apache Kafka documentation. Kafka with Ranger authorization The following privilege must be granted using Ranger for the topic __consumer_offsets . Publish\nConsume\nDescribe For further details, see Security - Create a Kafka Policy in the HDP Security Guide. References For further information see Apache Kafka documentation chapter # 7 Security. " }, 
{ "title" : "For single Kafka clients", 
"url" : "102221-kafka-security.html#UUID-659678a9-c416-1a02-b078-79c927c86d61_N1553513015914", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Kafka \/ Kafka security \/ For single Kafka clients", 
"snippet" : "Create a file named consumerConfig.properties . Add the following properties and copy\/move the file \/usr\/local\/unravel\/etc . You can locate your SSL + Kerberos configuration. ssl.protocol = TLSv1 sasl.mechanism = GSSAPI security.protocol = SASL_SSL sasl.kerberos.service.name = kafka ssl.truststore.l...", 
"body" : "Create a file named consumerConfig.properties . Add the following properties and copy\/move the file \/usr\/local\/unravel\/etc . You can locate your SSL + Kerberos configuration. ssl.protocol = TLSv1\nsasl.mechanism = GSSAPI security.protocol = SASL_SSL\nsasl.kerberos.service.name = kafka\nssl.truststore.location = \/usr\/java\/jdk1.7.0_67-cloudera\/jre\/lib\/security\/jssecacerts1 \nssl.truststore.password = changeit ssl.truststore.type = JKS\nssl.keystore.location = \/opt\/cloudera\/security\/jks\/server.keystore.jks \nssl.keystore.password = password\nssl.keystore.type = JKS\nssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1\nsasl.jaas.config = \\\ncom.sun.security.auth.module.Krb5LoginModule required \\\nuseKeyTab=true \\\nkeyTab=\"\/etc\/keytabs\/kafka.keytab\" \\\nprincipal=\"kafka\/edge-1.uddev.unraveldata.com@UDDEV.UNRAVELDATA.COM\" sasl.mechanism = GSSAPI\nsecurity.protocol = SASL_PLAINTEXT \nsasl.kerberos.service.name = kafka\nsasl.jaas.config = com.sun.security.auth.module.Krb5LoginModule required \\\nuseKeyTab=true \\\nkeyTab=\"\/etc\/keytabs\/kafka.keytab\" \\\nprincipal=\"kafka\/edge-1.uddev.unraveldata.com@UDDEV.UNRAVELDATA.COM\"; Copy\/move consumerConfig.properties to \/usr\/local\/unravel\/etc . Edit \/usr\/local\/unravel\/unravel.properties . Search for com.unraveldata.ext.kafka.clusters . com.unraveldata.ext.kafka.clusters= ClusterName Add the following property using the ClusterName from above. com.unraveldata.ext.kafka. ClusterName .consumer.config=\/usr\/local\/unravel\/etc\/consumerConfig.properties Restart the Kafka monitor daemon unravel_km . service unravel_km restart " }, 
{ "title" : "Miscellaneous", 
"url" : "102222-misc.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Miscellaneous", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Configuring email alerts", 
"url" : "102223-misc-configuring-email-alerts.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Miscellaneous \/ Configuring email alerts", 
"snippet" : "Configure the following properties to set up email alerts. In addition to these properties you must also configure com.unraveldata.login.admins . Property\/Description Set by user Unit Default com.unraveldata.monitoring.alert.email.enabled Enables email alerts. true : enables alerts false : disables ...", 
"body" : "Configure the following properties to set up email alerts. In addition to these properties you must also configure com.unraveldata.login.admins . Property\/Description Set by user Unit Default com.unraveldata.monitoring.alert.email.enabled Enables email alerts. true : enables alerts false : disables boolean true com.unraveldata.report.user.email.domain Default email domain used for email alerts, localhost.local. [empty] string localhost.local mail.smtp.from Used for email \"from\" and \"reply-to\" headers. [empty] string unravel.noreply@unraveldata.com mail.smtp2.from Used for email \"from\" and \"reply-to\" headers. [empty] string unravel.noreply@unraveldata.com mail.smtp.port smtp mail port. integer 25 mail.smtp.auth Enable\/ SMTP authentication. Note : If true then mail.smtp.user and mail.smtp.pw must be set as they are used when connecting. boolean false mail.smtp.starttls.enable Use start-TLS. boolean false mail.smtp.ssl.enable Use SSL right from the start.string. boolean false mail.smtp.user Username for SMTP authentication Note : If mail.smtp.auth =true you must set this property. Optional string - mail.smtp.pw Password for SMTP authentication Note : If mail.smtp.auth =true you must set this property. Optional string - mail.smtp.host Host for SMTP server. string localhost mail.smtp.localhost A domain name for apparent sender; must have at least one dot (for example, organization.com). string localhost.local mail.smtp.debug Enable debug mode. boolean false " }, 
{ "title" : "Creating Active Directory Kerberos principals and keytabs for Unravel", 
"url" : "102224-kerberos-create-active-dir.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Miscellaneous \/ Creating Active Directory Kerberos principals and keytabs for Unravel", 
"snippet" : "Define HOST Variable for Unravel Server as an FQDN. (Replace UNRAVEL_HOST with your host's FQDN): HOST= UNRAVEL_HOST Define the REALM Variable. (Use upper case for all; replace EXAMPLEDOTCOM with your realm): REALM= EXAMPLEDOTCOM Create the Active Directory (AD) Kerberos Principals and Keytabs. Use ...", 
"body" : "Define HOST Variable for Unravel Server as an FQDN. (Replace UNRAVEL_HOST with your host's FQDN): HOST= UNRAVEL_HOST Define the REALM Variable. (Use upper case for all; replace EXAMPLEDOTCOM with your realm): REALM= EXAMPLEDOTCOM Create the Active Directory (AD) Kerberos Principals and Keytabs. Use the two variables you defined above to replace the red text. Verify that the Unravel Server host is running ntpd service and that time is accurate. For proper Kerberos operation with AD-KDC, DNS entries, including reverse DNS entries, must be in place. On AD server, logged in as AD Administrator, add two Managed Service Accounts unravel and hdfs : Open the Active Directory Users and Computers snap-in. Confirm that the Managed Service Account container exists under the target REALM . Right-click the Managed Service Account container and choose New->User . Set names ( unravel and hdfs ) to account in first screen and click Next . Set a strong password to account (the password won't be used) and: Check Password never expires . Uncheck Password must be changed . Check Password cannot be changed . Right-click the created user, choose Properties , and select the Account tab. In the Account Options panel, check Kerberos AES256-SHA1 . On AD server, logged in as AD Administrator, create the Service Principal Names: Run these commands in a cmd or powershell console. setspn -A unravel\/ HOST unravel \nsetspn -A hdfs\/ HOST hdfs On AD server, logged in as AD Administrator, generate keytab files that Unravel Server will use to authenticate with Kerberos using the ktpass utility in Active Directory. ktpass -princ unravel\/ HOST @ REALM -mapUser unravel -Target REALM +rndPass -out unravel.keytab -ptype KRB5_NT_PRINCIPAL -crypto AES256-SHA1 \nktpass -princ hdfs\/ HOST @ REALM -mapUser hdfs -Target REALM +rndPass -out hdfs.keytab -ptype KRB5_NT_PRINCIPAL -crypto AES256-SHA1 Copy the two keytabs ( unravel.keytab and hdfs.keytab ) from AD server to the Unravel Server at HOST into \/etc\/keytabs\/ (create the destination directory if need be) and then run these commands. sudo chmod 700 \/etc\/keytabs\/*\nsudo chown unravel:unravel \/etc\/keytabs\/unravel.keytab\nsudo chown hdfs:hdfs \/etc\/keytabs\/hdfs.keytab Assurances : hdfs.keytab is only usable on Unravel Server and is only used to access HDFS log files and Hive Metastore (if applicable). " }, 
{ "title" : "Enable authentication for the Unravel Elastic daemon", 
"url" : "102225-misc-enable-auth-unravel-elastic-daemon-.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Miscellaneous \/ Enable authentication for the Unravel Elastic daemon", 
"snippet" : "You must have v4.5.1.0 in order to enable authentication. The auth plugin does not get installed as part of rpm install. Login to your Unravel host. Execute bin\/elasticsearch-plugin to install the ES auth plugin. cd \/usr\/local\/unravel\/elasticsearch\/ bin\/elasticsearch-plugin install -b file: ZIP_FILE...", 
"body" : "You must have v4.5.1.0 in order to enable authentication. The auth plugin does not get installed as part of rpm install. Login to your Unravel host. Execute bin\/elasticsearch-plugin to install the ES auth plugin. cd \/usr\/local\/unravel\/elasticsearch\/\nbin\/elasticsearch-plugin install -b file: ZIP_FILE_NAME Example using the current location of the auth-plugin zip file. sudo \/usr\/local\/unravel\/elasticsearch\/bin\/elasticsearch-plugin install \n-b \nfile:\/\/\/usr\/local\/unravel\/es-auth-plugin\/unravel-es-auth-plugin_4.5.1.0rc0169-1.0.zip Generate an encrypted password using \/usr\/local\/unravel\/install_bin\/pw_encrypt.sh . \/usr\/local\/unravel\/install_bin\/pw_encrypt.sh Example output: ENC(Hsmrxf1LGHNzpqKHxV\/2rw==) Add com.unraveldata.es.rest.password to \/usr\/local\/unravel\/etc\/unravel.properties . Set it to the password you just generated (just the string without the parentheses). com.unraveldata.es.rest.password= ENCRYPTED_PASSWORD Example: com.unraveldata.es.rest.password=Hsmrxf1LGHNzpqKHxV\/2rw== Check that Unravel auth plugin is installed. curl \"http:\/\/localhost:4171\/_cat\/plugins\" curl -H \"Authorization:UnravelDataBasic Hsmrxf1LGHNzpqKHxV\/2rw==\" \"http:\/\/localhost:4171\/_cat\/plugins\"\n\nunravel_s_1 UnravelDataAuth 4.5.1.0rc0169 Restart all daemons. \/etc\/init.d\/unravel_all.sh restart To verify the com.unraveldata.es.rest.password is correctly set, curl to internal elastic search with the wrong password. You should receive a response of not authorized . Example: curl -H \"Authorization:UnravelDataBasic 3I3EbODrX4LyJr\/metiJKQ==\" \"http:\/\/localhost:4171\/_cat\/“\nnot authorized curl to internal elastic search with the correct password. Example: curl -H \"Authorization:UnravelDataBasic Hsmrxf1LGHNzpqKHxV\/2rw==\" \"http:\/\/localhost:4171\/_cat\/“\n=^.^=\n\/_cat\/allocation\n\/_cat\/shards\n\/_cat\/shards\/{index}\n\/_cat\/master\n\/_cat\/nodes\n\/_cat\/tasks\n\/_cat\/indices\n\/_cat\/indices\/{index}\n\/_cat\/segments\n\/_cat\/segments\/{index}\n\/_cat\/count\n\/_cat\/count\/{index}\n\/_cat\/recovery\n\/_cat\/recovery\/{index}\n\/_cat\/health\n\/_cat\/pending_tasks\n\/_cat\/aliases\n\/_cat\/aliases\/{alias}\n\/_cat\/thread_pool\n\/_cat\/thread_pool\/{thread_pools}\n\/_cat\/plugins\n\/_cat\/fielddata\n\/_cat\/fielddata\/{fields}\n\/_cat\/nodeattrs\n\/_cat\/repositories\n\/_cat\/snapshots\/{repository}\n\/_cat\/templates\n If you do not see output similar to above verify: The password is correct. The call was properly formed. If necessary, repeat steps 1-8. " }, 
{ "title" : "Testing", 
"url" : "102225-misc-enable-auth-unravel-elastic-daemon-.html#UUID-92dcbb92-9b53-c7d2-0815-b24cd3deedd0_section-5cf5da85b7a0f-idm45390832616000", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Miscellaneous \/ Enable authentication for the Unravel Elastic daemon \/ Testing", 
"snippet" : "Try steps 7 and 8 with the following endpoints to make sure it is working as expected. curl \"localhost:4171\/_cat\/shards\" curl \"localhost:4171\/_cluster\/health?pretty\" curl \"localhost:4171\/_nodes\/stats?pretty\" Run an app and see if it appears in the UI....", 
"body" : "Try steps 7 and 8 with the following endpoints to make sure it is working as expected. curl \"localhost:4171\/_cat\/shards\" curl \"localhost:4171\/_cluster\/health?pretty\" curl \"localhost:4171\/_nodes\/stats?pretty\" Run an app and see if it appears in the UI. " }, 
{ "title" : "Encrypting passwords in Unravel properties and settings", 
"url" : "102226-misc-encrypting-passwords.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Miscellaneous \/ Encrypting passwords in Unravel properties and settings", 
"snippet" : "Unravel Server includes a command-line utility, pw_encrypt.sh , that can encrypt passwords (or anything alpha-numeric property value deemed sensitive). Sample run of pw_encrypt.sh : sudo \/usr\/local\/unravel\/install_bin\/pw_encrypt.sh ... [Password:] The text you enter on the keyboard will not be displ...", 
"body" : "Unravel Server includes a command-line utility, pw_encrypt.sh , that can encrypt passwords (or anything alpha-numeric property value deemed sensitive). Sample run of pw_encrypt.sh : sudo \/usr\/local\/unravel\/install_bin\/pw_encrypt.sh\n...\n[Password:] The text you enter on the keyboard will not be displayed. After you press Enter ( Return key), it displays a message like: ENC(gMJ5kx\/QioHJsum9rmqKROG0DRqbU51Z) This result, including the ENC() part, can be put into \/usr\/local\/unravel\/etc\/unravel.properties instead of the verbatim (unencrypted) raw password. How it works The file \/usr\/local\/unravel\/etc\/entropy contains random text that is used to do encryption. This file is created only once during installation (or upgrade, if missing) using strong random numbers only available locally. If the entropy value is changed, stored encrypted passwords will become invalid. Salt is included so that if two passwords are identical, the encrypted texts are not in order to avoid inadvertently revealing clues. Passwords are redacted from diagnostic or logs reports, but even if the encrypted form were accidentally transmitted or visible in an online meeting because the entropy file is never included, the encrypted value would be impossible to decrypt. " }, 
{ "title" : "Importing a private certificate into Unravel truststore", 
"url" : "102227-misc-import-private-cert.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Miscellaneous \/ Importing a private certificate into Unravel truststore", 
"snippet" : "You can import a private certificate in the Unravel truststore. Unravel bundles a wrapper script to help import self-signed certificates into the truststore. Prerequisites: openssl command Python 2.7 installed Limitation: Doesn’t work with multiple certificates that have the same issuer name. Script...", 
"body" : "You can import a private certificate in the Unravel truststore. Unravel bundles a wrapper script to help import self-signed certificates into the truststore. Prerequisites: openssl command Python 2.7 installed Limitation: Doesn’t work with multiple certificates that have the same issuer name. Script Location: \/usr\/local\/unravel\/install_bin\/cert_check.py Arguments: Required: --host hostname --port port number Optional: --storepass truststore password --keystore truststore file path default: \/usr\/local\/unravel\/jre\/lib\/security\/cacerts> Example: Log into the server. Run the Python script to import the cert. sudo \/usr\/local\/unravel\/install_bin\/cert_check.py --host test.unraveldata.com --port 8443 If the certificate is not already in the trust store it prompts for confirmation. Type y to automatically import it into Unravel truststore. Cert not found in the truststore do you want to add new cert to truststore [y\/n]\ny\nAdding new certs in \/usr\/local\/unravel\/jre\/lib\/security\/cacerts with alias name test.unraveldata.com \nOwner: CN=*unraveldata\nIssuer: CN=*unraveldata\nSerial Number: 409be60a\nValid from: Fri Sep 14 21:13:43 PDT 2019 until: Sun Aug 21 21:13:43 PDT 2019\nCertificate fingerprints:\n MD5: D1:16:B9:8D:22:61:48:AB:C1:43:28:89:BC:97:81:F4\n SHA1 A6:63:5E:B5:84:3F:B6:C2:33:29:C2:72:E0::72:A7:FE:D6:9F:B0:55\n SHA256: E6:67:E9:B5:85:F7:D6:F2:37:A9:F2:*2:B0::72:B8:EE:D7:92:D0:75\nSignature algorithm name: SHA512withRSA\nSubject Public Key Algorithm: 2048-bit RSA key\nVersion: 3\n\nExtensions:\n\n#1: ObjectID: 2.5.29.14 Criticality=fale\nSubjectKeyIdentifier [\nKey Identifer [\n0000: 89 E3 E0 5C 69 AZ 83 23 9D 80 95 A3 3F 6B 48 82 ...\\i..#....?kH\n0010: 94 09 ED DF ....\n]\n]\n\n\nTrust this certificate? [no]: yes\nCertificate was added to keystore Type n to print it on the screen for manual import. Cert not found in the truststore do you want to add new cert to truststore [y\/n]\nn\n-----BEGIN CERTIFICATE-----\nMIIC@TCAAACjkjdfsaasdfjiiACASJklsdfi'msdf01ej01d9FMRqodjaldasdk0\nsdfj0q4rwF+lqctaMNDp0asdfLM+MKDJALD\/FSAL9dajaj134kjlZE0had9\/sadfj\nMIIC@TCAAACjkjdfsaasdfjiiACASJklsdfi'msdf01ej01d9FMRqodjaldasdk0l\nsdfj0q4rwF+lqctaMNDp0asdfLM+MKDJALD\/FSAL9daj'j134kjlZE0had9\/sadfj\nMIIC@TCAAACjkjdfsaasdfjiiACASJklsdfi'msdf01ej01d9FMRqodjaldasdk0l\nsdfj0q4rwF+lqctaMNDp0asdfLM+MKDJALD\/FSAL9daj'j134kjlZE0had9\/sadfj\nMIIC@TCAAACjkjdfsaasdfjiiACASJklsdfi'msdf01ej01d9FMRqodjaldasdk0l\nsdfj0q4rwF+lq'j134kjlZE0had9\/sadfjctaMNDp0asdfLM+MKDJALD\/FSAL9daj\nj134kjlZE0had9\/sadfjMIIC@TCAAACjkjdfsafi'msdf01ej01d9FMRqodjaldas\nk0lasdfjiiACASJklsdsdfj0q4rwF+lqctaMNDp0asdfLM+MKDJALD\/FSAL9daj'j\n134kjlZE0aasdfjiiACASJklsdMIIC@TCAAACjkjdfsfi'msdf01ej01d9FMRqodj\naldasdk0lsdfj0q4rwF+lqctaMNDp0asdfLM+MKDJALD\/FSAL9daj'j134kjlZE0h\nad9\/sadfjMIjkjdfsaasdfjiiACASJklsdfi'mIC@TCAAACsdf01eodjaldasdk0l\nj01d9FMRqsdfj0q4rwF+lqctaMNDp0asdfLM+MKDJALD\/FSAL9daj'j134kjlZE0j\nhad9\/sadfjMIIC@TCAAACjkjdfsaasdfjiiACASJklsdfi'msdf01ej01d9FMRqod\njaldasdk0lsdfj0q4rwF+lqctaMNDp0asdfLM+MKDJALD\/FSAL9daj'j134kjlZE0\njiiACASJklsdfi'msdf01ej01==\n-----END CERTIFICATE-----\nCert not found in trustore please add the above cert to truststore " }, 
{ "title" : "Running Unravel daemons with custom user", 
"url" : "102228-misc-unravel-daemon-w-custom-user.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Miscellaneous \/ Running Unravel daemons with custom user", 
"snippet" : "Unravel Server daemons run as a local user unravel by default. You might want to run as a different user, for example: Run as hdfs or mapr because this user has access to log files needed by Unravel on a non-Kerberos cluster with simple Unix security. Run as a customized service account with a name ...", 
"body" : "Unravel Server daemons run as a local user unravel by default. You might want to run as a different user, for example: Run as hdfs or mapr because this user has access to log files needed by Unravel on a non-Kerberos cluster with simple Unix security. Run as a customized service account with a name aligned with your local policies. The \"run-as\" user should match the user you targeted for setfacl commands done during installation on a Kerberos-enabled cluster. Use the procedure below to change which user Unravel utilizes. This change only needs to be done once; it will be preserved by RPM upgrades. Procedure to switch user Run the following command to switch running Unravel daemons to user and with group . Replace both with valid names, without the curly braces; see the scenarios below. sudo \/usr\/local\/unravel\/install_bin\/switch_to_user.sh user group Scenario USER GROUP MapR installation mapr mapr CDH or HDP with simple Linux security hdfs hadoop or hdfs Kerberos enabled on CDH\/HDP and Sentry\/Ranger\/setfacl access already enabled for custom local user \"foo\" in group \"foo\". foo foo Kerberos enabled on CDH\/HDP and Sentry\/Ranger\/setfacl access already enabled for local user \"hdfs\" in group \"hadoop\". hdfs hadoop Start Unravel daemons. sudo \/etc\/init.d\/unravel_all.sh start Effect The effect of the switch_to_user.sh is: \/etc\/unravel_ctl defines environment variables RUN_AS and USE_GROUP . HDFS_KEYTAB_PATH and HDFS_KERBEROS_PRINCIPAL environment variables are removed from \/usr\/local\/unravel\/etc\/unravel.ext.sh . \/usr\/local\/unravel\/ and \/srv\/unravel\/* are recursively set to ownership RUN_AS: USE_GROUP . \/srv\/unravel\/tmp_hdfs\/ directory is removed (no longer needed). Log files in \/srv\/unravel\/log_hdfs are moved to \/usr\/local\/unravel\/logs . \/srv\/unravel\/log_hdfs directory is removed (no longer needed). The umask of the run-as daemon can now be more restrictive than 022; it can be 007 or 077. The permission ( chmod ) bits of \/usr\/local\/unravel and \/srv\/unravel can remove Group and Other visibility, if desired. " }, 
{ "title" : "Setting retention time in Unravel server", 
"url" : "102229-misc-retention-time-in-server.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Miscellaneous \/ Setting retention time in Unravel server", 
"snippet" : "To adjust the retention time ( time horizon ), you need to set\/change three (3) properties in \/usr\/local\/unravel\/etc\/unravel.properties . com.unraveldata.retention.max.days : number of days to keep the heaviest data (such as error logs and drill-down details) in the SQL Database. com.unraveldata.his...", 
"body" : "To adjust the retention time ( time horizon ), you need to set\/change three (3) properties in \/usr\/local\/unravel\/etc\/unravel.properties . com.unraveldata.retention.max.days : number of days to keep the heaviest data (such as error logs and drill-down details) in the SQL Database. com.unraveldata.history.maxSize.weeks : number of weeks retained for search results in Elastic Search. When changing these settings, be aware that long retention requires significant disk space. As a rule of thumb, each map-reduce or Spark job requires about 1 MB of disk space; you can store approximately 1000 jobs per 1 GB of disk. Open \/usr\/local\/unravel\/etc\/unravel.properties file. # vi \/usr\/local\/unravel\/etc\/unravel.properties Search for and set the following properties. If not found, add them. com.unraveldata.retention.max.days=30\ncom.unraveldata.history.maxSize.weeks=5 com.unraveldata.retention.max.days is the most significant factor in controlling disk space usage in the database used by Unravel. After changing any of the properties, restart Unravel for the change to take effect. # sudo \/etc\/init.d\/unravel_all.sh restart " }, 
{ "title" : "Using a private certificate authority with Unravel", 
"url" : "102230-misc-private-certificate-authority.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Miscellaneous \/ Using a private certificate authority with Unravel", 
"snippet" : "A private certificate authority (CA) is often used for signing certificates of non-public machines. Unravel server contains a bundled JRE that has well-known, public CAs. In order for Unravel to make REST requests to collect cluster metadata from HTTPS endpoints, it needs to know about your private ...", 
"body" : "A private certificate authority (CA) is often used for signing certificates of non-public machines. Unravel server contains a bundled JRE that has well-known, public CAs. In order for Unravel to make REST requests to collect cluster metadata from HTTPS endpoints, it needs to know about your private CA. Use one of the techniques below and restart all Unravel daemons with sudo \/etc\/init.d\/unravel_all.sh restart after making the change. \/path\/to\/jks_keystore is the path for your local settings. Externally managed JKS keystore The bundled JRE will use an external keystore ( jssecacerts ) in preference over the built-in one ( cacerts ). Simply create a symlink as shown to your JKS keystore: # chmod 444 \/path\/to\/jks_keystore\n# ln -s {\/path\/to\/jks_keystore} \/usr\/local\/unravel\/jre\/lib\/security\/jssecacerts Note: Substitute \/path\/to\/jks_keystore and ensure that the target file is updated whenever your CA certificates are updated. Externally managed JRE or JDK with curated cacerts An external JRE or JDK is often maintained for local use so that the cacerts or file it contains is up-to-date. If this is convenient, you can edit \/usr\/local\/unravel\/etc\/unravel.ext.sh and change the line for JAVA_HOME. Java 1.8 is required. The environment variable should point to the directory that contains bin\/java . If you are using unlimited encryption strength for your Hadoop services, be sure that the JRE\/JDK you specify also has unlimited encryption strength. Substitute your local settings \/usr\/java\/jdkl1.8 . For example: export JAVA_HOME \/usr\/java\/jdk1.8 Adding a CA certificate to bundled JRE You can add a CA certificate to the JRE that is bundled with Unravel server. First, copy cacerts to jssecacerts so that an upgrade of Unravel will preserve your change: # cd \/usr\/local\/unravel\/jre\/lib\/security\n# sudo cp -p cacerts jssecacerts List contents of the jssecacerts keystore: # sudo \/usr\/local\/unravel\/jre\/bin\/keytool -list -keystore jssecacerts Import\/insert a new certificate: Substitute your local values for mycompanyca and something.cer when you execute this command. Afterwards, repeat the listing step to see the effect of the insert. # sudo \/usr\/local\/unravel\/jre\/bin\/keytool -keystore jssecacerts -importcert -alias mycompanyca -file something.cer \n# sudo \/usr\/local\/unravel\/jre\/bin\/keytool -list -keystore jssecacerts " }, 
{ "title" : "OnDemand reports", 
"url" : "102231-ondemand.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ OnDemand reports", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "OnDemand configurations", 
"url" : "102232-ondemand-configurations.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ OnDemand reports \/ OnDemand configurations", 
"snippet" : "These configurations control the features which use OnDemand: Sessions File Reports Forecasting Small Files Cluster Optimization Cluster KPIS Queue Analysis Top X (including User Reports) Cluster Discovery Cloud Mapping Per Host Services and Versions Compatibility Configuration properties for report...", 
"body" : "These configurations control the features which use OnDemand: Sessions File Reports Forecasting Small Files Cluster Optimization Cluster KPIS Queue Analysis Top X (including User Reports) Cluster Discovery Cloud Mapping Per Host Services and Versions Compatibility Configuration properties for reports Migration planning Note: As of 29 July 2019, the default is false for new installations of Unravel 4.5.3.0 and later. Property\/Description Set by user Unit Default unravel.python.reporting.cloudreport.enable Controls the generation of Migration Reports. true : Reports can be created. false : Reports cannot be created. boolean true (See note.) Queue analysis Property\/Description Set by user Unit Default com.unraveldata.report.queue.metrics.sensor.enabled Enables or disables queue metric sensor. boolean true com.unraveldata.report.queue.http.retries YARN Resource Manager HTTP connection retries. count 3 com.unraveldata.report.queue.http.retry.period.msec YARN Resource Manager HTTP connection retry wait period. ms 0 com.unraveldata.report.queue.http.timeout.msec YARN Resource Manager HTTP connection timeout. ms 10000 com.unraveldata.report.queue.poll.interval.msec How often queue metric sensor polls polls YARN Resource Manager. ms 60000 unravel.python.queueanalysis.daterange.span UI report date picker range. days 30 unravel.python.queueanalysis.metrics.scale UI rendered graph metrics scale factor. integer 1000 Sessions Property\/Description Set by user Unit Default com.unraveldata.session.enabled Enables Sessions features tab in the UI. true : Sessions enabled. false : Sessions disabled. You must restart the ngui and ondemand daemons when changing the value. boolean true com.unraveldata.session.max.autotune.runs Maximum number of runs allowed in an auto-tune session. count 8 com.unraveldata.session.dynamicAllocation.enabled If set to true Spark sessions recommends dynamic allocation. If users don't want that they need to explicitly set it to false. boolean true com.unraveldata.session.spark1.submit.command The command to use for Spark1 applications while applying the session run. string spark-submit com.unraveldata.session.spark2.submit.command The command to use for Spark2 applications while applying the session run. string spark2-submit Small files and file reports You must restart the unravel_ondemand daemon for any changes to take effect Property\/Description Set by user Unit Default com.unraveldata.ngui.sfhivetable.schedule.interval Frequency, in days, in which to trigger FSimage extraction, for example, every 3 days The scheduler schedules extraction relative to the 1st of the month and then sets each extraction such that it 1st, 1st + X days, 1st + 2X days until 1st + nX days crosses into the next month, at which point the schedule resets to the 1st. See below for an example. Format: X d day 1d com.unraveldata.ngui.sfhivetable.schedule.time Specify the time to download in hours (using 24 hour time) the FSimage. Format: two digits between 00 and 23. two digits (member of set) 00 unravel.python.reporting.files.external_fsimage_dir Directory for fsimage when skip_fetch_fsimage =true. The fsimage externally fetched is expected to be in this directory. Unravel uses the latest file in this directory which starts with \" fsimage_\". This directory must be different than the Unravel's internal directory, i.e., \/srv\/unravel\/tmp\/reports\/fsimage. string - unravel.python.reporting.files.skip_fetch_fsimage If hdfs admin privileges can not be granted, set this to true to allow Unravel's Ondemand process to use an externally fetched FSimage. true : Ondemand etl_fsimage process does not fetch FSimage from name node. Instead, the FSimage is expected to be available in directory specified by unravel.python.reporting.files.external_fsimage_dir . boolean false In order for the configuration changes to take effect, unravel_ondemand and unravel_ngui daemons need to be restarted. \/etc\/init.d\/unravel_ngui restart\n\/etc\/init.d\/unravel_ondemand restart Property\/Description Set by user Unit Default unravel.python.reporting.files.disable Enables or disables Unravel ability to generate Small Files and File Reports. false : enables the Small Files and Files reports in both the backend and UI. true : disables the Small Files and Files reports. in both the backend and UI. boolean false unravel.python.reporting.files.hive_database Hive Database where Ondemand creates five Hive tables (four temporary, one permanent) for Small Files\/File Reports. When not set, tables are created in the default Hive Database. In addition, the Hive queries used for this feature run against default MR queue. It must point to a valid Hive database. string default database unravel.python.reporting.files.hive_mr_queue The Hcive queries ran by Ondemand process run against this MR queue. It must point to a valid MR queue. string default These are global properties which apply to both Small Files Report and File Reports. They each have equivalent \"local\" properties. The \"local\" property takes precedence over the equivalent global property. Should you unset\/delete any of the below properties and their equivalent properties in Small Files Report or Files Report, Unravel has hard-coded values to ensure your reports are generated. Property\/Description Set by user Unit Default unravel.python.reporting.files.files_use_avg_file_size_flag true : average of all the files is used against the threshold criteria and either all the files are accepted and counted or rejected and not counted as per the criteria. false :  absolute file size is used against the threshold criteria and a file is accepted\/counted or rejected\/not counted as per the criteria. Optional boolean - unravel.python.reporting.files.min_parent_dir_depth Directory depth to end the search at. For instance, if depth=2, search begins below two levels. Give given HDFS_root\/one\/two the search starts in the directory two Optional count - unravel.python.reporting.files.max_parent_dir_depth Directory depth to end search at. Maximum is 50. For instance, if depth=5 given HDFS_root\/one\/two\/three\/four\/five\/six\/seven the search ends at five . Optional count - unravel.python.reporting.drill_down_subdirs_flag When true a file is accounted for (listed) in all of its ancestors.   false : a file is accounted in only its immediate parent. This allows Unravel to find a specific directory with maximum number of files matching the size criteria. true : lists each file with its ancestors. For example given the directory structure is \/one\/two false : \/ - lists files in \/. \/one - lists files in one. \/one\/two - lists files in \/one\/two. true : \/ - lists files in \/, \/one, and \/one\/two. \/one - lists files in \/one, and \/one\/two. \/one\/two - lists files in \/one\/two. Optional boolean false Small Files You must restart the unravel_ondemand daemon for any changes to take effect. Property\/Description Set by user Unit Default unravel.python.reporting.small_files_use_avg_file_size_flag true : average of all the files is used against the threshold criteria and either all the files are accepted and counted or rejected and not counted as per the criteria. false :  absolute file size is used against the threshold criteria and a file is accepted\/counted or rejected\/not counted as per the criteria. boolean true unravel.python.reporting.files.small_files_min_parent_dir_depth Directory depth to end the search at. For instance, if depth=2, search begins below two levels. Give given HDFS_root\/one\/two the search starts in the directory two count 0 unravel.python.reporting.files.small_files_max_parent_dir_depth Directory depth to end search at. Maximum is 50. For instance, if depth=5 given HDFS_root\/one\/two\/three\/four\/five\/six\/seven the search ends at five . count 10 unravel.python.reporting.files.small_files_drill_down_subdirs_flag When true a file is accounted for (listed) in all of its ancestors.   false : a file is accounted in only its immediate parent. This allows Unravel to find a specific directory with maximum number of files matching the size criteria. true : lists each file with its ancestors. For example given the directory structure is \/one\/two false : \/ - lists files in \/. \/one - lists files in one. \/one\/two - lists files in \/one\/two. true : \/ - lists files in \/, \/one, and \/one\/two. \/one - lists files in \/one, and \/one\/two. \/one\/two - lists files in \/one\/two. boolean false File Reports Property\/Description Set by user Unit Default unravel.python.reporting.files.huge_files_threshold_size File size must be greater than or equal to the threshold_size for it to be counted. bytes 100GB unravel.python.reporting.files.huge_files_min_files The minimum number of files which must meet the threshold_size for the directory to be included. integer 1 unravel.python.reporting.files.huge_files_top_n_dirs Number of directories to show results for. That is only display the results for the top N directories. integer 10 unravel.python.reporting.files.medium_files_max_threshold_size The file size must be less than or equal to the max_threshold_size for it to be counted. bytes 10GB unravel.python.reporting.files.medium_files_min_threshold_size The file size must be greater than or equal to the min_threshold_size for it to be counted. integer 5GB unravel.python.reporting.files.medium_files_min_files The minimum number of files which must fall with the min and max threshold sizes for the directory to be included. integer 5 unravel.python.reporting.files.medium_files_top_n_dirs Maximum number of directories to display, that is, only display the results for the top N directories. integer 20 unravel.python.reporting.files.tiny_files_threshold_size The file size must be less than or equal to the max_threshold_size for it to be counted. bytes 100KB unravel.python.reporting.files.tiny_files_min_files The minimum number of files which must less than or equal to the the threshold size for the directory to be included. integer 10 unravel.python.reporting.files.tiny_files_top_n_dirs Maximum number of directories to display, in other wowrds, only display the results for the top N directories. integer 30 unravel.python.reporting.files.empty_files_min_files The minimum number of files that must empty for the directory to be included. integer 10 unravel.python.reporting.files.empty_files_top_n_dirs Maximum number of directories to display, in other words, only display the results for the top N directories. integer 3 The following four properties are defined per file size type; Size : huge, medium, tiny, or empty Property\/Description Set by user Unit Default Size _files_use_avg_file_size_flag true : average of all the files is used against the threshold criteria and either all the files are accepted and counted or rejected and not counted as per the criteria. false :  absolute file size is used against the threshold criteria and a file is accepted\/counted or rejected\/not counted as per the criteria. boolean false Size _files_min_parent_dir_depth Directory depth to end the search at. For instance, if depth=2, search begins below two levels. Give given HDFS_root\/one\/two the search starts in the directory two integer 0 Size _files_max_parent_dir_depth Directory depth to end search at. Maximum is 50. For instance, if depth=5 given HDFS_root\/one\/two\/three\/four\/five\/six\/seven the search ends at five . integer 10 Size _files_drill_down_subdirs_flag When true a file is accounted for (listed) in all of its ancestors.   false : a file is accounted in only its immediate parent. This allows Unravel to find a specific directory with maximum number of files matching the size criteria. true : lists each file with its ancestors. For example given the directory structure is \/one\/two false : \/ - lists files in \/. \/one - lists files in one. \/one\/two - lists files in \/one\/two. true : \/ - lists files in \/, \/one, and \/one\/two. \/one - lists files in \/one, and \/one\/two. \/one\/two - lists files in \/one\/two. boolean false Top X Property\/Description Set by user Unit Default com.unraveldata.ngui.topx.enabled Controls the generation of Top X reports. true : report enabled. false : report are disabled. boolean true " }, 
{ "title" : "Required", 
"url" : "102232-ondemand-configurations.html#UUID-0930c5e4-5083-d782-4980-ff07452f65d5_sidebar-5d36620e83b19-idm46391458244512", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ OnDemand reports \/ OnDemand configurations \/ Required", 
"snippet" : "General Property\/Description Set by user Unit Default unravel.server.ip FQDN or IP-Address of Unravel Server Required string - com.unraveldata.python.enabled Enable\/disable all ondemand reports and Sessions features in UI. (This property is configured during the OnDemand installation.) boolean true ...", 
"body" : "General Property\/Description Set by user Unit Default unravel.server.ip FQDN or IP-Address of Unravel Server Required string - com.unraveldata.python.enabled Enable\/disable all ondemand reports and Sessions features in UI. (This property is configured during the OnDemand installation.) boolean true HiveServer2 Property\/Description Set by user Unit Default unravel.hive.server2.host FQDN or IP-Address of the HiveServer2 instance. string - unravel.hive.server2.port Port for the HiveServer2 instance. number 10000 Celery Property\/Description Set by user Unit Default com.unraveldata.ngui.proxy.celery string (URL) http:\/\/localhost:5000 unravel.celery.broker.url Optional string (URL) - unravel.celery.result.backend Optional - Hive Metastore Property\/Description Set by user Unit Default javax.jdo.option.ConnectionDriverName JDBC Driver class name for the data store containing the metadata. Examples: MySQL: com.mysql.jdbc.Driver Oracle: oracle.jdbc.driver.OracleDriver Microsoft: com.microsoft.sqlserver.jdbc.SQLServerDriver Required string - javax.jdo.option.ConnectionPassword Password used to access the data store. Required string - javax.jdo.option.ConnectionUserName Username used to access the data store. Required string - javax.jdo.option.ConnectionURL JDBC connection string for the data store containing the metadata of the form: jdbc: DB_Driver :\/\/ HOST : PORT \/hive Example: Oracle: jdbc:oracle:thin:@prodHost:1521:ORCL Microsoft: jdbc:sqlserver:\/\/ jdbc_url Requited string (URL) - If KERBEROS ( unravel.hive.server2.authentication =KERBEROS) set unravel.hive.server2.kerberos.service.name =hive define Kerberos properties . See Configuring Forecasting and Migration Planning Reports for required cluster configurations. See for complete configuration requirements. " }, 
{ "title" : "Configuring forecasting and migration planning reports", 
"url" : "102233-ondemand-enabling-cloud-forecasting.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ OnDemand reports \/ Configuring forecasting and migration planning reports", 
"snippet" : "If your cluster manager is Ambari you must have the Ambari Metrics System (AMS) installed and running in order to generate these reports. HTTPS support is available only for Unravel 4.5.1.0 and up. See . Enable\/Disable migration planning reports Note: As of 29 July 2019, the default is false for new...", 
"body" : "If your cluster manager is Ambari you must have the Ambari Metrics System (AMS) installed and running in order to generate these reports. HTTPS support is available only for Unravel 4.5.1.0 and up. See . Enable\/Disable migration planning reports Note: As of 29 July 2019, the default is false for new installations of Unravel 4.5.3.0 and later. Property\/Description Set by user Unit Default unravel.python.reporting.cloudreport.enable Controls the generation of Migration Reports. true : Reports can be created. false : Reports cannot be created. boolean true (See note.) Cluster properties Make sure the cluster type is set correctly, the default is CDH. Property\/Description Set by user Unit Default com.unraveldata.cluster.name Cluster to connect to if multiple options exist. Required in multi-cluster environments only. Unravel first attempts to match on the cluster ID, and then falls back to matching on the display name. For Ambari, the cluster ID and the display name are equivalent, which is the \"cluster_name\" attribute from the \"\/clusters\" endpoint, for example, http:\/\/HOST:8080\/api\/v1\/clusters\/. For Cloudera Manager, the cluster ID is the \"name\" attribute from the \"\/clusters\" endpoint, for example, http:\/\/HOST:7180\/api\/v17\/clusters\/. Required CSL default com.unraveldata.cluster.type Possible values are DB , CDH , HDP , or MAPR .   set member CDH Ambari Property\/Description Set by user Unit Default com.unraveldata.ambari.manager.url URL of Ambari Manager, for example, https:\/\/$ambariserver:8083. It must start with http:\/\/ or https:\/\/. Required string - com.unraveldata.ambari.manager.username Ambari username. Required string - com.unraveldata.ambari.manager.password Ambari password. Required string - Cloudera Property\/Description Set by user Unit Default com.unraveldata.cloudera.manager.url URL of Cluster Manager, for example, http:\/\/$clouderaserver In order to properly track Impala jobs, make sure that the value of this property does not contain a port number since there is already a separate config for the port. Required string - com.unraveldata.cloudera.manager.username Cloudera manager username. Required string - com.unraveldata.cloudera.manager.password Cloudera manager password. Required string - com.unraveldata.cloudera.manager.port You only need to specify this if your Cloudera Manager is not on port 7180. As of c4.5.4.0 this property is deprecated and the only way to specifiy the CM URL is with com.unraveldata.cloudera.manager.url . Required integer - com.unraveldata.cloudera.manager.api_version Optional and only valid for Cloudera Manager. Specifies the API version number to use, such as \"17\". When not set, Unravel attempts to auto discover the version within the range 16-19. Do not set this property if you are using Impala. Optional integer - " }, 
{ "title" : "Configuring small files and files reports", 
"url" : "102234-ondemand-small-and-file.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ OnDemand reports \/ Configuring small files and files reports", 
"snippet" : "While the Small File Reports and File Reports features are enabled by default, extensive configuration is required. This topic explains how to configure them to make them available through the Unravel UI. This feature runs as hdfs user and uses Hive to insert FSimage data into tables. Small Files an...", 
"body" : "While the Small File Reports and File Reports features are enabled by default, extensive configuration is required. This topic explains how to configure them to make them available through the Unravel UI. This feature runs as hdfs user and uses Hive to insert FSimage data into tables. Small Files and Files Reports can be turned off. See the enable\/disable section for toggle the status of the reports. 1 - Determine how Unravel will access FSimage. Unravel can run as hdfs admin. This allows Unravel to access the FSimage on the Namenode using the hdfs dfsadmin command. See Running Unravel Daemons with Custom User to configure Unravel to run as hdfs admin. For CDH: Check to see if Unravel user has hdfs admin permission. hdfs dfsadmin -report If Unravel has hdfs admin permission, immediately go to Step 2 , otherwise locate Unravel's group name. id unravel Go to CDH HDFS configs page. Locate the Superuser Group property and change the value to result obtained in step 2 above. Unravel can not run as hdfs admin . This is the most common setup. The FSimage must be downloaded for Unravel to generate these reports. Create a cron to download it to the Unravel server. The download can take up to 10 hours depending on the size of FSimage. The time can determine how often cron is\/should be run. If the raw FSimage can be parsed faster into a tab delimited file using the HDFS OIV utility, that can also be part of the cron job. The old image must be deleted before running the cron job. Unravel uses this image to create the report. Therefore, the Small Files and Files reports are analyzing a snapshot, which with the passage of time becomes outdated. Edit \/usr\/local\/unravel\/etc\/unravel.properties and set the following properties. You must restart the unravel_ondemand daemon for any changes to take effect. service unravel_ondemand restart Property\/Description Default unravel.python.reporting.files.skip_fetch_fsimage If hdfs admin privileges can not be granted, set this to true to allow Unravel's Ondemand process to use an externally fetched FSimage. true : Ondemand etl_fsimage process does not fetch FSimage from name node. Instead, the FSimage is expected to be available in directory specified by unravel.python.reporting.files.external_fsimage_dir . false unravel.python.reporting.files.external_fsimage_dir Directory for fsimage when skip_fetch_fsimage =true. The fsimage externally fetched is expected to be in this directory. Unravel uses the latest file in this directory which starts with \" fsimage_\". This directory must be different than the Unravel's internal directory, i.e., \/srv\/unravel\/tmp\/reports\/fsimage. - unravel.python.reporting.files.skip_fetch_fsimage=true;\nunravel.python.reporting.files.external_fsimage_dir=\/srv\/unravel\/tmp\/fsimages\/reports; 3 - Configure your Sentry-Secured CDH or Ranger-Secured HDP cluster. Supported CDH settings for SSL\/TLS SSL\/TLS unravel.hive.server2.authentication Supported Configuration Notes Yes KERBEROS Yes unravel.hive.server2.use.SSL =true unravel.hive.server2.authentication =KERBEROS unravel.hive.server2.kerberos.service.name =hive com.unraveldata.kerberos.principal = unravel kerberos principal com.unraveldata.kerberos.keytab.path = keytab path No KERBEROS Yes unravel.hive.server2.authentication =KERBEROS unravel.hive.server2.kerberos.service.name =hive com.unraveldata.kerberos.principal = unravel kerberos principal com.unraveldata.kerberos.keytab.path = keytab path No NONE (No authentication) Yes No security config properties needed. Yes NOSASL, LDAP, or CUSTOM No N\/A No NOSASL, LDAP, or CUSTOM No N\/A Settings for Sentry-Secured CDH cluster If your CDH cluster is secured with Sentry, Unravel's Small Files and Files Reports (on the Data Insights tab) won't contain any data until you change the permissions on your cluster as follows: If upgrading from Unravel 4.5.3.0 onwards just perform step 2. If upgrading from Unravel 4.5.2.x or earlier just perform steps 2, 8 and 9. Grant dfsadmin privilege or this alternative to the Unravel user. To grant this privilege, set\/update the HDFS configuration property dfs.cluster.administrators and restart all services affected by this change. If you can't grant dfsadmin privilege to the Unravel user, follow the steps in the Troubleshooting section of Triggering an import of FSImage . These steps allow Unravel to use the FSImage you import manually. Remove the following line from \/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/scripts\/hive_queries_reporting\/hive_properties.hive : ADD JAR {UDF_JAR_LOC}\/ unravel-udf-0.2.jar This line is no longer required because UDFs jars are stored locally on the HiveServer2 node in Sentry-secured CDH clusters. For Unravel versions, 4.5.x and earlier, remove ADD JAR {UDF_JAR_LOC}\/unravel-udf-0.1.jar. Allow the Unravel user to submit Hive queries to a YARN queue. If you can't allow this on your default YARN queue, you can grant this permission on a different YARN queue: Create a different YARN queue for the Unravel user. Give the Unravel user permission to submit Hive queries to the new YARN queue. In unravel.properties on Unravel Server, set unravel.python.reporting.files.hive_mr_queue to the new YARN queue. For details on this property, see Unravel Properties. Create a new Sentry role, unravel_role , using the beeline CLI as the Hive admin user (hive by default). create role unravel_role Map the unravel group to unravel_role : grant role unravel_role to group unravel Set HDFS access privileges for the Unravel user: The Unravel user needs to copy FSImage to \/tmp\/fsimage , so allow this access as follows: grant all on uri 'hdfs:\/\/\/tmp\/fsimage' to role unravel_role; Grant the Unravel user the following privileges on the Hive tables under the default database: Create\/drop\/truncate\/alter Hive tables Run\/select\/insert queries on Hive tables Alternatively, you can: Use a different database for the Unravel user, such as unravel_db . Give the Unravel user the above permissions on that database: grant all on database unravel_db to role unravel_role; In unravel.properties on Unravel Server, set unravel.python.reporting.files.hive_database to the alternate database. For details on this property, see Unravel Properties. For example, unravel.python.reporting.files.hive_database=unravel_db Add a JAR and create temporary UDFs: As the Unravel user, copy the Unravel JAR, \/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/jars\/fsimage_reports\/unravel-udf-0.2.jar , to the HiveServer2 aux JAR path. For Unravel versions4.5.x and earlier, use \/usr\/local\/unravel\/ondemand\/unravel-python-1.0.0\/jars\/fsimage_reports\/unravel-udf-0.1.jar . The HiveServer2 aux JAR path is specified by the Hive Auxillary Jars directory (in a Hive Service wide configuration) OR by hive.reloadable.aux.jars.path (in a Hive Server2 hive_site.xml configuration). Get your settings from Cloudera Manager. The goal is to get the JAR into the path HiveServer2 recognizes for aux JARs. For more information, see Using Cloudera Manager to Create User-Defined Functions (UDFs) with HiveServer2 . Hive user and group should own this JAR. For example, if the HiveServer2 aux JAR path is \/tmp\/hive_jars , this directory and the copied jar must be owned by hive admin user ( hive by default): chown -R hive:hive \/tmp\/hive_jars Grant the Unravel user access to this JAR: grant all on uri 'file:\/\/\/tmp\/hive_jars\/' to role unravel_role; In Cloudera Manager, restart HiveServer2. Use the show grant role unravel_role command to verify that permissions now look like this: +--------------------------------------------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+\n\n| database | table | partition | column | principal_name | principal_type | privilege | grant_option | grant_time | grantor |\n\n+--------------------------------------------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+\n\n| file:\/\/\/tmp\/hive_jars\/unravel-udf-0.2.jar | | | | unravel_role | ROLE | * | false | 1550829915318000 | -- |\n\n| unravel_db | | | | unravel_role | ROLE | * | false | 1550829820331000 | -- |\n\n| hdfs:\/\/\/tmp\/fsimage | | | | unravel_role | ROLE | * | false | 1550830532328000 | -- |\n\n+--------------------------------------------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+\n\n For prior Unravel versions, 4.5.x and earlier, the output should list file:\/\/\/tmp\/hive_jars\/unravel-udf-0.1.jar HDP settings for TLS SSL\/TLS unravel.hive.server2.authentication Supported Configuration Notes Yes KERBEROS Not tested No KERBEROS Not tested No NONE (No authentication) Not tested Yes NOSASL, LDAP, or CUSTOM No N\/A No NOSASL, LDAP, or CUSTOM No N\/A Settings for Ranger-Secured HDP clusters If your HDP cluster is secured with Ranger, Unravel's Small Files and Files Reports (on the Data Insights tab) won't contain any data until you change the permissions on your cluster as follows: Grant dfsadmin access or this alternative to the Unravel user. For example: Allow the Unravel user to connect to HiveServer2. Allow the Unravel user to CREATE, TRUNCATE, ALTER, DROP, INSERT, and SELECT Hive tables. For example: Allow the Unravel user to change or switch the Hive database. For example: Allow the Unravel user to submit Hive queries to a particular YARN queue. For example: Allow the Unravel user to use concurrent Hive queries: Set hive.txn.manager=DbTxnManager and hive.support.concurrency=true in Hive configuration. The Unravel user must be able to set the following properties dynamically: Set the following parameters in custom hive-site properties. hive.auto.convert.join\nhive.support.sql11.reserved.keywords\nhive.variable.substitute\nmapred.job.queue.name\nmapreduce.map.java.opts\nmapreduce.map.memory.mb\n This can be done by setting the following key\/value pair in custom hive-site.xml . property name = hive.security.authorization.sqlstd.confwhitelist.append\nproperty value = \nhive\\\\.auto\\.convert\\.join|hive\\.support\\.concurrency|hive\\.support\\.sql11\\.reserved\\.keywords|hive\\.txn\\.manager|hive\\.variable\\.substitute|mapred\\.job\\.queue\\.name|mapreduce\\.map\\.java\\.opts|mapreduce\\.map\\.memory\\.mb Allow Unravel user TempUDFAdmin privileges at global level. 4 - Run the run_small_files task. curl -v “http:\/\/localhost:5000\/small-files-etl” Debugging tips The relevant log file is \/usr\/local\/unravel\/ondemand\/logs\/unravel_ondemand.out The etl_fsimage task extracts FSimage and runs File Reports . The run_small_files task runs ad hoc Small Files Report . It is triggered every day at 00:00 UTC. Run the following command to trigger run_small_files it manually. (You must trigger after the initial install.) curl -v “http:\/\/localhost:5000\/small-files-etl” Run one of the following commands to display the progress of the etl_fsimage task. egrep 'ETL_FSIMAGE|FSIMAGE_REPORTS_UTILS' unravel_ondemand.out grep etl_fsimage\\(\\) unravel_ondemand.out Run one of the following commands to display the progress of the run_small_files which is started whenever Small Files Report is triggered from UI. egrep 'SMALL_FILES_REPORT|FSIMAGE_REPORTS_UTILS' unravel_ondemand.out grep run_small_files\\(\\) unravel_ondemand.out The FSimage file is present on Unravel node at \/srv\/unravel\/tmp\/reports\/ fsimage\/fsimage.txt . The FSimage file is present in HDFS at \/tmp\/fsimage\/fsimage.txt . In case of problems it may be helpful to look at HiveServer2 and Yarn logs. " }, 
{ "title" : "2 - Define the following.", 
"url" : "102234-ondemand-small-and-file.html#UUID-fef52101-33f8-0a5a-dd48-297d492df971_N1561597302090", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ OnDemand reports \/ Configuring small files and files reports \/ 2 - Define the following.", 
"snippet" : "Edit \/usr\/local\/unravel\/etc\/unravel.properties and set the following properties. Property\/Description Default unravel.hive.server2.host FQDN or IP-Address of the HiveServer2 instance. - unravel.hive.server2.port Port for the HiveServer2 instance. You need only define this if the unravel.hive.server2...", 
"body" : "Edit \/usr\/local\/unravel\/etc\/unravel.properties and set the following properties. Property\/Description Default unravel.hive.server2.host FQDN or IP-Address of the HiveServer2 instance. - unravel.hive.server2.port Port for the HiveServer2 instance. You need only define this if the unravel.hive.server2.host port is not 1000. 10000 unravel.hive.server2.authentication Define the authentication type. Possible values are: KERBEROS , LDAP , NOSASL , NONE , or CUSTOM . When set to KERBEROS you must also set kerberos.service.name =hive. - unravel.hive.server2.kerberos.service.name Set only when unravel.hive.server2.authentication= KERBEROS . This must be set to hive to run the various reports in a kerberos enviornment. - " }, 
{ "title" : "Enable\/Disable small files and files report status", 
"url" : "102234-ondemand-small-and-file.html#UUID-fef52101-33f8-0a5a-dd48-297d492df971_N1561448824414", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ OnDemand reports \/ Configuring small files and files reports \/ Enable\/Disable small files and files report status", 
"snippet" : "In order for the configuration changes to take effect, unravel_ondemand and unravel_ngui daemons need to be restarted. \/etc\/init.d\/unravel_ngui restart \/etc\/init.d\/unravel_ondemand restart Set the following property in \/usr\/local\/unravel\/etc\/unravel.properties . Property\/Description Default unravel....", 
"body" : "In order for the configuration changes to take effect, unravel_ondemand and unravel_ngui daemons need to be restarted. \/etc\/init.d\/unravel_ngui restart\n\/etc\/init.d\/unravel_ondemand restart Set the following property in \/usr\/local\/unravel\/etc\/unravel.properties . Property\/Description Default unravel.python.reporting.files.disable Enables or disables Unravel ability to generate Small Files and File Reports. false : enables the Small Files and Files reports in both the backend and UI. true : disables the Small Files and Files reports. in both the backend and UI. false " }, 
{ "title" : "Triggering an import of FSimage", 
"url" : "102235-ondemand-triggering-import-of-fsimage.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ OnDemand reports \/ Triggering an import of FSimage", 
"snippet" : "The etl_fsimage task imports the latest FSimage from Namenode and incorporates it into a Hive table ( unravel_hdfs_fsimage_master_orc ) used for Unravel's Small Files\/File Reports feature, and it also generates four (4) precomputed file reports. We recommend scheduling etl_fsimage for a daily run (w...", 
"body" : "The etl_fsimage task imports the latest FSimage from Namenode and incorporates it into a Hive table ( unravel_hdfs_fsimage_master_orc ) used for Unravel's Small Files\/File Reports feature, and it also generates four (4) precomputed file reports. We recommend scheduling etl_fsimage for a daily run (which is the default configuration). The default configuration leads to triggering of etl_fsimage at 00:00 UTC every day. As of 4.5.3.0 you can configure the time and interval for downloading the FSimage, see FSimage properties . However, there may be times when you want to import FSimage immediately, such as after Unravel Server is installed or upgraded. In this case, you have to start etl_fsimage manually by running the following script on the Unravel node: curl -v http:\/\/localhost:5000\/small-files-etl This script ensures that the latest FSImage is incorporated in Unravel's Small Files\/File Reports. The etl_fsimage task's run time is proportional to the size of FSImage. In testing on a single node cluster, we observe the following run times. Please note that these times may not match your deployment; these times illustrate that etl_fsimage is proportional to the size of FSimage. FSImage Size etl_fsimage Run Time 19 GB 24 hours 9 GB 14 hours 4 GB 7 hours Troubleshooting If etl_fsimage fails with the warning OR if the Unravel user does not have dfsadmin privileges --i.e. you see the following error: [2018-09-10 23:11:57,357: WARNING\/ForkPoolWorker-1]* stderr: sudo: hdfs: command not found* In this case, do the following: Fetch the FSImage as a user with dfadmin privileges using the commands rm -rf unravel_node_fsimage_dir\/*\nhdfs dfsadmin -fetchImage unravel_node_fsimage_dir These commands delete all existing FSImage files and then copy the latest FSImage into the directory you specify ( unravel_node_fsimage_dir ). The directory unravel_node_fsimage_dir must be different then Unravel's default directory \/srv\/unravel\/tmp\/reports\/fsimage and it should be readable by unravel user. Best practice is to run these commands in a cron job that completes before Unravel's etl_fsimage task is triggered every day at 00:00 UTC. Configure Unravel OnDemand to access FSImage from unravel_node_fsimage_dir by setting the following properties in unravel.properties : unravel.python.reporting.files.skip_fetch_fsimage=true\nunravel.python.reporting.files.external_fsimage_dir=unravel_node_fsimage_dir Note: For this to work, the OnDemand user must have read privileges for the directory specified by unravel_node_fsimage_dir. Restart the Unravel OnDemand daemon. rm -rf unravel_node_fsimage_dir\/*\nhdfs dfsadmin -fetchImage unravel_node_fsimage_dir Unravel OnDemand assumes the FSImage filename starts with fsimage and does not end with extension .txt . " }, 
{ "title" : "Secure UI access", 
"url" : "102236-access-secured-ui.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Secure UI access", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Adding SSL and TLS to Unravel web UI", 
"url" : "102237-access-adding-ssl-tls-to-ui.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Secure UI access \/ Adding SSL and TLS to Unravel web UI", 
"snippet" : "You can configure an Apache2 web server (HTTPD) as a reverse proxy to provide HTTPS (SSL\/TLS) security to Unravel Web UI. Complete the following steps to make this work. Secure cookies are NOT supported when using this Apache2 reverse-proxy method. Follow the instructions in Enabling TLS to Unravel ...", 
"body" : "You can configure an Apache2 web server (HTTPD) as a reverse proxy to provide HTTPS (SSL\/TLS) security to Unravel Web UI. Complete the following steps to make this work. Secure cookies are NOT supported when using this Apache2 reverse-proxy method. Follow the instructions in Enabling TLS to Unravel Web UI Directly to enable TLS directly in unravel_ngui , which listens on port 3000. These steps were tested with HTTPD 2.4 and support listening on port 443. Install needed packages. sudo yum install httpd mod_ssl There is no need to change the default \/etc\/httpd\/conf\/httpd.conf file. Create \/etc\/httpd\/conf.d\/unravel_https.conf . Use the following as a model (replace unravelhost_FQDN and settings for <varname>SSLCertificate*<\/varname> with values appropriate for your installation). <VirtualHost *:80> \n ServerName unravelhost_FQDN \n Redirect permanent \/ https:\/\/ unravelhost_FQDN \n \n<\/VirtualHost> <VirtualHost *:443>\n\n DocumentRoot \/var\/www\/html\n ServerName unravelhost_FQDN\n # use this if http to https errors #RequestHeader set X-FORWARDED-PROTO 'https'\n\n SSLEngine on \n <varname>SSLCertificateFile<\/varname> \/etc\/certs\/wildcard_unravelhost_ssl_certificate.crt \n <varname>SSLCertificateKeyFile<\/varname> \/etc\/certs\/wildcard_unravelhost_RSA_private.key \n <varname>SSLCertificateChainFile<\/varname> \/etc\/certs\/IntermediateCA.crt\n\n # set this off for reverse proxy security \n ProxyRequests Off \n # might be helpful in logs \n ProxyPreserveHost On \n ProxyPass \/ http:\/\/localhost:3000\/ connectiontimeout=180 timeout=180 \n ProxyPassReverse \/ http:\/\/localhost:3000\/\n <Location \/> \n Order deny,allow \n Deny from all \n Allow from al\n \n <\/VirtualHost>\n Adjust or add property in \/usr\/local\/unravel\/etc\/unravel.properties . (No trailing slash. :port is optional). com.unraveldata.advertised.url=https:\/\/unravelhost_FQDN Restart the unravel_ngui daemon. sudo service unravel_ngui restart Start the HTTP daemon. sudo service httpd start Visit https:\/\/unravelhost_FQDN (using value appropriate for your site) to test access. Troubleshooting To enable verbose logging in Apache2, add LogLevel where LogLevel can be set to debug, trace1,..., trace8. LogLevel debug Do not leave debug settings enabled long term because they add overhead and can fill up the log area if logs are not auto-rolled. To force HTTPS protocol, even if a user requests http:\/\/ Add the following line after the ServerName line in the virtual host httpd RequestHeader set X-FORWARDED-PROTO 'https' Restart Apache2. " }, 
{ "title" : "Enabling LDAP authentication for Unravel UI", 
"url" : "102238-access-enabling-ldap-authentication.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Secure UI access \/ Enabling LDAP authentication for Unravel UI", 
"snippet" : "You can configure lightweight directory access protocol (LDAP) by Using UPN lookup and Group lookups using DN. sAMAccount name to match users. LDAP Processing Flow You must restart the ngui module ( \/etc\/init.d\/unravel_ngui restart ) after LDAP configuration. Advanced configuration where UPN cannot ...", 
"body" : "You can configure lightweight directory access protocol (LDAP) by Using UPN lookup and Group lookups using DN. sAMAccount name to match users. LDAP Processing Flow You must restart the ngui module ( \/etc\/init.d\/unravel_ngui restart ) after LDAP configuration. Advanced configuration where UPN cannot be used. This configuration uses sAMAccount name to match users. bindDN and password must be configured in \/usr\/local\/unravel\/etc\/unravel.properties . It uses CN to match groups instead of DN which was used in the example above. The configuration uses bind user to get groups, then match it using CN to filter out groups located in  com.unraveldata.ldap.groupFilter , as well as assigning admin users specified in  com.unraveldata.login.admins.ldap.groups . You must substitute your local values for the parameters and values used in the following example. Please contact your LDAP Admin if you do not know the following directory information. Check that the object can be found and the user is part of the expected groups. ldapsearch -v -h ariel.unraveldata.com -p 389 -D \nCN=sethbind,OU=seth,DC=unraveldata,DC=com -w unraveldata1! -b \nDC=unraveldata,DC=com -s sub \"(sAMAccountname=commauser)\" Set the following properties in \/usr\/local\/unravel\/etc\/unravel.properties . Add these properties if they are not found. #LDAP\ncom.unraveldata.login.mode=ldap\ncom.unraveldata.ldap.url=ldap:\/\/ariel.unraveldata.com\ncom.unraveldata.ldap.baseDN=DC=unraveldata,DC=com\ncom.unraveldata.ldap.use_jndi=true\ncom.unraveldata.ldap.verbose=true\ncom.unraveldata.ldap.bind_dn=CN=sethbind,OU=seth,DC=unraveldata,DC=com\ncom.unraveldata.ldap.bind_pw=unraveldata1!\ncom.unraveldata.ldap.guidKey=sAMAccountName \n\n#LDAP groups\ncom.unraveldata.ldap.groupFilter=seth-test-group,seth-test-admingroup\ncom.unraveldata.ldap.groupMembershipKey=member\ncom.unraveldata.ldap.groupQueryFilter=(CN=seth*)\ncom.unraveldata.login.admins.ldap.groups=seth-test-admingroup " }, 
{ "title" : "Simple configuration using UPN lookup and Group lookups using DN", 
"url" : "102238-access-enabling-ldap-authentication.html#UUID-735c3243-e053-b349-d4b4-9f7bd90f1972_section-5ccc7d0da4d3f-idm45390849429520", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Secure UI access \/ Enabling LDAP authentication for Unravel UI \/ Simple configuration using UPN lookup and Group lookups using DN", 
"snippet" : "This configuration example is for the newer implementation of Unravel with MS Active Directory and for objects located in separate OUs. In this method the user lookup is on login id appended with the configured domain defined in \/usr\/local\/unravel\/etc\/unravel.properties to make UPN (User Principal N...", 
"body" : "This configuration example is for the newer implementation of Unravel with MS Active Directory and for objects located in separate OUs. In this method the user lookup is on login id appended with the configured domain defined in \/usr\/local\/unravel\/etc\/unravel.properties to make UPN (User Principal Name) for the lookup. This configuration does not Work if objects in the directory do not have the expected UPN format. Include bindDn and password which older implementations used. You must substitute your local values for the parameters and values used in the following examples. Please contact your LDAP Admin if you do not know the following directory information. Check that the object can be found and the user is part of the expected groups. ldapsearch -v -h ariel.unraveldata.com -p 389 -D CN=sethbind,OU=seth,DC=unraveldata,DC=com -w unraveldata1! -b DC=unraveldata,DC=com -s sub \"(userPrincipalName=commauser@unraveldata.com)\"\n Set the following properties in \/usr\/local\/unravel\/etc\/unravel.properties . Add these properties if they are not found. #LDAP\ncom.unraveldata.login.mode=ldap\ncom.unraveldata.ldap.url=ldap:\/\/ariel.unraveldata.com\ncom.unraveldata.ldap.baseDN=DC=unraveldata,DC=com\ncom.unraveldata.ldap.use_jndi=true\ncom.unraveldata.ldap.verbose=true\n \n#LDAP groups\ncom.unraveldata.ldap.groupFilter=seth-test-group,seth-test-admingroup\ncom.unraveldata.login.admins.ldap.groups=seth-test-admingroup " }, 
{ "title" : "What is the difference between the two group properties in LDAP configurations?", 
"url" : "102238-access-enabling-ldap-authentication.html#UUID-735c3243-e053-b349-d4b4-9f7bd90f1972_section-5ccc83a26f662-idm45444640847600", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Secure UI access \/ Enabling LDAP authentication for Unravel UI \/ What is the difference between the two group properties in LDAP configurations?", 
"snippet" : "com.unraveldata.ldap.groupFilter : Lists the groups Unravel looks in for users who are allowed to log in. com.unraveldata.login.admins.ldap.groups : Lists the groups Unravel looks in for users who are allowed log in as admins. com.unraveldata.login.admins.ldap.groups is a subset of com.unraveldata.l...", 
"body" : "com.unraveldata.ldap.groupFilter : Lists the groups Unravel looks in for users who are allowed to log in. com.unraveldata.login.admins.ldap.groups : Lists the groups Unravel looks in for users who are allowed log in as admins. com.unraveldata.login.admins.ldap.groups is a subset of com.unraveldata.ldap.groupFilter , i.e., a group defined in com.unraveldata.login.admins.ldap.groups must also be defined in com.unraveldata.ldap.groupFilter . For example, com.unraveldata.ldap.groupFilter=secs-lab-admins,secs-lab-users\n# the admins.ldap group is also defined in ldap.groupFilter\ncom.unraveldata.login.admins.ldap.groups=secs-lab-admins If a user is: Not listed in the groups defined in com.unraveldata.ldap.groupFilter , they can not log in. Listed in group defined in com.unraveldata.login.admins.ldap.groups , they are logged in as an admin. Only listed in the groups defined in com.unraveldata.ldap.groupFilter , they are logged in as a non-admin user. " }, 
{ "title" : "Enabling SAML authentication for Unravel Web UI", 
"url" : "102239-access-enabling-saml-authentication.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Secure UI access \/ Enabling SAML authentication for Unravel Web UI", 
"snippet" : "To use SAML you must configure both your Unravel Host and SAML server. Configure Unravel host Add\/modify these properties in \/usr\/local\/unravel\/etc\/unravel.properties . com.unraveldata.login.mode=saml com.unraveldata.login.saml.config=\/usr\/local\/unravel\/etc\/saml.json To use SAML with RBAC see Config...", 
"body" : "To use SAML you must configure both your Unravel Host and SAML server. Configure Unravel host Add\/modify these properties in \/usr\/local\/unravel\/etc\/unravel.properties . com.unraveldata.login.mode=saml \ncom.unraveldata.login.saml.config=\/usr\/local\/unravel\/etc\/saml.json To use SAML with RBAC see Configure LDAP or SAML RBAC Properties . Edit saml.config.json file Property Description Req Example Values entryPoint Identity provider entrypoint, Ping IdP address (SSO URL). Note: Identity provider entrypoint is required to be spec-compliant when the request is signed. Yes \"http:\/\/myHost:9080\/simplesaml\/saml2\/idp\/SSOService.php\" issuer Name of app that will connect to the saml server. Issuer string to supply to identity provider (Environment name). Should match the name configured in Idp. Yes \"unravel-myHost” cert IDP's public cert to validate auth response signature. Note: You retrieve this from saml host. Yes Idp Cert String logoutUrl Base address to call with logout requests. Default: entryPoint No \"http:\/\/myHost:9080\/simplesaml\/saml2\/idp\/SingleLogoutService.php\" logoutEnabled If true logs you out from every app. No false unravel_mapping Mapping saml auth response attributes to Unravel attributes. Yes { \"username\":\"userid\", \"groups\":\"ds_groups\" } privateCert Unravel private cert string to sign Auth requests. No Unravel cert string Example saml.json {\n \"entryPoint\":\"http:\/\/myHost.unraveldata.com:9080\/simplesaml\/saml2\/idp\/SSOService.php\",\n\n \"issuer\":\"localhost\",\n\n \"logoutUrl\":\"http:\/\/myHost.unraveldata.com:9080\/simplesaml\/saml2\/idp\/SingleLogoutService.php\",\n\n \/\/ generated by saml host\n\"cert\":\"MIIDXTCCAkWgAwIBAgIJALmVVuDWu4NYMA0GCSqGSIb3DQEBCwUAMEUxCzAJBgNVBAYTAkFVMRMwEQYDVQQIDApTb21lLVN0YXRlMSEwHwYDVQQKDBhJbnRlcm5ldCBXaWRnaXRzIFB0eSBMdAcQf2CGAaVfwTTfSlzNLsF2lW\/ly7yapFzlYSJLGoVE+OHEu8g5SlNACUEfkXw+5Eghh+KzlIN7R6Q7r2ixWNFBC\/jWf7NKUfJyX8qIG5md1YUeT6GBW9Bm2\/1\/RiO24JTaYlfLdKK9TYb8sG5B+OLab2DImG99CJ25RkAcSobWNF5zD0O6lgOo3cEdB\/ksCq3hmtlC\/DlLZ\/D8CJ+7VuZnS1rR2naQ==\",\n\"privateCert\":\"-----BEGIN PRIVATE \/\/ generated by unravel node\n KEY-----\\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDEt4Ma2k4DUkoW\\nG9QDHUBnY7S\/iS\/+u2BjPZqUG2JktzYZl30J05zA6i642i2VDn8eUIPHqt2Hw249\\nZ3nHKL4YnBVqa3yTfEkdMB\/6GSAkoCbnufaD3IsGcFJnlW5raDiT\/GZMy+1WnDfJ\\npB0\/.......vD8kRkcmEi9t3KLmKVy3SO15\/YHAhLxP9oTnTFGkPnIqZLRM0Y55UfwbRSZDlgH\/\\ny9GGmsV5IaIwhepuALJMdkHp\\n-----END PRIVATE KEY-----\\n\",\n\n \"unravel_mapping\":\n {\n \"username\":\"userid\",\n \"groups\":\"ds_groups\"\n }\n} For Ping, the IdP certificate can be obtained as follows: In the Server Configuration section, select Certificate Management and Digital Signing & XML Decryption Keys & Certificates. Click Export for the IdP certificate that you require. Select Certificate Only and click Next . Click Export , and save the file. Configure SAML server Configure the following properties on the SAML server. Replace UNRAVEL_HOST with the fully qualified path or IP address of your Unravel host. Property Description Req PingFederate Specific configuration AssertionConsumerService \/ ACS Url http(s):\/\/ UNRAVEL_HOST :3000\/saml\/consume Yes Edit a SAML Application Setting Assertion Consumer Service URLs Entity Identifier unravel-Congo24 Yes Should be same as the issuer in saml.json . Single Logout Endpoint http:\/\/ UNRAVEL_HOST :3000\/ Specifying Single Logout Service URL Single Logout Response Endpoint http:\/\/ UNRAVEL_HOST :3000\/ No " }, 
{ "title" : "Enabling TLS to Unravel Web UI directly", 
"url" : "102240-access-enable-tls-reach-un-ui-directly.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Secure UI access \/ Enabling TLS to Unravel Web UI directly", 
"snippet" : "The following steps show you how to directly enable TLS (SSL) to unravel_ngui which is listening on port 3000. Alternatively, see Adding SSL and TLS to Unravel Web UI to add an Apache2 reverse proxy which supports listening on port 443, the usual HTTPS port. In this example, we stay on default port ...", 
"body" : "The following steps show you how to directly enable TLS (SSL) to unravel_ngui which is listening on port 3000. Alternatively, see Adding SSL and TLS to Unravel Web UI to add an Apache2 reverse proxy which supports listening on port 443, the usual HTTPS port. In this example, we stay on default port 3000 but change the protocol to HTTPS. We need SSL\/TLS certificate files accessible from the Unravel host. For more information, see Defining a Custom Web UI Port . On Unravel Server, edit \/usr\/local\/unravel\/etc\/unravel.properties as follows: OPTION 1 - Simple SSL config Update or add the following properties. For example, to enable SSL with minimal configuration: #ENABLE\/DISABLE SSL \ncom.unraveldata.ngui.ssl.enabled=true \n\n#PATH TO CERT FILE \ncom.unraveldata.ngui.ssl.cert.file=\/etc\/certs\/wildcard_unravelhost_ssl_certificate \n\n#PATH TO KEY FILE \ncom.unraveldata.ngui.ssl.key.file=\/etc\/certs \/wildcard_unravelhost_RSA_private.key \n\n#OPTIONAL - COMMA SEPARATED LIST OF CA FILES \ncom.unraveldata.ngui.ssl.ca.files=\/etc\/certs\/IntermediateCA1.crt,\/etc\/certs\/IntermediateCA2.crt \n\n#OPTIONAL- PASSPHRASE IF NEEDED FOR KEY FILE \ncom.unraveldata.ngui.ssl.passphrase=testp OPTION 2 - Advanced ssl config Update or add the following properties. For example, to enable SSL with advance configuration, update\/add these properties: #ENABLE\/DISABLE SSL \ncom.unraveldata.ngui.ssl.enabled=true \n\n#PROVIDE SSL CONFIG THROUGH JS FILE FOR ADVANCE CONFIG\ncom.unraveldata.ngui.ssl.advance.config=\/usr\/local\/unrave\/etc\/advanced_unravel_ssl.js Content of advanced_unravel_ssl.js : \/* advanced_unravel_ssl.js \n update below config variables \n SSL_KEY_FILE_PATH \n CA_CERT_FILE_PATH \n comment and uncomment the needed blocks \n *\/ \nconst fs = require('fs');\nconst constants = require('constants');\n\n\/* absolute path for ssl key file *\/\nconst SSL_KEY_FILE_PATH= '\/cert\/unravel_ssl.key'\n\n\/* absolute path for ssl cert file *\/\nconst SSL_CERT_FILE_PATH= '\/certunravel_ssl.crt'\n\n\/* absolute path for CA certs *\/\n\/* const CA_CERT_FILE_PATH=''*\/\nmodule.exports = {\nkey: fs.readFileSync(SSL_KEY_FILE_PATH),\npassphrase:'The password you gave when you created the key',\ncert: fs.readFileSync(SSL_CERT_FILE_PATH),\n\/\/ un comment below if using custom ca certs\n\/\/ ca : fs.readFileSync(CA_CERT_FILE_PATH),\n\/\/ uncomment below to enable disable TLS version.\n\/\/ secureOptions: constants.SSL_OP_NO_TLSv1 | constants.SSL_OP_NO_TLSv1_1,\n\n\/* LIST OF RECOMMENDED CIPHERS *\/\n\/* note OpenSSL-style format *\/\nciphers: ['TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384',\n'ECDHE_ECDSA_WITH_AES_128_GCM_SHA256',\n'ECDHE_ECDSA_WITH_AES_256_CBC_SHA384',\n'ECDHE_ECDSA_WITH_AES_256_CBC_SHA',\n'ECDHE_ECDSA_WITH_AES_128_CBC_SHA256',\n'ECDHE_ECDSA_WITH_AES_128_CBC_SHA',\n'ECDHE_RSA_WITH_AES_256_GCM_SHA384',\n'ECDHE_RSA_WITH_AES_128_GCM_SHA256',\n'ECDHE_RSA_WITH_AES_256_CBC_SHA384',\n'ECDHE_RSA_WITH_AES_128_CBC_SHA256',\n'ECDHE_RSA_WITH_AES_128_CBC_SHA'].join(':')\n} Set your advertised host in \/usr\/local\/unravel\/etc\/unravel.properties . This prefix will be used by Unravel server right after login or logout. com.unraveldata.advertised.url=https:\/\/unravel.example.com:3000 Restart Unravel Web UI. sudo service unravel_ngui restart " }, 
{ "title" : "Spark", 
"url" : "102241-spark.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Spark", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Configure Spark properties for Spark Worker daemon @ Unravel", 
"url" : "102242-spark-worker-daemon-properties.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Spark \/ Configure Spark properties for Spark Worker daemon @ Unravel", 
"snippet" : "Live pipeline Property\/Description Set by user Unit Default com.unraveldata.spark.live.pipeline.enabled Specifies if Unravel should process the live job data coming from sensor or not. true: The live job data will be processed as soon as it is received. false: Live job data will not be processed. bo...", 
"body" : "Live pipeline Property\/Description Set by user Unit Default com.unraveldata.spark.live.pipeline.enabled Specifies if Unravel should process the live job data coming from sensor or not. true: The live job data will be processed as soon as it is received. false: Live job data will not be processed. boolean true com.unraveldata.spark.live.pipeline.maxStoredStages Maximum number of jobs\/stages stored in the DB. If an application has (# jobs\/stages) > maxStoredStages only the last maxStoredStages are stored. This setting affects only the live pipeline . When processing the event log file (after the application has completed its execution) this property is not considered. count 1000 com.unraveldata.spark.master Default spark master mode to be used if not available from Sensor. Possible values: local, standalone or yarn (default) set member yarn Event log processing Property\/Description Set by user Unit Default com.unraveldata.spark.eventlog.location All the possible locations of the event log files. Multiple locations are supported as a comma separated list of values. This property is used only when the Unravel sensor is not enabled. When the sensor is enabled, the event log path is taken from the application configuration at runtime. string hdfs:\/\/\/user\/spark\/applicationHistory\/ com.unraveldata.spark.eventlog.maxSize Maximum size of the event log file that will be processed by the Spark worker daemon. Event logs larger than MaxSize will not be processed. bytes 1000000000 (~1GB) com.unraveldata.spark.eventlog.appDuration.mins Maximum duration (in minutes) of application to pull Spark event log. min 1440 (1 day) com.unraveldata.spark.hadoopFsMulti.useFilteredFiles Specifies how to search the event log files. true : prefix search false : prefix + suffix search Prefix + suffix search is faster as it avoids listFiles() API which may take a long time for large directories on HDFS. This search requires that all the possible suffixes for the event log files are known. Possible suffixes are specified by com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes. . boolean false com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes Specifies suffixes used for prefix+suffix search of the event logs when com.unraveldata.spark.hadoopFsMulti.useFilteredFiles = false . NOTE : the empty suffix (,,) be part of this value for uncompressed event log files. CSL ,,.lz4,.snappy,.inprogres com.unraveldata.spark.appLoading.maxAttempts Maximum number of attempts for loading the event log file from HDFS\/S3\/ ADL\/WASB etc. count 3 com.unraveldata.spark.appLoading.delayForRetry Delay used among consecutive retries when loading the event log files. The actual delay is not constant, it increases progressively by 2^attempt * delayForRetry . ms 2000 (2 s) com.unraveldata.spark.tasks.inMemoryLimit Number of tasks to be kept in memory and DB per stage. All stats are calculated for all the task attempts but only the configured number of tasks will be kept in memory\/DB. count 1000 Events Related com.unraveldata.spark.events.enableCaching Enables logic for executing caching events. boolean false Other properties Property\/Description Set by user Unit Default com.unraveldata.spark.appLoading.maxConcurrentApps This is the number of applications Unravel keep metadata in Spark worker daemon memory. count 5 ccom.unraveldata.spark.time.histogram Specifies whether the timeline histogram is generated or not. Note: Timeline histogram generation is memory intensive. boolean false Properties defined in spark-default.conf Property\/Description Set by user Unit Default com.unraveldata.spark.shutdown.delay.ms Amount of time to delay shutdown so the last messages are processed (allows Btrace sensor to send all the data before the spark driver exits). ms 0 com.unraveldata.spark.live.interval.sec This is the interval in seconds after which live application data is updated. It allows for tracking of Spark tasks. The Spark APM updates on Task completion in addition Job start, and Job and Stage completion. s 60 Property\/Description Set by user Unit Default com.unraveldata.job.collector.running.load.conf When set to true Running MR jobs are linked to corresponding Hive app if Hive-on-MR app. Auto Action metrics for running hive queries will be sent to AA2 backend. boolean false com.unraveldata.job.collector.hive.queries.cache.size This is used to improve the Hive-MR pipeline by caching data so it can be retrieved from cache instead of external API. You should not have to change this value. count 1000 com.unraveldata.max.attempt.log.dir.size.in.bytes Maximum size of the aggregated executor log that are imported and processed by the Spark worker for a successful application. byte 500000000 (~500 MB) com.unraveldata.max.failed.attempt.log.dir.size.in.bytes Maximum size of the aggregated executor log that are imported and processed by the Spark worker for a failed application. byte 2000000000 (~2 GB) com.unraveldata.min.job.duration.for.attempt.log Minimum duration of a successful application or which executor logs are processed (in milliseconds). ms 600000 (10 mins) com.unraveldata.min.failed.job.duration.for.attempt.log Minimum duration of failed\/killed application for which executor logs are processed (in milliseconds). ms 60000 com.unraveldata.attempt.log.max.containers Maximum number of containers for the application. If application has more that configured number of containers then the aggregated executor log isprocessed for the application. ms 500 com.unraveldata.spark.master Default master for spark applications. (Used to download executor log using correct APIs.) Valid Options: yarn , mesos , standalone . string yarn Property\/Description Set by user Unit Default com.unraveldata.s3.profile.config.file.path The path to the s3 profile file, e.g., \/usr\/local\/unravel\/etc\/s3ro.properties . string - com.unraveldata.spark.s3.profilesToBuckets Comma separated list of profile to bucket mappings in the following format: <s3_profile>:<s3_bucket>, for example, com.unraveldata.spark.s3.profileToBuckets =profile-prod:com.unraveldata.dev,profile-dev:com.unraveldata.dev. Note Ensure that the profiles defined in the property above are actually present in the s3 properties file and that each profile has associated a corresponding pair of credentials aws_access_key and aws_secret_access_key . The old format: access_key\/secretKey is no longer supported.) CSL - Property\/Description Set by user Unit Default com.unraveldata.tagging.enabled Enables tagging functionality. boolean true com.unraveldata.tagging.script.enabled Enables tagging. boolean false com.unraveldata.app.tagging.script.path Specifies tagging script path to use when enabled =true. string (path) \/usr\/local\/unravel\/etc\/apptag.py com.unraveldata.app.tagging.script.method.name The name of the method in the python script that generates the tagging dictionary. string generate_unravel_tags Property\/Description Set by user Unit Default com.unraveldata.hdinsight.storage-account. X Storage account name that a HDInsight cluster uses. You must define this property for each storage account. X starts with 1 and then is incremented by 1 for each additional account. The account numbers must be consecutive. Optional string Azure storage account name. (See finding the storage name .) com.unraveldata.hdinsight.access-key. X Storage account key. For each storage-account. X you must define access-key. X If you have two access keys, pick one to use here. Optional string Azure storage account key. (See finding the access key .) Property\/Description Set by user Unit Default com.unraveldata.adl.accountFQDN The data lake's fully qualified domain name, for example, mydatalake.azuredatalakestore.net. Optional string Azure storage account name. (See finding the storage name .) com.unraveldata.adl.clientId An application ID. An application registration has to be created in the Azure Active Directory. Optional string Azure application id. (See finding the application Id .) com.unraveldata.adl.clientKey An application access key which can be created after registering an application. Optional string Azure storage access key. (See finding the storage access key .) com.unraveldata.adl.accessTokenEndpoint The OAUTH 2.0 Access Token Endpoint. It is obtained from the application registration tab on Azure portal. Optional string Azure OAUTH 2.0 token endpoint (See finding the OAUTH endpoint .) com.unraveldata.adl.clientRootPath The path in the Data lake store where the target cluster has been given access. Optional string URL Azure CONTAINER\/DIRECTORY path for storage account name. (See finding the container path .) " }, 
{ "title" : "Configuring notebooks for Spark", 
"url" : "102243-spark-configure-notebooks.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Spark \/ Configuring notebooks for Spark", 
"snippet" : "Notebook programs like Zepplin and Jupyte,r overwrite the spark.driver.extraJavaOptions which prevents the data from apps run being captured by Unravel. To ensure the data is loaded into Unravel you must set the following properties in your notebook program. SPARK_HOME : The location of your Spark l...", 
"body" : "Notebook programs like Zepplin and Jupyte,r overwrite the spark.driver.extraJavaOptions which prevents the data from apps run being captured by Unravel. To ensure the data is loaded into Unravel you must set the following properties in your notebook program. SPARK_HOME : The location of your Spark library. spark.driver.extraJavaOptions : Spark driver options. spark.executor.extraJavaOptions : Spark executor options. For example, SPARK_HOME : \/opt\/cloudera\/parcels\/CDH-6.3.0-1.cdh6.3.0.p0.1279813\/lib\/spark spark.driver.extraJavaOptions : -Dfile.encoding=UTF-8 -Dlog4j.configuration=file:\/\/\/root\/zeppelin-0.8.2-bin-all\/conf\/log4j.properties -Dzeppelin.log.file=\/root\/zeppelin-0.8.2-bin-all\/logs\/zeppelin-interpreter-spark-root-tnode113.unraveldata.com.log -javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=spark-$SPARK_VERSION,config=driver spark.executor.extraJavaOptions : -javaagent:-javaagent:unravel-agent-pack-bin.zip\/btrace-agent.jar=libs=spark-$SPARK_VERSION,config=executor " }, 
{ "title" : "Enable\/disable live monitoring of Spark Streaming applications", 
"url" : "102244-spark-enable-live-streaming.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Spark \/ Enable\/disable live monitoring of Spark Streaming applications", 
"snippet" : "This topic explains how to enable and disable live monitoring of Spark Streaming applications. Unravel uses a StreamingProbe to perform the live monitoring. By default, the probe is not enabled because monitoring can generate a lot of data and potentially overload Unravel's ES. The probe is controll...", 
"body" : "This topic explains how to enable and disable live monitoring of Spark Streaming applications. Unravel uses a StreamingProbe to perform the live monitoring. By default, the probe is not enabled because monitoring can generate a lot of data and potentially overload Unravel's ES. The probe is controlled by spark.driver.extraJavaOptions which is defined in spark-defaults.conf . To enable the probe edit spark-defaults.conf and set the property as follows: spark.driver.extraJavaOptions=-javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=spark-2.3,script=StreamingProbe.btclass,config=driver Set spark.driver.extraJavaOptions as follows. You can comment out the above definition and just add another definition to ease toggling the live monitoring. \/\/ comment out\n#spark.driver.extraJavaOptions=-javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=spark-2.3,script=StreamingProbe.btclass,config=driver\n\/\/ and add \nspark.driver.extraJavaOptions=-javaagent:\/usr\/local\/unravel-agent\/jars\/btrace-agent.jar=libs=spark-2.3,config=driver\n " }, 
{ "title" : "User interface", 
"url" : "102245-ui.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ User interface", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Defining a custom banner", 
"url" : "102246-ui-defining-custom-banner.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ User interface \/ Defining a custom banner", 
"snippet" : "You can define a custom banner which appears below Unravel's title bar; it remains open until the user closes it. The banner is displayed until com.unraveldata.custom.banner.end.date . In this example, the banner is used to notify users of upcoming maintenance. The following properties define the ba...", 
"body" : "You can define a custom banner which appears below Unravel's title bar; it remains open until the user closes it. The banner is displayed until com.unraveldata.custom.banner.end.date . In this example, the banner is used to notify users of upcoming maintenance. The following properties define the banner. Property\/Description Set by user Unit Default com.unraveldata.custom.banner.display Displays a banner at the top of the Unravel UI. true : banner displays text until end.date . false : no change to UI. boolean false com.unraveldata.custom.banner.text Text to display when display = true . The text and end.date must both be defined for the banner to be displayed. The banner displays the text until end.date . Optional string - com.unraveldata.custom.banner.end.date Date and Time to stop displaying the banner. There is no date\/time limit. The text and end.date must both be defined for the banner to be displayed. Format: YYYYMMDDTHHMMSSZ-000000 Optional string (date) - " }, 
{ "title" : "Defining a custom web UI port", 
"url" : "102247-ui-defining-custom-web-ui-port.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ User interface \/ Defining a custom web UI port", 
"snippet" : "These instructions apply to any platform. Port numbers under 1024 are restricted to root or setuid programs. An alternative technique to use low port numbers is to enable iptables and essentially do port mapping of 443 to a higher port. Edit \/usr\/local\/unravel\/etc\/unravel.ext.sh on the Unravel serve...", 
"body" : "These instructions apply to any platform. Port numbers under 1024 are restricted to root or setuid programs. An alternative technique to use low port numbers is to enable iptables and essentially do port mapping of 443 to a higher port. Edit \/usr\/local\/unravel\/etc\/unravel.ext.sh on the Unravel server (all hosts, for multi-host installs) and add this environment variable. Replace 18080 with your chosen port number. export NGUI_PORT= 18080 After making this change, restart the affected daemon. # sudo \/etc\/init.d\/unravel_ngui restart " }, 
{ "title" : "Disabling browser telemetry", 
"url" : "102248-ui-disabling-browser-telemetry.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ User interface \/ Disabling browser telemetry", 
"snippet" : "By default, Unravel Web UI collects data about your use of Unravel. You can disable this functionality by the following steps. If you have a multi-host Unravel install, you must use logical host1. Disable Mixpanel On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties . sudo vi \/usr\/local...", 
"body" : "By default, Unravel Web UI collects data about your use of Unravel. You can disable this functionality by the following steps. If you have a multi-host Unravel install, you must use logical host1. Disable Mixpanel On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties . sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Set com.unraveldata.do.not.track to true . If this property isn't in the file, add it and set it to false . com.unraveldata.do.not.track=true Restart the Unravel UI. sudo service unravel_ngui restart Re-enable Mixpanel Follow the above steps but in step 2 set com.unraveldata.do.not.track to false . " }, 
{ "title" : "Disabling support\/comments panel", 
"url" : "102249-ui-disable-support-comments-panel.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ User interface \/ Disabling support\/comments panel", 
"snippet" : "This button\/feature is no longer available as of 4.5.1. Unravel's title bar has a support button allowing users to contact Unravel Data directly via a pop-up. See an example of the pop-up below. You have the option to disable this function and hide the support button. Hide\/Disable support button On ...", 
"body" : "This button\/feature is no longer available as of 4.5.1. Unravel's title bar has a support button allowing users to contact Unravel Data directly via a pop-up. See an example of the pop-up below. You have the option to disable this function and hide the support button. Hide\/Disable support button On Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties with vi . # sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Search for com.unraveldata.ngui.support.enabled . If not found, add the property. Set it to false. com.unraveldata.ngui.support.enabled=false Restart the Unravel UI. sudo service unravel_ngui restart Your title bar should be missing the support button like below. Show\/re-enable support button To enable the support\/comments panel, repeat the above steps 1-3, but in step 2 set com.unraveldata.ngui.support.enabled to true or remove the property from unravel.properties file. Pop-up support box " }, 
{ "title" : "Restricting direct access to Unravel UI", 
"url" : "102250-ui-restrict-direct-access-ui.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ User interface \/ Restricting direct access to Unravel UI", 
"snippet" : "On Unravel server, edit \/usr\/local\/unravel\/etc\/unravel.ext.sh . sudo vi \/usr\/local\/unravel\/etc\/unravel.ext.sh Add or modify NGUI_HOSTNAME . SET NGUI_HOSTNAME export NGUI_HOSTNAME=127.0.0.1 Restart the Unravel UI. sudo service unravel_ngui restart...", 
"body" : "On Unravel server, edit \/usr\/local\/unravel\/etc\/unravel.ext.sh . sudo vi \/usr\/local\/unravel\/etc\/unravel.ext.sh Add or modify NGUI_HOSTNAME . SET NGUI_HOSTNAME \nexport NGUI_HOSTNAME=127.0.0.1 Restart the Unravel UI. sudo service unravel_ngui restart " }, 
{ "title" : "Specifying a cluster ID or name", 
"url" : "102251-ui-specifying-clusterid-name.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ User interface \/ Specifying a cluster ID or name", 
"snippet" : "Throughout the Unravel UI there are pages and tabs where the cluster information is used or needed. For example Various reports, e.g., Chargeback reports. Applications tab where Unravel displays the app's cluster. When a cluster manager has only a single cluster, Unravel discovers the cluster ID\/nam...", 
"body" : "Throughout the Unravel UI there are pages and tabs where the cluster information is used or needed. For example Various reports, e.g., Chargeback reports. Applications tab where Unravel displays the app's cluster. When a cluster manager has only a single cluster, Unravel discovers the cluster ID\/name automatically. However, when your manager has multiple clusters, you must tell Unravel the clusters' names; allowing you, when necessary, to choose the cluster you want to use. Use the following port numbers: HTTP: Cloudera Manager = 7180 Ambari = 8080 MapR Control System = 8440 HTTPS: Cloudera Manager = 7183 Ambari = 8443 MapR Control System = 8443 Don't specify cluster ID in the javaagent part of sensor properties, i.e., spark.driver.extraJavaOptions and spark.executor.extraJavaOptions Set  com.unraveldata.cluster.name in unravel.properties as described below. Find the cluster's internal \"name\". Example using the Cloudera Manager API using the name field, with fallback to the displayName field. Depending on the version of Cloudera, the endpoint may use an API version that is between 16 and 19. In Unravel version 4.5.0.5, the HDFS Capacity Forecasting and Cloud Reports first attempt to get a match on the name attribute, and if no match is found, then search based on the displayName attribute. http:\/\/$server:7180\/api\/v17\/clusters \n{ \n \"name\": \"cluster\", \n \"displayName\":\"QACDH514E\", \n ... \n} Example with Ambari API using the cluster_name field: http:\/\/$server:8080\/api\/v1\/clusters\n{\n...\n \"Clusters\": {\n \"cluster_name\": \"DEVHDP26E\",\n }\n... Example with MapR API using the name field https:\/\/$server:8440\/rest\/dashboard\/info\n \"cluster\": {\n \"name\": \"QAMAPR60E\",\n ...\n }, In unravel.properties , set com.unraveldata.cluster.name to the exact cluster name you retrieved from the cluster manager. For example: com.unraveldata.cluster.name=QAMAPR60E In unravel.properties , set com.unraveldata.cluster.type to CDH, HDP, or MAPR. If you don't set this, Unravel attempts to determine this automatically if there are artifacts or config files on the Unravel server that may indicate if it contains Ambari (or HDP), Cloudera Manager (or CDH), or MAPR. " }, 
{ "title" : "Unravel admins", 
"url" : "102252-unravel-admins-login.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Unravel admins", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Adding more admins to Unravel", 
"url" : "102253-unravel-admins-adding.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Unravel admins \/ Adding more admins to Unravel", 
"snippet" : "On the Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties with vi . sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Append comma-separated usernames to the value of com.unraveldata.login.admins . For example, com.unraveldata.login.admins=admin,admin1,admin2 Restart unravel_ngui service...", 
"body" : "On the Unravel Server, open \/usr\/local\/unravel\/etc\/unravel.properties with vi . sudo vi \/usr\/local\/unravel\/etc\/unravel.properties Append comma-separated usernames to the value of com.unraveldata.login.admins . For example, com.unraveldata.login.admins=admin,admin1,admin2 Restart unravel_ngui service. sudo service unravel_ngui restart " }, 
{ "title" : "Adding read-only admins to Unravel", 
"url" : "102254-unravel-admins-adding-read-only.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Unravel admins \/ Adding read-only admins to Unravel", 
"snippet" : "A read-only admin has access to all the UI's page including the Manage page but is not able to write, e.g., create Auto Actions. Open \/usr\/local\/unravel\/etc\/unravel.properties . Search for com.unraveldata.login.admins.readonly . If you can not find the property add it. Using a comma separated list, ...", 
"body" : "A read-only admin has access to all the UI's page including the Manage page but is not able to write, e.g., create Auto Actions. Open \/usr\/local\/unravel\/etc\/unravel.properties . Search for com.unraveldata.login.admins.readonly . If you can not find the property add it. Using a comma separated list, add\/append the users you wish to grant read-only admin status. com.unraveldata.login.admins.readonly= user1 , user2 , user3 If you are using LDAP or SAML, you must configure the read-only admins using com.unraveldata.login.admins.readonly. MODE . groups (where MODE is the value of com.unraveldata.login.mode ). Using a comma separated list, add\/append the users you wish to grant read-only admin status. For more information, see Configure LDAP or SAML RBAC properties . com.unraveldata.login.admins.readonly. MODE .groups= admin1 , admin2 , admin3 , user2 , user3 " }, 
{ "title" : "Changing Unravel admin's password", 
"url" : "102255-unravel-admins-changing-password.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Unravel admins \/ Changing Unravel admin's password", 
"snippet" : "Retrieve the following configurations from the \/usr\/local\/unravel\/etc\/unravel.properties file. unravel.jdbc.url=jdbc:$ENGINE:\/\/$HOST:$PORT\/$DATABASE unravel.jdbc.username=$USERNAME unravel.jdbc.password=$PASSWORD Connect to your database (either MySQL) CLI using one of the following methods. We use ...", 
"body" : "Retrieve the following configurations from the \/usr\/local\/unravel\/etc\/unravel.properties file. unravel.jdbc.url=jdbc:$ENGINE:\/\/$HOST:$PORT\/$DATABASE\nunravel.jdbc.username=$USERNAME \nunravel.jdbc.password=$PASSWORD\n Connect to your database (either MySQL) CLI using one of the following methods. We use MySQL in this example. Run the db_access.sh script \/usr\/local\/unravel\/install_bin\/db_access.sh\n Open your database's (MySQL CLI) command line. mysql -u $USER -h $HOST $DATABASE -p  Query for the user whose password you want to change and keep track of its \"id\" field. mysql> use $DATABASE; \nmysql> select id, login, encrypted_password from users;\n+----+-------+-------------------------------------------------------+ \n| id | login | encrypted_password | \n+----+-------+-------------------------------------------------------+ \n| 1 | admin | ######################################################| Use the bcrypt library to generate a new encrypted password. Navigate to Bcrypt-Generator.com . Enter your desired password and select at least 10 rounds of hashing. Save the hash. Update the record in the database with the new hash. Replace your local value for HASH : hash value you generated, USER with the user whose password you are changing, and ID : the ID from the query in Step 3. mysql> UPDATE users SET encrypted_password = ' HASH ' WHERE id = ID AND login = '{LOGIN}' LIMIT 1; Navigate to the Unravel UI. Logout and then login with your new password. " }, 
{ "title" : "Workflows", 
"url" : "102256-workflow.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Workflows", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Monitoring Airflow workflows", 
"url" : "102257-workflow-airflow.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Workflows \/ Monitoring Airflow workflows", 
"snippet" : "This topic describes how to set up Unravel Server to monitor Airflow workflows so you can see them in Unravel Web UI. Due to an Airflow bug in v1.10.0, Unravel only supports v1.10.1+, not v1.10.0. Before you start, ensure the Unravel Server host and the server that runs Airflow web service are in th...", 
"body" : "This topic describes how to set up Unravel Server to monitor Airflow workflows so you can see them in Unravel Web UI. Due to an Airflow bug in v1.10.0, Unravel only supports v1.10.1+, not v1.10.0. Before you start, ensure the Unravel Server host and the server that runs Airflow web service are in the same cluster. All the following steps are on the Unravel Server host that runs the unravel_jcs2 daemon. Connecting to your Airflow Web UI In \/usr\/local\/unravel\/etc\/unravel.properties , update or add these properties: HTTP for Airflow Web UI access If your Airflow Web UI uses HTTP, set these properties: com.unraveldata.airflow.protocol=http\ncom.unraveldata.airflow.server.url= airflow-ui-url \ncom.unraveldata.airflow.available=true\n HTTPS for Airflow Web UI access If your Airflow Web UI uses HTTPS, set these properties: com.unraveldata.airflow.server.url= airflow-ui-url \ncom.unraveldata.airflow.available=true\ncom.unraveldata.airflow.login.name= airflow-ui-userame \ncom.unraveldata.airflow.login.password= airflow-ui-password \n Property\/Description Set by user Unit Default com.unraveldata.airflow.available Notes if the airflow is currently available. false : not available true : available boolean false airflow.look.back.num.days The number of days to look back. The look-back days can be specified as a positive or negative number. For instance -5 or 5 sets the look back number of days to 5. count 1 airflow.look.back.num.hours The look-back time window in hours, which can be either a positive or negative integer. If present, it takes precedence over airflow.look.back.num.days to have finer granularity. Suggested value for large clusters: 2 count 24 com.unraveldata.airflow.http.max.body.size.byte Set maximum number of bytes Unravel fetches data from Airflow Web UI. Default unlimited. bytes 0 com.unraveldata.airflow.login.name Airflow UI login username. You must set this if airflow.server.url = https. Required string - com.unraveldata.airflow.login.password Password for Airflow UI com.unraveldata.airflow.login.name . You must set set this if airflow.login.name is set. Required string - com.unraveldata.airflow.protocol Type of connection, e.g., HTTPS or HTTP. You must set the airflow.login.name and airflow.login.password when this value is https. https com.unraveldata.airflow.server.url Full URL of the airflow server, starting with http:\/\/ and https:\/\/ . url http:\/\/localhost:10080 com.unraveldata.airflow.status.timeout.sec Set Airflow workflow status timeout in Unravel. sec 3600 com.unraveldata.airflow.task.log.parsing.enabled Controls whether to parse the Airflow Task logs. These logs are used to populate the Workflow Instance entities in the \"Jobs - Workflows\" page. boolean true com.unraveldata.airflow.task.log.parsing.operators Controls the Task logs to parse based on the Operator that produced it. Since Unravel only derives insights for Hive, Spark, and MR applications, it is set to only analyze operators that can launch those types of jobs. The values are delimited using a \",\" and it treats \"*\" as a wildcard to many any or no characters. Any special characters like \"\\\" or \".\" will be removed. string BashOperator, PythonOperator, *Hive*, *Spark* com.unraveldata.airflow.task.thread.pool.size Controls whether to process the Airflow Task logs sequentially or in parallel. Process the logs in parallel improves performance. This config takes effect only if airflow.task.log.parsing.enabled =true. Possible values are 1-16. If one, logs are processed sequentially. If greater than one, logs are processed in parallel using a Thread Pool with of the size specified value. count 1 Restart unravel_jcs2daemon . sudo \/etc\/init.d\/unravel_jcs2 restart Changing the Monitoring Window By default, Unravel Server ingests all the workflows that started within the last five (5) days. You change the date range to the last X days. Open \/usr\/local\/unravel\/etc\/unravel.properties and update the following property. If you can't find it, add it. Note there’s a “-” (minus sign) in the value. airflow.look.back.num.days=-X Restart the unravel_jcs2daemon . sudo \/etc\/init.d\/unravel_jcs2 restart Enabling AirFlow Here is a sample script, spark-test.py , for Spark. spark-test.py from airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators import PythonOperator\nfrom datetime import datetime, timedelta\nimport subprocess\n\n\ndefault_args = {\n 'owner': 'airflow',\n 'depends_on_past': False,\n 'start_date': datetime(2015, 6, 1),\n 'email': ['airflow@airflow.com'],\n 'email_on_failure': False,\n 'email_on_retry': False,\n 'retries': 1,\n 'retry_delay': timedelta(minutes=5),\n # 'queue': 'bash_queue',\n # 'pool': 'backfill',\n # 'priority_weight': 10,\n # 'end_date': datetime(2016, 1, 1),\n}\n In Airflow, workflows are represented by directed acyclic graphs (DAGs). For example, dag = DAG('spark-test', default_args=default_args)\n Add hooks for Unravel instrumentation. The following script, example-hdp-client.sh , adds hooks for Unravel instrumentation by setting three Unravel-specific configuration parameters for Spark applications. spark.driver.extraJavaOptions spark.executor.extraJavaOptions spark.unravel.server.hostport We recommend setting these parameters on a per-application basis only when you want to monitor\/profile certain applications, rather than all the applications running in the cluster. Alternatively, you can specify these parameters in spark-defaults.conf . This script can be invoked to submit an Airflow Spark application via spark-submit . It references the following variables, which need to be changed to values valid for your local environment. PATH_TO_SPARK_EXAMPLE_JAR =\/usr\/hdp\/2.3.6.0-3796\/spark\/lib\/spark-examples-*.jar UNRAVEL_SERVER_IP_PORT =10.20.30.40:4043 SPARK_EVENT_LOG_DIR =hdfs:\/\/ip-10-0-0-21.ec2.internal:8020\/user\/ec2-user\/eventlog example-hdp-client.sh hdfs dfs -rmr pair.parquet\nspark-submit \\\n--class org.apache.spark.examples.sql.RDDRelation \\\n--master yarn-cluster \\\n--conf \"spark.driver.extraJavaOptions=-javaagent:\/usr\/local\/unravel_client\/btrace-agent.jar=unsafe=true,stdout=false,noServer=true,startupRetransform=false,bootClassPath=\/usr\/local\/unravel_client\/unravel-boot.jar,systemClassPath=\/usr\/local\/unravel_client\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=DriverProbe.class:SQLProbe.class -Dcom.sun.btrace.FileClient.flush=-1 -Dcom.unraveldata.spark.sensor.disableLiveUpdates=true\" \\\n--conf \"spark.executor.extraJavaOptions=-javaagent:\/usr\/local\/unravel_client\/btrace-agent.jar=unsafe=true,stdout=false,noServer=true,startupRetransform=false,bootClassPath=\/usr\/local\/unravel_client\/unravel-boot.jar,systemClassPath=\/usr\/local\/unravel_client\/unravel-sys.jar,scriptOutputFile=\/dev\/null,script=ExecutorProbe.class -Dcom.sun.btrace.FileClient.flush=-1\" \\\n--conf \"spark.unravel.server.hostport=$UNRAVEL_SERVER_IP_PORT\" \\\n--conf \"spark.eventLog.dir=$SPARK_EVENT_LOG_DIR\" \\\n--conf \"spark.eventLog.enabled=true\" \\\n$PATH_TO_SPARK_EXAMPLE_JAR Submit the workflow. Operators (also called tasks) determine execution order (dependencies). In the example below, t1 and t2 are operators created by BashOperator or PythonOperator . They invoke example-hdp-client.sh , that submits the workflow for execution. Note: The path name of example-hdp-client.sh is relative to the current directory, not to ~\/airflow\/dags as in the Airflow operator above. t1 = BashOperator(\ntask_id='example-hdp-client',\nbash_command=\"example-scripts\/example-hdp-client.sh\",\nretries=3,\ndag=dag)\n\n\ndef spark_callback(**kwargs):\nsp\n = subprocess.Popen(['\/bin\/bash', \n'airflow\/dags\/example-scripts\/example-hdp-client.sh'], \nstdout=subprocess.PIPE, stderr=subprocess.STDOUT)\nprint sp.stdout.read()\n\n\nt2 = PythonOperator(\ntask_id='example-python-call',\nprovide_context=True,\npython_callable=spark_callback,\nretries=1,\ndag=dag)\n\n\nt2.set_upstream(t1)\n You can test the operators first. For example, in Airflow: airflow test spark-test example-python-call " }, 
{ "title" : "Monitoring Oozie workflows", 
"url" : "102258-workflow-oozie.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Configurations \/ Workflows \/ Monitoring Oozie workflows", 
"snippet" : "Enabling Oozie Open \/usr\/local\/unravel\/etc\/unravel.properties and search for oozie.server.url . If you don't find it, add it. Set it to your Oozie_host , e.g., http:\/\/Oozie_host:11000\/oozie. oozie.server.url= Oozie_host Restart unravel_os3 service. You can now monitor your Oozie workflows. \/etc\/init...", 
"body" : "Enabling Oozie Open \/usr\/local\/unravel\/etc\/unravel.properties and search for oozie.server.url . If you don't find it, add it. Set it to your Oozie_host , e.g., http:\/\/Oozie_host:11000\/oozie. oozie.server.url= Oozie_host Restart unravel_os3 service. You can now monitor your Oozie workflows. \/etc\/init.d\/unravel_os3 restart Monitoring Oozie To start monitoring your Oozie workflows go to Applications | Workflows . " }, 
{ "title" : "Miscellaneous", 
"url" : "102259-misc.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Miscellaneous", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Backup and archive metric database", 
"url" : "102260-adv-misc-archive-metric-db.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Miscellaneous \/ Backup and archive metric database", 
"snippet" : "Unravel's metBackup , is a utility for creating backups and archives for the metric database. Alternatively, you can use PostgreSQL pg_dump utility. The metBackup is run on demand, either periodically as a cron job or as needed. It creates a single tar file containing the data backup. When a table o...", 
"body" : "Unravel's metBackup , is a utility for creating backups and archives for the metric database. Alternatively, you can use PostgreSQL pg_dump utility. The metBackup is run on demand, either periodically as a cron job or as needed. It creates a single tar file containing the data backup. When a table or a partition is backed-up or archived, a log entry noting the time of the backup is added to the part_log file. The backup is not validated. metBackup takes advantage of the fact that frozen partitions are immutable and need to be backed-up only once. Optionally active partitions, i.e., not-yet-frozen, and the non-partitioned tables (node_desc, metric_desc, and part_log) can be backed-up. The metBackup utility is invoked through the script met_backup.sh . met_backup.sh [options] By default, regardless of data creation date, only frozen partitions are backed-up, if and only if, they were not previously backed-up. Options -f backupFile Name of backup tar file. Default metBackup_currentTimeAsEpoch . -d startDateAsEpoch Backup only data created on or after this date. -a Backup active partitions and the non-partitioned tables, node_desc, metric_desc, and part_log. -z Backup frozen data, regardless of whether it was previously backed-up. The retire thread is responsible for removing partitions past their expiration date. The normal procedure to retire the partition is to detach it from the parent table and delete it. When archiving is enabled (see the doArchive configuration property): Archived partitions are detached and deleted. Non-archived partitions are detached. The part_log is then periodically checked to see if the partition has been archived. Once archived, the partition is deleted. " }, 
{ "title" : "Cluster wide report", 
"url" : "102261-adv-misc-cluster-wide.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Miscellaneous \/ Cluster wide report", 
"snippet" : "This topic explains how to set up and run Unravel's Cluster Wide Report. Cluster Wide Report is a java App designed to help you fine tune your cluster wide parameters to maximize its efficiency based upon your cluster's typical workload. Cluster Wide Report Collects performance data of prior complet...", 
"body" : "This topic explains how to set up and run Unravel's Cluster Wide Report. Cluster Wide Report is a java App designed to help you fine tune your cluster wide parameters to maximize its efficiency based upon your cluster's typical workload. Cluster Wide Report Collects performance data of prior completed jobs. Analyzes the jobs relative to the cluster's current configuration. Generates recommended cluster parameter changes. Predicts and quantifies the impact the changes will have on future runs of the jobs. Most of the recommendations revolve around the parameters MapSplitSizeParams HiveExecReducersBytesParam HiveExecParallelParam MapReduceSlowStartParam MapReduceMemoryParams You can chose to implement some or all of the recommended settings. " }, 
{ "title" : "Step-by-step guide", 
"url" : "102261-adv-misc-cluster-wide.html#UUID-5e359a38-d38e-5bb0-a9db-42e16ec26d00_id_ClusterWideReport-Step-by-stepguide", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Miscellaneous \/ Cluster wide report \/ Step-by-step guide", 
"snippet" : "Download the report tar ball from the Unravel preview site. curl -v https:\/\/preview.unraveldata.com\/img\/ClusterReportSetup.tar.gz -o ClusterReportSetup.tar.gz Unpack and run the setup script to install app in \/usr\/local\/unravel\/install_bin . tar zxvf ClusterReportSetup.tar.gz cd ClusterReportSetup s...", 
"body" : "Download the report tar ball from the Unravel preview site. curl -v https:\/\/preview.unraveldata.com\/img\/ClusterReportSetup.tar.gz -o ClusterReportSetup.tar.gz Unpack and run the setup script to install app in \/usr\/local\/unravel\/install_bin . tar zxvf ClusterReportSetup.tar.gz\ncd ClusterReportSetup\nsudo .\/setup.sh \/usr\/local\/unravel\/install_bin The app is now installed in \/usr\/local\/unravel\/install_bin\/ClusterReport . cd to the installation directory. ls\ndbin\/ etc\/ origJars\/ \ndlib\/ logs\/ origJars.tar.gz cd to dbin and edit Input.txt . cd \/usr\/local\/unravel\/install_bin\/ClusterReport\/dbin \nvi Input.txt Configure Input.txt for your cluster and report parameters. cluster_id =\nqueue =\nstart_date=2018-01-01\nend_date=2018-03-28\nmapreduce.map.memory.mb=2048\nmapreduce.reduce.memory.mb=2048\nhive.exec.reducers.bytes.per.reducer=268435456\nmapreduce.input.fileinputformat.split.maxsize=256000000 Run the report. su - hdfs .\/cluster_report.sh " }, 
{ "title" : "Stopping, restarting, and configuring the AutoAction daemon", 
"url" : "102262-adv-misc-aa-daemon.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Miscellaneous \/ Stopping, restarting, and configuring the AutoAction daemon", 
"snippet" : "Unravel AutoActions is managed by the AutoAction daemon, unravel_aa , which you can stop, restart, and configure. There are no restrictions and the daemon can be stopped and started any time. Upon restart it synchronizes with the metric stream automatically and will continue to process data and enfo...", 
"body" : "Unravel AutoActions is managed by the AutoAction daemon, unravel_aa , which you can stop, restart, and configure. There are no restrictions and the daemon can be stopped and started any time. Upon restart it synchronizes with the metric stream automatically and will continue to process data and enforce Auto Action policies. Stopping\/starting or restarting the daemon may cause some data loss though and some violation events may be missed. You may want to stop the daemon if you do not want to use AutoActions or if they are not supported for your platform. Currently, AutoActions is supported for every platform except Unravel for Azure Databricks. You must be logged onto the Unravel server to start, stop, restart the daemon or configure the properties. To stop the daemon. sudo service unravel_aa stop To start the daemon. sudo service unravel_aa start To restart the daemon. sudo service unravel_aa restart Configure the daemon by resetting the one or more of the following properties in unravel.properties . Any changes and deviation from the default values should be done with extreme care as it can drastically change the behavior of the daemon and may cause unexpected results. It is always advisable to consult Unravel Support before making any adjustments. After making changes to the any of the properties, you must restart the daemon for them to take effect. Property\/Description Set by user Unit Default com.unraveldata.auto.action.enable.policy.enforce.in.jcs Enables AutoActions to be processed under the legacy (pre-4.5.2.0) mode of operation when AutoAction enforcement resided in JCS2 daemon. This works for on-prem mode only. When using this mode you can stop AutoAction daemon. boolean false com.unraveldata.auto.action.max.buffered.group.count A message group represents one complete YARN metrics polling cycle. It is incomplete when it has missing messages, messages out of order, etc. This property controls how many message groups can be buffered while waiting for completion of the current group. Once the daemon has buffered the max.buffered.group.count , it either drops or accepts the incomplete group (see max.lost.messages.count ) and moves onto processing the next group. Setting this value higher than 1 may increase latency of enforcement of an Auto Actions but can help to alleviate errors in message transport protocol. count 1 com.unraveldata.auto.action.max.lost.messages.count Maximum number of lost messages a group can have and still be be accepted for aggregation and policy enforcement. Accepting incomplete groups lowers the consistency of triggered violations but lets the daemon to operate with an “unstable” connection. 0: only complete groups are accepted. > 0: groups missing up to X messages are accepted. count 0 com.unraveldata.auto.action.metric.discard.ms When a incoming metric's timestamp is older than this value it is not processed but discarded. This mechanism is designed to deal with Kafka latency, consumer lags and other message delivery delays. It prevents the daemon from acting on outdated data and issuing false-positive violation events that are irrelevant at the present time ms 3600000 (1 hour) com.unraveldata.auto.action.policy.db.update.skip.cycle Defines how often the daemon refreshes the AutoAction policy definitions when aggregating and enforcing per enforcement cycle (see enforce.period.ms ). 0: All policies are read every time the daemon's ready to aggregate and enforce policies. > 0: The policies are read every X cycle. For instance, 1: the daemon reads them every other cycle, 2: every third period and so on. Changing this value makes sense only if there are a lot of policies defined. count 0 com.unraveldata.auto.action.policy.enforce.period.ms Maximum wait period for policy enforcement to be triggered. AutoAction metric aggregation and enforcement cycle is driven by metric producers, i.e., the daemon synchronizes with the metric polling cycles for each monitored cluster. When no YARN metrics are delivered or accepted and therefore policies are not getting evaluated for longer than this period, the daemon will forcibly execute metric aggregation and policy enforcement to process other internal Unravel metrics, such as Workflow, Hive, Tez, Impala metrics. ms 180000 (3 min) com.unraveldata.auto.action.transport.receiver.ttl.ms Identifies how long a transport protocol connection remains active without receiving any data from the cluster. If no metrics are received for longer than this value, the cluster is considered terminated and connection is closed on the receiver (daemon's) side. ms 1800000 (30 minutes) " }, 
{ "title" : "Moving MySQL to a custom location", 
"url" : "102263-adv-mysql-move.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Miscellaneous \/ Moving MySQL to a custom location", 
"snippet" : "Applicable only to Unravel v4.3.x or before. Depending on your deployment, follow the steps in one of the sections below. If you're using Unravel bundled MySQL, see Move a Bundled MySQL . If you're using an external MySQL, see Move an External MySQL . Whichever move you perform, you must do a slow s...", 
"body" : "Applicable only to Unravel v4.3.x or before. Depending on your deployment, follow the steps in one of the sections below. If you're using Unravel bundled MySQL, see Move a Bundled MySQL . If you're using an external MySQL, see Move an External MySQL . Whichever move you perform, you must do a slow shutdown ( ) which increases the time to stop MySQL. If you need to move MySQL to another host, you need to install MySQL first on the new host. For instructions, see here . " }, 
{ "title" : "Moving a bundled MySQL", 
"url" : "102263-adv-mysql-move.html#UUID-fe3917dd-1db4-a8c9-376e-fa2eecbb7029_MoveaBundledMySQL", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Miscellaneous \/ Moving MySQL to a custom location \/ Moving a bundled MySQL", 
"snippet" : "The daemon user must have read\/write access to the new path. Perform a slow shutdown ( ) of the MySQL server and make sure that it stops without errors. Get DB root password from \/root\/unravel.install.include or \/root\/unravel.install.include.prev . The example below uses *.include . grep 'DB_ROOT_PA...", 
"body" : "The daemon user must have read\/write access to the new path. Perform a slow shutdown ( ) of the MySQL server and make sure that it stops without errors. Get DB root password from \/root\/unravel.install.include or \/root\/unravel.install.include.prev . The example below uses *.include . grep 'DB_ROOT_PASSWORD' { \/root\/unravel.install.include | \/root\/unravel.install.include.prev } Run the following commands to set MySQL clean shutdown. \/usr\/local\/unravel\/mysql\/bin\/mysql -uroot --port=3316 --host=127.0.0.1 -p mysql> SET GLOBAL innodb_fast_shutdown=0; Stop unravel_db daemon. \/etc\/init.d\/unravel_db stop Back up MySQL database folder \/srv\/unravel\/db_data . cd \/srv\/unravel # tar cvf unravel_db_data.tar db_data\/ Restore MySQL datadir to the custom path. Replace new-db-location with the fully qualified path to the directory you want to place MySQL data base. cp unravel_db_data.tar new-db-location \ncd new-db-location # tar xvf unravel_db_data.tar Update unravel.install.include and the MySQL .cnf file: In \/root\/unravel.install.include set DB_DATADIR to new-db-location . In \/usr\/local\/unravel\/mysql\/unravel_mysql.cnf set both innodb_data_home_dir and innodb_log_group_home_dir to new-db-location . " }, 
{ "title" : "Moving an external MySQL", 
"url" : "102263-adv-mysql-move.html#UUID-fe3917dd-1db4-a8c9-376e-fa2eecbb7029_MoveanExternalMySQL", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Miscellaneous \/ Moving MySQL to a custom location \/ Moving an external MySQL", 
"snippet" : "The MySQL user must have read\/write access to the new path. Perform a slow shutdown ( ) of the MySQL server and make sure that it stops without errors. Run the following commands to set MySQL clean shutdown. mysql -uroot -p mysql> SET GLOBAL innodb_fast_shutdown=0; Stop MySQL daemon service mysqld s...", 
"body" : "The MySQL user must have read\/write access to the new path. Perform a slow shutdown ( ) of the MySQL server and make sure that it stops without errors. Run the following commands to set MySQL clean shutdown. mysql -uroot -p mysql> SET GLOBAL innodb_fast_shutdown=0; Stop MySQL daemon service mysqld stop Backup MySQL database folder \/var\/lib\/mysql cd \/var\/lib # tar cvf unravel_db_data.tar mysql\/ Restore MySQL datadir to the custom path. Replace NEW_DB_LOCATION with the fully qualified path to the directory you want to place MySQL data base. cp unravel_db_data.tar new-db-location \ncd new-db-location \ntar xvf unravel_db_data.tar Update MySQL cnf file In \/etc\/my.cnf set DB_DATADIR to new-db-location . " }, 
{ "title" : "Partitioning MySQL and migrating data", 
"url" : "102264-adv-mysql-partition.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Miscellaneous \/ Partitioning MySQL and migrating data", 
"snippet" : "Applicable only to Unravel v4.3.x or before. MySQL is not bundled with Unravel and you must manually migrate Unravel's tables for its use. The time for the data migration varies by machine and MySQL configuration. Unravel uses the following partitioned tables: BLACKBOARDS EVENT_INSTANCES HIVE_QUERIE...", 
"body" : "Applicable only to Unravel v4.3.x or before. MySQL is not bundled with Unravel and you must manually migrate Unravel's tables for its use. The time for the data migration varies by machine and MySQL configuration. Unravel uses the following partitioned tables: BLACKBOARDS EVENT_INSTANCES HIVE_QUERIES IMPALA_QUERIES JOBS OOZIE_WORKFLOW_JOBS Upgrade Unravel Server. Stop all daemons. \/etc\/init.d\/unravel_all.sh stop Make sure MySQL is running. Perform the data migration. For external MySQL servers you might need to change the chunk size to 100 or 1000. \/usr\/local\/unravel\/dbin\/db_schema_upgrade.sh migration:migration_partitioning -verbose During migration process the status of the tables is written to stdout . The output is continually updated until the migration for the table is complete. Start migration: migration_partitioning\nStart table rows calculation...\nTable blackboards_old has 0 rows total\nTable impala_queries_old has 0 rows total\nTable jobs_old has 205437 rows total\nTable event_instances_old has 0 rows total\nTable oozie_workflow_jobs_old has 0 rows total\nTable hive_queries_old has 0 rows total\nTable rows calculation finished.\nMigrating records for table: blackboards, chunk size: 10000, chunk count: 10...\nMigrated records for table: blackboards, rows total: 0, chunk size: 10000, chunk count: 10, finished: true, time: 0 seconds, progress 100,00 %\nMigrating records for table: impala_queries, chunk size: 1500, chunk count: 10...\nMigrated records for table: impala_queries, rows total: 0, chunk size: 1500, chunk count: 10, finished: true, time: 0 seconds, progress 100,00 %\nMigrating records for table: jobs, chunk size: 500, chunk count: 10...\nMigrated records for table: jobs, rows total: 5000, chunk size: 500, chunk count: 10, finished: false, time: 32 seconds, progress 2,43 %\nMigrating records for table: event_instances, chunk size: 50000, chunk count: 10...\nMigrated records for table: event_instances, rows total: 0, chunk size: 50000, chunk count: 10, finished: true, time: 0 seconds, progress 100,00 %\nMigrating records for table: oozie_workflow_jobs, chunk size: 1500, chunk count: 10...\nMigrated records for table: oozie_workflow_jobs, rows total: 0, chunk size: 1500, chunk count: 10, finished: true, time: 0 seconds, progress 100,00 %\nMigrating records for table: hive_queries, chunk size: 1500, chunk count: 10...\nMigrated records for table: hive_queries, rows total: 0, chunk size: 1500, chunk count: 10, finished: true, time: 0 seconds, progress 100,00 %\nMigrating records for table: jobs, chunk size: 500, chunk count: 10...\nMigrated records for table: jobs, rows total: 10000, chunk size: 500, chunk count: 10, finished: false, time: 33 seconds, progress 4,87 %\nMigrating records for table: jobs, chunk size: 500, chunk count: 10... When all the tables have been successfully migrated you see: Migration: migration_partitioning is finished During the process a temporary migration_partitioning DB table keeps track of the tables' status. You can view this table using the following MySQL query. The chunks field increases until the migration for the table is complete. Upon completion the table's finished column is set to 1 . \/usr\/local\/unravel\/install_bin\/db_access.sh\n\nmysql> select * from migration_partitioning;\n+---------------------+--------------------+--------+----------+\n| migr_table_name | lowest_id_migrated | chunks | finished |\n+---------------------+--------------------+--------+----------+\n| blackboards | 11018258 | 119 | 0 |\n| event_instances | 0 | 1 | 1 |\n| hive_queries | 0 | 1 | 1 |\n| impala_queries | 0 | 1 | 1 |\n| jobs | 240884 | 100 | 0 |\n| oozie_workflow_jobs | 0 | 1 | 1 |\n+---------------------+--------------------+--------+----------+\n\n To see the oldest migrated records per each table you can use the following MySQL query. \/usr\/local\/unravel\/install_bin\/db_access.sh\n\nmysql>\nselect \"blackboards\" as \"table_name\", min(created_at) as \"oldest_migrated_record\" from blackboards\nunion select \"event_instances\", min(created_at) from event_instances\nunion select \"hive_queries\", min(created_at) from hive_queries\nunion select \"impala_queries\", min(created_at) from impala_queries\nunion select \"jobs\", min(created_at) from jobs\nunion select \"oozie_workflow_jobs\", min(created_at) from oozie_workflow_jobs;\n+---------------------+------------------------+\n| table_name | oldest_migrated_record |\n+---------------------+------------------------+\n| blackboards | 2018-08-30 00:57:45 |\n| event_instances | 2018-08-30 00:57:55 |\n| hive_queries | 2018-08-30 00:59:10 |\n| impala_queries | NULL |\n| jobs | 2018-08-30 00:57:50 |\n| oozie_workflow_jobs | 2018-09-07 10:25:22 |\n+---------------------+------------------------+ If the migration process is interrupted or killed, you can run shell script again. However, if the process has failed you must truncate and reset ID's before running the script again. During migration the original files were renamed to TableName _old . Upon completion, delete these and the migration_partitioning table. \/usr\/local\/unravel\/install_bin\/db_access.sh\n\nDROP TABLE blackboards_old;\nDROP TABLE event_instances_old;\nDROP TABLE hive_queries_old;\nDROP TABLE impala_queries_old;\nDROP TABLE jobs_old;\nDROP TABLE oozie_workflow_jobs_old;\nDROP TABLE migration_partitioning;\nquit Restart daemons. \/etc\/init.d\/unravel_all.sh start " }, 
{ "title" : "Restarting a failed migration", 
"url" : "102264-adv-mysql-partition.html#UUID-99ad6cac-1199-67aa-b557-fcbc10f83e00_RestartHowtorestartafailedmigration", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Miscellaneous \/ Partitioning MySQL and migrating data \/ Restarting a failed migration", 
"snippet" : "Truncate partitioned tables, reset autoincrement ID values and truncate migration_partitioning table. \/etc\/init.d\/unravel_all.sh stop \/usr\/local\/unravel\/install_bin\/db_access.sh truncate blackboards; truncate event_instances; truncate hive_queries; truncate impala_queries; truncate jobs; truncate oo...", 
"body" : "Truncate partitioned tables, reset autoincrement ID values and truncate migration_partitioning table. \/etc\/init.d\/unravel_all.sh stop\n\n\/usr\/local\/unravel\/install_bin\/db_access.sh\n\ntruncate blackboards;\ntruncate event_instances;\ntruncate hive_queries;\ntruncate impala_queries;\ntruncate jobs;\ntruncate oozie_workflow_jobs;\n\ntruncate migration_partitioning;\n\ncall resetAutoIncrementId('blackboards_old', 'blackboards');\ncall resetAutoIncrementId('event_instances_old', 'event_instances');\ncall resetAutoIncrementId('hive_queries_old', 'hive_queries');\ncall resetAutoIncrementId('impala_queries_old', 'impala_queries');\ncall resetAutoIncrementId('jobs_old', 'jobs');\ncall resetAutoIncrementId('oozie_workflow_jobs_old', 'oozie_workflow_jobs'); Return to Step 3 and complete the remaining steps. " }, 
{ "title" : "REST API", 
"url" : "102265-api.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API", 
"snippet" : "Unravel's REST API allows you to query and collect data from your cluster, using either HTTP or HTTPS protocol. The API returns information similar to what you see in Unravel UI. All requests and responses are in JSON format. Authentication If you're running the API through Unravel UI's Manage | API...", 
"body" : "Unravel's REST API allows you to query and collect data from your cluster, using either HTTP or HTTPS protocol. The API returns information similar to what you see in Unravel UI. All requests and responses are in JSON format. Authentication If you're running the API through Unravel UI's Manage | API , authenticate through the UI ; be sure to select the correct protocol (HTTPS or HTTP). Authenticate through the \/signIn request. Your access level is determined by your RBAC role . Unravel Server returns a session token which you then include in all subsequent requests. Methods The API adheres to the standard create-read-update-delete (CRUD) semantics in which the request path defines the endpoint, and the method specifies the action to perform on the resource. Method Action POST Creates a resource. GET Retrieves a resource. PUT Updates or edits a resource. DELETE Deletes a resource. Endpoints Endpoint Description Authenticates your API session and gets a token which you include in all requests in this session. Gets all apps filtered by app type, status, username, queue, and tags. Gets average number of jobs by type. Gets tags for a given app. Gets total job count per status in a given time window (at one hour intervals). Gets a time series for CPU allocattions by app type. Gets a time series for allocated memory by app type. Gets a time series of finished apps. Gets a list of running apps. Gets the status code for a killed YARN app. Gets the status code for a moved YARN app. Gets a summary of a given app. Gets the errors associated with a given app. Gets a detailed summary of a given app. Gets the logs associated with a given app. Gets Unravel's recommendations for the given application. Gets the status of a given app. \/reports\/files\/reports\/latest?reportType=huge_files Under construction. Under construction. \/reports\/data\/diskusage\/get_latest_reports?task_name=capacity_prediction_task \/reports\/data\/diskusage\/get_latest_reports?task_name=cloud_mappings_grouped_reports_task \/reports\/data\/diskusage\/get_latest_reports?task_name=cloud_reports_task \/reports\/data\/diskusage\/get_latest_reports?task_name=cluster_discovery_task \/reports\/data\/diskusage\/get_latest_reports?task_name=queue_analysis_task \/reports\/data\/diskusage\/get_latest_reports?task_name=small_files_report \/reports\/data\/diskusage\/get_latest_reports?task_name=topx_report \/reports\/data\/diskusage\/get_latest_success_reports?task_name=cloud_mappings_grouped_reports_task \/reports\/data\/diskusage\/get_latest_success_reports?task_name=cloud_reports_task \/reports\/data\/diskusage\/get_latest_success_reports?task_name=cluster_discovery_task \/reports\/data\/diskusage\/get_latest_success_reports?task_name=queue_analysis_task \/reports\/data\/diskusage\/get_latest_success_reports?task_name=topx_report Generates a Key Performance Indicator (KPI) report. Gets details about the small file report. Under construction. Under construction. Under construction. \/reports\/files\/reports\/latest?reportType=empty_files \/reports\/files\/reports\/latest?reportType=medium_files \/reports\/files\/reports\/latest?reportType=tiny_files Under construction. Gets chargeback reports by application type. Gives the count of all applications in all queues for all users across all clusters. Gets chargeback reports by queue. Gets chargeback reports by user. Generates a Queue Analysis report. Generates a Cloud Mapping grouped report. Generates a Cloud Mappping report. Generates a report on cloud providers. Generates a Cluster Discovery report. Generates a cluster app parameter report. Generates a Forecasting report. Gets the Small Files report. Gets a list of active and inactive auto actions. Gets a list of auto action metrics. Gets a list of recent auto action violations. Gets total job count, at one hour intervals. Gets the number of jobs in a given timeframe, grouped by state, app type, user, or queue. Gets the total job count per job type, at one hour intervals, in a given time window. Gets the total job count per job type, at one hour intervals, in a given time window. Gets a list of clusters. Gets a summary of cluster nodes. Gets a time series of CPU allocations across all clusters. Gets a time series of memory allocations across all clusters. Gets total CPU usage across all clusters. Gets total memory usage across all clusters. Gets a time series for allocated CPU in a given cluster. Gets a time series for allocated memory in a given cluster. Gets a time series for total CPU usage in a given cluster. Gets a time series for total memory usage in a given cluster. Gets a time series of all the nodes of given cluster grouped by node state. Gets a list of Kafka brokers active on a given cluster during a given timeframe. Gets KPIs for a given Kafka broker. Gets KPIs for a given Kafka cluster. Gets KPIs for the Kafka topics within a given timespan. Gets Kafka cluster metrics. Gets details about a given partition. Gets chargeback reports per app type and the number of apps in all queues for all users across all clusters. Gets chargeback reports by application type for a specific queue. Gets chargeback reports per queue. Lists all workflows. Lists workflows with missed SLA. Error codes Unravel Server returns these HTTP error codes: Error Code Description 400 Invalid request parameters or malformed requests. 401 Authentication failure. 403 Authorization failure. 404 Object not found. 500 Internal API error. 503 Response is temporarily unavailable; try again later. " }, 
{ "title" : "Base URL", 
"url" : "102265-api.html#UUID-42e26ca0-83bc-4134-4fb4-e2d32b60bf7c_section-5cf0c53f9c422-idm45390834619520", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ Base URL", 
"snippet" : "All endpoints are relative to the base URL, http:\/\/ unravel-host :3000\/api\/v1 or https:\/\/ unravel-host :3000\/api\/v1...", 
"body" : "All endpoints are relative to the base URL, http:\/\/ unravel-host :3000\/api\/v1 or https:\/\/ unravel-host :3000\/api\/v1 " }, 
{ "title" : "Swagger content", 
"url" : "102266-api-swagger-45xx.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ Swagger content", 
"snippet" : "If the content of this page is slow to load, reload the page. Swagger content for Unravel 4.5.x.x...", 
"body" : "If the content of this page is slow to load, reload the page. Swagger content for Unravel 4.5.x.x " }, 
{ "title" : "Authentication through Unravel UI", 
"url" : "102267-api-rest-ui.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ Authentication through Unravel UI", 
"snippet" : "Log into Unravel UI and select the API page. Expand POST \/signIn and click Try it out . Enter your username and password and click Execute . Copy the value of the token field in the response body. Don't include the quotation marks. Click Authorize at the top of the page. In the Value text box, paste...", 
"body" : "Log into Unravel UI and select the API page. Expand POST \/signIn and click Try it out . Enter your username and password and click Execute . Copy the value of the token field in the response body. Don't include the quotation marks. Click Authorize at the top of the page. In the Value text box, paste the token after the string JWT . Make sure there's a space between JWT and the token. Click Authorize . " }, 
{ "title" : "Applications API", 
"url" : "102268-api-rest-applications.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ Applications API", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Apps API", 
"url" : "102269-api-rest-apps.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ Apps API", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "AutoActions API", 
"url" : "102270-api-rest-auto-actions.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ AutoActions API", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Capacity forecasting API", 
"url" : "102271-api-rest-forecasting-reports.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ Capacity forecasting API", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Chargeback reports API", 
"url" : "102272-api-rest-chargeback-reports.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ Chargeback reports API", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Cluster optimization reports API", 
"url" : "102273-api-rest-cluster-optimization-reports.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ Cluster optimization reports API", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Data insights API", 
"url" : "102274-api-rest-data-insights-reports.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ Data insights API", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "File reports API", 
"url" : "102275-api-rest-files-reports.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ File reports API", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Jobs API", 
"url" : "102276-api-rest-jobs.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ Jobs API", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Migration reports API", 
"url" : "102277-api-rest-migration-reports.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ Migration reports API", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Operational", 
"url" : "102278-api-rest-operational-move.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ Operational", 
"snippet" : "All timestamps are in Unix epoch. Cluster Nodes by health Return the count by node status\/health. curl -X GET \"http:\/\/ UNRAVEL_HOST : Port \/api\/v1\/clusters\/resources\/clusters\/nodes?to= End &interval= Interval &from= Start \" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Additional requ...", 
"body" : "All timestamps are in Unix epoch. Cluster Nodes by health Return the count by node status\/health. curl -X GET \"http:\/\/ UNRAVEL_HOST : Port \/api\/v1\/clusters\/resources\/clusters\/nodes?to= End &interval= Interval &from= Start \" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Additional required parameters: Polling Interval : 1m | 5m | 10m | 30m |1h Schema Note : total = active + unhealthly { \n \"date\" : [ \n \/\/ array of polling EPOCH_timestamp\n EPOCH_timestamp\n ],\n \"total\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n },\n \"active\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n },\n \"lost\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n }, \n \"unhealthy\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n }, \n \"decommissioned\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n }, \n \"rebooted\" : { \n \/\/ for each polling interval\n EPOCH_timestamp: count\n }\n}\n Example curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/nodes?to=1536339111&interval=1h&from=1535734311\" -H \"accept: application\/json\" -H \"Authorization: JWT token \"\n Sample Output {\n \"date\": [\n 1535688000000,\n 1535691600000,\n 1535695200000,\n 1535698800000\n ],\n \"total\": {\n \"1535688000000\": 3,\n \"1535691600000\": 3,\n \"1535695200000\": 3,\n \"1535698800000\": 3\n },\n \"active\": {\n \"1535688000000\": 1,\n \"1535691600000\": 1,\n \"1535695200000\": 1,\n \"1535698800000\": 1\n },\n \"lost\": {\n \"1535688000000\": 0,\n \"1535691600000\": 0,\n \"1535695200000\": 0,\n \"1535698800000\": 0\n },\n \"unhealthy\": {\n \"1535688000000\": 1,\n \"1535691600000\": 1,\n \"1535695200000\": 1,\n \"1535698800000\": 1\n },\n \"decommissioned\": {\n \"1535688000000\": 1,\n \"1535691600000\": 1,\n \"1535695200000\": 1,\n \"1535698800000\": 1\n },\n \"rebooted\": {\n \"1535688000000\": 0,\n \"1535691600000\": 0,\n \"1535695200000\": 0,\n \"1535698800000\": 0\n }\n} " }, 
{ "title" : "Operations API", 
"url" : "102279-api-rest-operations.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ Operations API", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Queue analysis reports API", 
"url" : "102280-api-rest-queue-analysis-reports.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ Queue analysis reports API", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Report archives API", 
"url" : "102281-api-rest-report-archives.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ Report archives API", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Scheduled reports API", 
"url" : "102282-api-rest-scheduled-reports.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ Scheduled reports API", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Small files reports API", 
"url" : "102283-api-rest-small-files-reports.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ Small files reports API", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Top \"x\" reports API", 
"url" : "102284-api-rest-topx-reports.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ Top \"x\" reports API", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Workflows API", 
"url" : "102285-api-rest-workflows.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ Workflows API", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Use case - AutoActions and Pagerduty", 
"url" : "102286-api-use-case-autoactions-and-pagerduty.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ Use case - AutoActions and Pagerduty", 
"snippet" : "This topics explains how to configure Pagerduty and your Unravel server in order to receive auto action notifications via Pagerduty. The auto action's template provides the ability to send alerts via email to one or more recipients and\/or create a HTTP endpoint allowing integration with a third part...", 
"body" : "This topics explains how to configure Pagerduty and your Unravel server in order to receive auto action notifications via Pagerduty. The auto action's template provides the ability to send alerts via email to one or more recipients and\/or create a HTTP endpoint allowing integration with a third party tool, e.g., Slack. These two options are specified when creating\/editing an auto action. Once entered nothing further needs to be done for notifications to be sent. Unravel has developed a third option allowing you to use Pagerduty to send notifications to one or more users through Unravel's auto actions API . Currently, this action is initiated outside of Unravel's server. For the integration you need to complete two (2) setups: Set up a service at Pagerduty and specify who should be notified and how (email, etc.), and Run a python script on your local machine, specifying the Unravel server and Pagerduty information. The python script must be running with the correct parameters in order for pagerduty to receive and forward notifications. Set up a Pagerduty service Login to your Pagerduty Account. Select the Configuration pulldown menu and chose Services . You can register at Pager Duty . Select Add New Service . Name the new service. Select Use our API directly and chose Events API v2 from the pull down menu. Use the defaults for the remaining options and click Add Service . Copy the Integration Key from the Service Details Integrations tab as you need it for Unravel's Auto Actions API. Run the Unravel API python script on your local computer Download the Unravel demo program to the directory of your choice on your local machine. # git clone https:\/\/github.com\/Unravel-Andy\/unravel-api-demo-v1 cd to unravel-api-demo-v1 . # cd unravel-api-demo-v1\n# ls\nREADME.md Run the python script to bring up the Unravel API Demo window Enter your Unravel Server's address in Unravel Autoactions API Address text box. The Autoactions API address has the form of http[s]:\/\/UNRAVEL_HOST_IP\/api\/v1\/autocactions . Substitute a fully qualified DNS or IP address for UNRAVEL_HOST_IP . In the example below the server is https:\/\/playground.unraveldata.com. Enter the pagerduty integration key in the Pagerduty API key text box. Be sure to enter in the integration key correctly ensure a connection with Pagerduty. An incorrect key, or spaces around the key will prevent connecting with Pagerduty. Click Start to populate the window with the defined auto actions and their status. These are sorted on status first and then alphabetically in ascending order. To scroll within list, click within the Autoaction Name list and scroll. Open your browser and login to the Unravel Server you entered above. Navigate to the Auto Actions page ( Manage | Auto Actions ). See Creating Auto Actions if you don't know how to navigate to Auto Actions. The Unravel UI shows the same list of active and inactive Auto Actions listed in Unravel API window. Both the Unravel API window and the Unravel UI shows the exact same list of Autoactions and their status. Toggle one of the autoactions to see the effect. In our example there were 2 active autoactions, Kill job hogging the cluster and Rogue App AA #1. We deactivated Kill job hogging the cluster . In the UI and Unravel API window, you will see the change. Kill job hogging the cluster was moved to the inactive status\/list in both the Unravel UI and Unravel Autoactions API window. Updated Unravel UI Updated Unravel API - In both screenshots below, Kill job hogging the cluster is shown inactive. The one on the left shows a successful connection to pagerduty, so notification was made. The shot on the right shows a failed connection with no notification made. The key was invalid because a space was added to the end of the string. Pagerduty, upon receiving the status change information, will transmit it to the parties specified in the service detail. Sample sms message Sample email The python script must be running with the correct parameters in order for pagerduty to receive and forward notifications. " }, 
{ "title" : "All endpoints by resource name", 
"url" : "102287-api-rest-all-endpoints.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Apps endpoints", 
"url" : "102288-resource-apps.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Apps endpoints", 
"snippet" : "Apps endpoints provide detailed information about your applications, similar to the Applications page in Unravel UI. You can collect data based on application name, type, user, queues, and tags....", 
"body" : "Apps endpoints provide detailed information about your applications, similar to the Applications page in Unravel UI. You can collect data based on application name, type, user, queues, and tags. " }, 
{ "title" : "\/apps\/{app_id}\/resource_usage", 
"url" : "102289-apps-appid-resource-usage.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Apps endpoints \/ \/apps\/{app_id}\/resource_usage", 
"snippet" : "Gets an array of resource usage metrics for a given application. Request GET http:\/\/ unravel-host :3000\/api\/v1\/apps\/{app_id}\/resource_usage?metric_id={metric_id} Path parameters Name Type Description app_id string App ID Query parameters Required parameters are highlighted . Name Type Description me...", 
"body" : "Gets an array of resource usage metrics for a given application. Request GET http:\/\/ unravel-host :3000\/api\/v1\/apps\/{app_id}\/resource_usage?metric_id={metric_id} Path parameters Name Type Description app_id string App ID Query parameters Required parameters are highlighted . Name Type Description metric_id string Metric ID Valid values are: 115: gcLoad 125: maxHeap 126: usedHeap 134: processCpuLoad 135: systemCpuLoad 137: availableMemory 138: vmRss Response body Name Type Description appId string App ID unravel_metric_id integer Metric ID system string entity_id string entity_name string host_name string sampleCount integer sum integer min integer max integer avg integer ts integer endTs integer unit string Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/apps\/20190626T032742Z--6535643260096439568\/resource_usage?metric_id=138\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response body: [\n {\n \"appId\":\"20190626T032742Z--6535643260096439568\",\n \"unravel_metric_id\":138,\n \"system\":\"mr\",\n \"entity_id\":\"a_1\",\n \"entity_name\":\"a_1\",\n \"host_name\":\"myserver.com\",\n \"sampleCount\":2,\n \"sum\":772329472,\n \"min\":384274432,\n \"max\":388055040,\n \"avg\":386164736,\n \"ts\":1553500850000,\n \"endTs\":1553500860000,\n \"unit\":\"BYTES\"\n }\n] " }, 
{ "title" : "\/apps\/{app_id}\/tags", 
"url" : "102290-apps-appid-tags.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Apps endpoints \/ \/apps\/{app_id}\/tags", 
"snippet" : "Gets tags for a given app. Request GET http:\/\/ unravel-host :3000\/api\/v1\/apps\/{app_id}\/tags Path parameters Name Type Description app_id string App ID Query parameters None. Response body Name Type Description realUser string Username project string Project name dept string Department name Examples ...", 
"body" : "Gets tags for a given app. Request GET http:\/\/ unravel-host :3000\/api\/v1\/apps\/{app_id}\/tags Path parameters Name Type Description app_id string App ID Query parameters None. Response body Name Type Description realUser string Username project string Project name dept string Department name Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/apps\/hive_20190531170145_12ccd59c-cecf-457b-b440-86b5f187097c\/tags\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" " }, 
{ "title" : "\/apps\/events\/inefficient_apps", 
"url" : "102291-apps-events-inefficient-apps.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Apps endpoints \/ \/apps\/events\/inefficient_apps", 
"snippet" : "Gets total job count per status in a given time window (at one hour intervals). Request GET http:\/\/ unravel-host :3000\/api\/v1\/apps\/events\/inefficient_apps?start_time={timestamp}&end_time={timestamp}&entity_type={application type} Path parameters None. Query parameters Required parameters are highlig...", 
"body" : "Gets total job count per status in a given time window (at one hour intervals). Request GET http:\/\/ unravel-host :3000\/api\/v1\/apps\/events\/inefficient_apps?start_time={timestamp}&end_time={timestamp}&entity_type={application type} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description start_time string Start date. Format YYYY-MM-DD end_time string End date. Format YYYY-MM-DD entity_type integer Application type. Valid values are: 0 : MR 1 : Hive 2 : Spark 16 : Impala Response body Name Type Description entity_type integer Entity type event_name string Event name count integer Examples Request: curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/apps\/events\/inefficient_apps\/?start_time=2019-03-01&end_time=2019-03-06&amp;entity_type=2\" -H \"accept: application\/json\"-H \"accept: application\/json\" -H \"Authorization: JWT token \" Response body: [\n {\n \"entity_type\": 0,\n \"event_name\": \"MRTooLargeMapEvent\",\n \"count\": 10559\n },\n {\n \"entity_type\": 0,\n \"event_name\": \"MRTimeBreakdownEvent\",\n \"count\": 7868\n },\n {\n \"entity_type\": 0,\n \"event_name\": \"MRTooLargeReduceEvent\",\n \"count\": 4782\n },\n {\n \"entity_type\": 0,\n \"event_name\": \"MRTooManyMapEvent2\",\n \"count\": 2097\n },\n {\n \"entity_type\": 0,\n \"event_name\": \"MRTooManyReduceEvent2\",\n \"count\": 1066\n },\n {\n \"entity_type\": 0,\n \"event_name\": \"LongReduceTasksStartBeforeMapFinishEvent\",\n \"count\": 21\n },\n {\n \"entity_type\": 0,\n \"event_name\": \"MRTooFewMapEvent2\",\n \"count\": 3\n },\n {\n \"entity_type\": 0,\n \"event_name\": \"MRReduceTimeSkewEvent\",\n \"count\": 2\n },\n {\n \"entity_type\": 0,\n \"event_name\": \"MRMapTimeSkewEvent\",\n \"count\": 1\n }\n] " }, 
{ "title" : "\/apps\/resources\/cpu\/allocated", 
"url" : "102292-apps-resources-cpu-allocated.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Apps endpoints \/ \/apps\/resources\/cpu\/allocated", 
"snippet" : "Gets a time series for CPU allocattions by app type. Request GET http:\/\/ unravel-host :3000\/api\/v1\/apps\/resources\/cpu\/allocated Path parameters None. Query parameters Required parameters are highlighted . Name Type Description from string Start date. Format YYYY-MM-DD to string End date. Format YYYY...", 
"body" : "Gets a time series for CPU allocattions by app type. Request GET http:\/\/ unravel-host :3000\/api\/v1\/apps\/resources\/cpu\/allocated Path parameters None. Query parameters Required parameters are highlighted . Name Type Description from string Start date. Format YYYY-MM-DD to string End date. Format YYYY-MM-DD Response body Name Type Description date integer Timestamp in Unix epoch format SUCCEEDED integer Examples Request: curl -X GET \"http:\/\/playground3.unraveldata.com\/api\/v1\/apps\/events\/inefficient_apps\/?start_time=2019-03-01&end_time=2019-03-06&entity_type=2\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response body: [\n {\n \"date\": 1551938400000,\n \"SUCCEEDED\": 86\n },\n {\n \"date\": 1551942000000,\n \"SUCCEEDED\": 20\n },\n {\n \"date\": 1551945600000,\n \"SUCCEEDED\": 34\n }\n] " }, 
{ "title" : "\/apps\/resources\/memory\/allocated", 
"url" : "102293-apps-resources-memory-allocated.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Apps endpoints \/ \/apps\/resources\/memory\/allocated", 
"snippet" : "Gets a time series for allocated memory by app type. Request GET http:\/\/ unravel-host :3000\/api\/v1\/apps\/resources\/memory\/allocated Path parameters None. Query parameters Required parameters are highlighted . Name Type Description from string Start date. Format YYYY-MM-DD to string End date. Format Y...", 
"body" : "Gets a time series for allocated memory by app type. Request GET http:\/\/ unravel-host :3000\/api\/v1\/apps\/resources\/memory\/allocated Path parameters None. Query parameters Required parameters are highlighted . Name Type Description from string Start date. Format YYYY-MM-DD to string End date. Format YYYY-MM-DD Response body Name Type Description timestamp string Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/apps\/resources\/memory\/allocated?from=2019-03-01&to=2019-03-06\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response body: {\n \"1551787200000\":\"2\"\n} " }, 
{ "title" : "\/apps\/search", 
"url" : "102294-apps-search.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Apps endpoints \/ \/apps\/search", 
"snippet" : "Gets all apps filtered by app type, status, username, queue, and tags. Request POST http:\/\/ unravel-host :3000\/api\/v1\/apps\/search -d '{\"from\":0,\"appTypes\":[{app_types}],\"appStatus\":[{app_status},\"end_time\":\"{timestamp}\",\"start_time\":\"{timestamp}\",\"users\":[\"{username}\"],\"queues\":[\"{queuename} \"],\"app...", 
"body" : "Gets all apps filtered by app type, status, username, queue, and tags. Request POST http:\/\/ unravel-host :3000\/api\/v1\/apps\/search -d '{\"from\":0,\"appTypes\":[{app_types}],\"appStatus\":[{app_status},\"end_time\":\"{timestamp}\",\"start_time\":\"{timestamp}\",\"users\":[\"{username}\"],\"queues\":[\"{queuename} \"],\"appTags\":{\"taglist\"}}' Path parameters None. Query parameters Required parameters are highlighted . Name Type Description from integer Set this to 0 . appTypes string List of app types. Format: [\" type1 \",\" type2 \"] Valid values are mr | hive | spark | pig | cascading | impala | tez . appStatus string App status. Format: [\" status1 \",\" status2 \"] Valid values are S uccess | F ailed | K illed | R unning | W | P ending | U nkown. For Hive and MR you must specify at least one status type. start_time string Start time. Format: YYYY-MM-DDTHH:MM:SS.NNNZ end_time string End time. Format: YYYY-MM-DDTHH:MM:SS.NNNZ users string Usernames(s). Format: [\" user1 \",\" user2 \"] queues string Cluster's queue names(s). Format: [\" queue1 \",\" queue2 \"] taglist string Tag(s). Format: \" key \":[\" value , value \"] . For example, \"dept\":[finance, mktg] Response body The JSON response body contains a metadata section and a results section that vary according to the request. Examples List all apps: curl -X POST \"http:\/\/myserver.com:3000\/api\/v1\/apps\/search\" -H \"Content-Type: application\/json\" -H \"Authorization: JWT token \" List all failed, killed, or unknown Hive and Tez apps within a specific timeframe: curl -X POST \"http:\/\/myserver.com:3000\/api\/v1\/apps\/search\" -H \"Content-Type: application\/json\" -H \"Authorization: JWT token \" -d '{\"from\":0,\"appTypes\":[\"hive\",\"tez\"],\"appStatus\":[\"F\",\"K\",\"U\"],\"end_time\":\"2018-10-09T05:18:42.000Z\",\"start_time\":\"2018-10-09T02:18:42.000Z\"}' List all apps owned by a user regardless of status, queue, or tags. curl -X POST \"http:\/\/myserver.com:3000\/api\/v1\/apps\/search\" -H \"Content-Type: application\/json\" -H \"Authorization: JWT token \" -d '{\"from\":0,\"appTypes\":[\"mr\",\"hive\",\"spark\",\"pig\",\"cascading\",\"impala\",\"tez\"],\"appStatus\":[\"S\",\"F\",\"K\",\"R\",\"W\",\"P\",\"U\"],\"end_time\":\"20190626T042742.000Z\",\"start_time\":\"20190626T032742.000Z\",\"users\":[\"root\"]}' Response body: {\n \"metadata\": {\n \"duration\": {\n \"max\": 163367,\n \"min\": 4135\n },\n \"resource\": {\n \"max\": null,\n \"min\": null\n },\n \"events\": {\n \"max\": 3,\n \"min\": 0\n },\n \"appTypes\": {\n \"spark\": 28\n },\n \"appStatus\": {\n \"S\": 23,\n \"F\": 5\n },\n \"users\": {\n \"unravel\": 321\n },\n \"queues\": {\n \"root.users.unravel\": 113,\n \"default\": 94\n },\n \"clusters\": {\n \"QACDH601A\": 29\n },\n \"totalRecords\": 28\n },\n \"results\": [\n {\n \"appId\": \"20190626T032742Z--6535643260096439568\",\n \"appType\": \"wfi\",\n \"appt\": \"wfi\",\n \"gotoId\": \"20190626T032742Z--6535643260096439568\",\n \"gotoLevel\": \"WFI\",\n \"id\": \"application_1558643494852_0916\",\n \"nick\": \"spark\",\n \"name\": \"multiple_jobs_stages.py\",\n \"queue\": \"root.users.unravel\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"unravel\",\n \"raw_user\": \"unravel\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"-\"\n ],\n \"aa2Badge\": false,\n \"inBadge\": false,\n \"clusterId\": \"QACDH601A\",\n \"clusterTag\": \"1558643494852\",\n \"start_time\": \"06\/26\/19 10:28:18\",\n \"start_time_long\": \"2019-06-26T10:28:18.403Z\",\n \"duration_long\": 21658,\n \"predDuration_long\": 0,\n \"io_long\": 0,\n \"read_long\": 0,\n \"write_long\": 0,\n \"resource\": 0,\n \"service\": 0,\n \"events\": 3,\n \"numApps\": 0,\n \"numSparkApps\": 25,\n \"numMRJobs\": 25,\n \"numEvents\": 3,\n \"mrJobIds\": [\n \"stage-0\",\n \"stage-1\",\n \"stage-2\",\n \"stage-3\",\n \"stage-4\",\n \"stage-9\",\n \"stage-10\",\n \"stage-11\",\n \"stage-12\",\n \"stage-13\",\n \"stage-22\",\n \"stage-23\",\n \"stage-24\",\n \"stage-25\",\n \"stage-26\",\n \"stage-39\",\n \"stage-40\",\n \"stage-41\",\n \"stage-42\",\n \"stage-43\",\n \"stage-60\",\n \"stage-61\",\n \"stage-62\",\n \"stage-63\",\n \"stage-64\"\n ],\n \"appIds\": [],\n \"sm\": 50,\n \"sr\": 100,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": null,\n \"ss\": 50,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": 50,\n \"totalReduceTasks\": null,\n \"totalSparkTasks\": 50,\n \"totalMapSlotDuration\": 7650,\n \"totalReduceSlotDuration\": null,\n \"totalSparkSlotDuration\": 7650,\n \"inputTables\": null,\n \"outputTables\": [],\n \"wi\": \"20190626T032742Z--6535643260096439568\",\n \"wn\": \"workflow_spark_test_20190626T032742Z\",\n \"wt\": \"1561519662000\",\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"jobId\": \"-\",\n \"runName\": \"-\",\n \"runId\": \"-\",\n \"db\": \"-\",\n \"output\": \"-\",\n \"aid\": \"-\",\n \"userType\": \"-\",\n \"cents\": 0,\n \"metrics\": null,\n \"totalProcessingTime\": 0,\n \"memorySeconds\": 65779,\n \"cpuTime\": 0,\n \"storageWaitTime\": 0,\n \"networkSendWaitTime\": 0,\n \"networkReceiveWaitTime\": 0,\n \"elastic\": true,\n \"kind\": \"spark\",\n \"kindLong\": \"Spark\",\n \"name_long\": \"multiple_jobs_stages.py\",\n \"goToApp\": {\n \"appId\": \"20190626T032742Z--6535643260096439568\",\n \"appType\": \"WFI\"\n },\n \"kind_url\": \"spark\",\n \"kind_parent_url\": \"workflow\"\n },\n {\n \"appId\": null,\n \"appType\": null,\n \"gotoId\": null,\n \"gotoLevel\": null,\n \"id\": \"application_1558643494852_0915\",\n \"nick\": \"spark\",\n \"name\": \"basic.py\",\n \"queue\": \"root.users.unravel\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"unravel\",\n \"raw_user\": \"unravel\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"-\"\n ],\n \"aa2Badge\": false,\n \"inBadge\": false,\n \"clusterId\": \"QACDH601A\",\n \"clusterTag\": \"1558643494852\",\n \"start_time\": \"06\/26\/19 10:25:43\",\n \"start_time_long\": \"2019-06-26T10:25:43.626Z\",\n \"duration_long\": 31493,\n \"predDuration_long\": 0,\n \"io_long\": 785,\n \"read_long\": 785,\n \"write_long\": 0,\n \"resource\": 0,\n \"service\": 0,\n \"events\": 3,\n \"numApps\": 0,\n \"numSparkApps\": 18,\n \"numMRJobs\": 18,\n \"numEvents\": 3,\n \"mrJobIds\": [\n \"stage-0\",\n \"stage-1\",\n \"stage-2\",\n \"stage-3\",\n \"stage-4\",\n \"stage-5\",\n \"stage-6\",\n \"stage-8\",\n \"stage-10\",\n \"stage-12\",\n \"stage-14\",\n \"stage-15\",\n \"stage-16\",\n \"stage-17\",\n \"stage-18\",\n \"stage-19\",\n \"stage-20\",\n \"stage-21\"\n ],\n \"appIds\": [],\n \"sm\": 214,\n \"sr\": 100,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": null,\n \"ss\": 214,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": 214,\n \"totalReduceTasks\": null,\n \"totalSparkTasks\": 214,\n \"totalMapSlotDuration\": 10030,\n \"totalReduceSlotDuration\": null,\n \"totalSparkSlotDuration\": 10030,\n \"inputTables\": [\n \"default.people\"\n ],\n \"outputTables\": [],\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"jobId\": \"-\",\n \"runName\": \"-\",\n \"runId\": \"-\",\n \"db\": \"-\",\n \"output\": \"-\",\n \"aid\": \"-\",\n \"userType\": \"-\",\n \"cents\": 0,\n \"metrics\": null,\n \"totalProcessingTime\": 0,\n \"memorySeconds\": 86976,\n \"cpuTime\": 0,\n \"storageWaitTime\": 0,\n \"networkSendWaitTime\": 0,\n \"networkReceiveWaitTime\": 0,\n \"elastic\": true,\n \"kind\": \"spark\",\n \"kindLong\": \"Spark\",\n \"name_long\": \"basic.py\",\n \"kind_url\": \"spark\",\n \"kind_parent_url\": \"app\"\n },\n {\n \"appId\": null,\n \"appType\": null,\n \"gotoId\": null,\n \"gotoLevel\": null,\n \"id\": \"application_1558643494852_0914\",\n \"nick\": \"spark\",\n \"name\": \"Spark Test App\",\n \"queue\": \"root.users.unravel\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"unravel\",\n \"raw_user\": \"unravel\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"-\"\n ],\n \"aa2Badge\": false,\n \"inBadge\": false,\n \"clusterId\": \"QACDH601A\",\n \"clusterTag\": \"1558643494852\",\n \"start_time\": \"06\/26\/19 10:23:45\",\n \"start_time_long\": \"2019-06-26T10:23:45.120Z\",\n \"duration_long\": 18955,\n \"predDuration_long\": 0,\n \"io_long\": 0,\n \"read_long\": 0,\n \"write_long\": 0,\n \"resource\": 0,\n \"service\": 0,\n \"events\": 2,\n \"numApps\": 0,\n \"numSparkApps\": 25,\n \"numMRJobs\": 25,\n \"numEvents\": 2,\n \"mrJobIds\": [\n \"stage-0\",\n \"stage-1\",\n \"stage-2\",\n \"stage-3\",\n \"stage-4\",\n \"stage-9\",\n \"stage-10\",\n \"stage-11\",\n \"stage-12\",\n \"stage-13\",\n \"stage-22\",\n \"stage-23\",\n \"stage-24\",\n \"stage-25\",\n \"stage-26\",\n \"stage-39\",\n \"stage-40\",\n \"stage-41\",\n \"stage-42\",\n \"stage-43\",\n \"stage-60\",\n \"stage-61\",\n \"stage-62\",\n \"stage-63\",\n \"stage-64\"\n ],\n \"appIds\": [],\n \"sm\": 50,\n \"sr\": 100,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": null,\n \"ss\": 50,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": 50,\n \"totalReduceTasks\": null,\n \"totalSparkTasks\": 50,\n \"totalMapSlotDuration\": 7651,\n \"totalReduceSlotDuration\": null,\n \"totalSparkSlotDuration\": 7651,\n \"inputTables\": null,\n \"outputTables\": [],\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"jobId\": \"-\",\n \"runName\": \"-\",\n \"runId\": \"-\",\n \"db\": \"-\",\n \"output\": \"-\",\n \"aid\": \"-\",\n \"userType\": \"-\",\n \"cents\": 0,\n \"metrics\": null,\n \"totalProcessingTime\": 0,\n \"memorySeconds\": 49704,\n \"cpuTime\": 0,\n \"storageWaitTime\": 0,\n \"networkSendWaitTime\": 0,\n \"networkReceiveWaitTime\": 0,\n \"elastic\": true,\n \"kind\": \"spark\",\n \"kindLong\": \"Spark\",\n \"name_long\": \"Spark Test App\",\n \"kind_url\": \"spark\",\n \"kind_parent_url\": \"app\"\n },\n {\n \"appId\": null,\n \"appType\": null,\n \"gotoId\": null,\n \"gotoLevel\": null,\n \"id\": \"application_1558643494852_0913\",\n \"nick\": \"spark\",\n \"name\": \"multiple_jobs_stages.py\",\n \"queue\": \"root.users.unravel\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"unravel\",\n \"raw_user\": \"unravel\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"-\"\n ],\n \"aa2Badge\": false,\n \"inBadge\": false,\n \"clusterId\": \"QACDH601A\",\n \"clusterTag\": \"1558643494852\",\n \"start_time\": \"06\/26\/19 10:21:53\",\n \"start_time_long\": \"2019-06-26T10:21:53.088Z\",\n \"duration_long\": 21233,\n \"predDuration_long\": 0,\n \"io_long\": 0,\n \"read_long\": 0,\n \"write_long\": 0,\n \"resource\": 0,\n \"service\": 0,\n \"events\": 2,\n \"numApps\": 0,\n \"numSparkApps\": 0,\n \"numMRJobs\": 0,\n \"numEvents\": 2,\n \"mrJobIds\": [],\n \"appIds\": [],\n \"sm\": null,\n \"sr\": 100,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": null,\n \"ss\": null,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": null,\n \"totalReduceTasks\": null,\n \"totalSparkTasks\": null,\n \"totalMapSlotDuration\": null,\n \"totalReduceSlotDuration\": null,\n \"totalSparkSlotDuration\": null,\n \"inputTables\": null,\n \"outputTables\": [],\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"jobId\": \"-\",\n \"runName\": \"-\",\n \"runId\": \"-\",\n \"db\": \"-\",\n \"output\": \"-\",\n \"aid\": \"-\",\n \"userType\": \"-\",\n \"cents\": 0,\n \"metrics\": null,\n \"totalProcessingTime\": 0,\n \"memorySeconds\": 63554,\n \"cpuTime\": 0,\n \"storageWaitTime\": 0,\n \"networkSendWaitTime\": 0,\n \"networkReceiveWaitTime\": 0,\n \"elastic\": true,\n \"kind\": \"spark\",\n \"kindLong\": \"Spark\",\n \"name_long\": \"multiple_jobs_stages.py\",\n \"kind_url\": \"spark\",\n \"kind_parent_url\": \"app\"\n },\n {\n \"appId\": null,\n \"appType\": null,\n \"gotoId\": null,\n \"gotoLevel\": null,\n \"id\": \"application_1558643494852_0912\",\n \"nick\": \"spark\",\n \"name\": \"multiple_jobs_stages.py\",\n \"queue\": \"root.users.unravel\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"unravel\",\n \"raw_user\": \"unravel\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"-\"\n ],\n \"aa2Badge\": false,\n \"inBadge\": false,\n \"clusterId\": \"QACDH601A\",\n \"clusterTag\": \"1558643494852\",\n \"start_time\": \"06\/26\/19 10:19:22\",\n \"start_time_long\": \"2019-06-26T10:19:22.725Z\",\n \"duration_long\": 21803,\n \"predDuration_long\": 0,\n \"io_long\": 0,\n \"read_long\": 0,\n \"write_long\": 0,\n \"resource\": 0,\n \"service\": 0,\n \"events\": 2,\n \"numApps\": 0,\n \"numSparkApps\": 0,\n \"numMRJobs\": 0,\n \"numEvents\": 2,\n \"mrJobIds\": [],\n \"appIds\": [],\n \"sm\": null,\n \"sr\": 100,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": null,\n \"ss\": null,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": null,\n \"totalReduceTasks\": null,\n \"totalSparkTasks\": null,\n \"totalMapSlotDuration\": null,\n \"totalReduceSlotDuration\": null,\n \"totalSparkSlotDuration\": null,\n \"inputTables\": null,\n \"outputTables\": [],\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"jobId\": \"-\",\n \"runName\": \"-\",\n \"runId\": \"-\",\n \"db\": \"-\",\n \"output\": \"-\",\n \"aid\": \"-\",\n \"userType\": \"-\",\n \"cents\": 0,\n \"metrics\": null,\n \"totalProcessingTime\": 0,\n \"memorySeconds\": 63723,\n \"cpuTime\": 0,\n \"storageWaitTime\": 0,\n \"networkSendWaitTime\": 0,\n \"networkReceiveWaitTime\": 0,\n \"elastic\": true,\n \"kind\": \"spark\",\n \"kindLong\": \"Spark\",\n \"name_long\": \"multiple_jobs_stages.py\",\n \"kind_url\": \"spark\",\n \"kind_parent_url\": \"app\"\n },\n {\n \"appId\": null,\n \"appType\": null,\n \"gotoId\": null,\n \"gotoLevel\": null,\n \"id\": \"application_1558643494852_0911\",\n \"nick\": \"spark\",\n \"name\": \"multiple_jobs_stages.py\",\n \"queue\": \"root.users.unravel\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"unravel\",\n \"raw_user\": \"unravel\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"-\"\n ],\n \"aa2Badge\": false,\n \"inBadge\": false,\n \"clusterId\": \"QACDH601A\",\n \"clusterTag\": \"1558643494852\",\n \"start_time\": \"06\/26\/19 10:16:03\",\n \"start_time_long\": \"2019-06-26T10:16:03.395Z\",\n \"duration_long\": 16942,\n \"predDuration_long\": 0,\n \"io_long\": 0,\n \"read_long\": 0,\n \"write_long\": 0,\n \"resource\": 0,\n \"service\": 0,\n \"events\": 1,\n \"numApps\": 0,\n \"numSparkApps\": 10,\n \"numMRJobs\": 10,\n \"numEvents\": 1,\n \"mrJobIds\": [\n \"stage-0\",\n \"stage-1\",\n \"stage-3\",\n \"stage-4\",\n \"stage-7\",\n \"stage-8\",\n \"stage-12\",\n \"stage-13\",\n \"stage-18\",\n \"stage-19\"\n ],\n \"appIds\": [],\n \"sm\": 20,\n \"sr\": 100,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": null,\n \"ss\": 20,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": 20,\n \"totalReduceTasks\": null,\n \"totalSparkTasks\": 20,\n \"totalMapSlotDuration\": 4628,\n \"totalReduceSlotDuration\": null,\n \"totalSparkSlotDuration\": 4628,\n \"inputTables\": null,\n \"outputTables\": [],\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"jobId\": \"-\",\n \"runName\": \"-\",\n \"runId\": \"-\",\n \"db\": \"-\",\n \"output\": \"-\",\n \"aid\": \"-\",\n \"userType\": \"-\",\n \"cents\": 0,\n \"metrics\": null,\n \"totalProcessingTime\": 0,\n \"memorySeconds\": 44537,\n \"cpuTime\": 0,\n \"storageWaitTime\": 0,\n \"networkSendWaitTime\": 0,\n \"networkReceiveWaitTime\": 0,\n \"elastic\": true,\n \"kind\": \"spark\",\n \"kindLong\": \"Spark\",\n \"name_long\": \"multiple_jobs_stages.py\",\n \"kind_url\": \"spark\",\n \"kind_parent_url\": \"app\"\n },\n {\n \"appId\": null,\n \"appType\": null,\n \"gotoId\": null,\n \"gotoLevel\": null,\n \"id\": \"application_1558643494852_0910\",\n \"nick\": \"spark\",\n \"name\": \"multiple_jobs_stages.py\",\n \"queue\": \"root.users.unravel\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"unravel\",\n \"raw_user\": \"unravel\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"-\"\n ],\n \"aa2Badge\": false,\n \"inBadge\": false,\n \"clusterId\": \"QACDH601A\",\n \"clusterTag\": \"1558643494852\",\n \"start_time\": \"06\/26\/19 10:12:43\",\n \"start_time_long\": \"2019-06-26T10:12:43.909Z\",\n \"duration_long\": 17429,\n \"predDuration_long\": 0,\n \"io_long\": 0,\n \"read_long\": 0,\n \"write_long\": 0,\n \"resource\": 0,\n \"service\": 0,\n \"events\": 1,\n \"numApps\": 0,\n \"numSparkApps\": 10,\n \"numMRJobs\": 10,\n \"numEvents\": 1,\n \"mrJobIds\": [\n \"stage-0\",\n \"stage-1\",\n \"stage-3\",\n \"stage-4\",\n \"stage-7\",\n \"stage-8\",\n \"stage-12\",\n \"stage-13\",\n \"stage-18\",\n \"stage-19\"\n ],\n \"appIds\": [],\n \"sm\": 20,\n \"sr\": 100,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": null,\n \"ss\": 20,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": 20,\n \"totalReduceTasks\": null,\n \"totalSparkTasks\": 20,\n \"totalMapSlotDuration\": 4770,\n \"totalReduceSlotDuration\": null,\n \"totalSparkSlotDuration\": 4770,\n \"inputTables\": null,\n \"outputTables\": [],\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"jobId\": \"-\",\n \"runName\": \"-\",\n \"runId\": \"-\",\n \"db\": \"-\",\n \"output\": \"-\",\n \"aid\": \"-\",\n \"userType\": \"-\",\n \"cents\": 0,\n \"metrics\": null,\n \"totalProcessingTime\": 0,\n \"memorySeconds\": 45714,\n \"cpuTime\": 0,\n \"storageWaitTime\": 0,\n \"networkSendWaitTime\": 0,\n \"networkReceiveWaitTime\": 0,\n \"elastic\": true,\n \"kind\": \"spark\",\n \"kindLong\": \"Spark\",\n \"name_long\": \"multiple_jobs_stages.py\",\n \"kind_url\": \"spark\",\n \"kind_parent_url\": \"app\"\n },\n {\n \"appId\": null,\n \"appType\": null,\n \"gotoId\": null,\n \"gotoLevel\": null,\n \"id\": \"application_1558643494852_0909\",\n \"nick\": \"spark\",\n \"name\": \"multiple_jobs_stages.py\",\n \"queue\": \"root.users.unravel\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"unravel\",\n \"raw_user\": \"unravel\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"-\"\n ],\n \"aa2Badge\": false,\n \"inBadge\": false,\n \"clusterId\": \"QACDH601A\",\n \"clusterTag\": \"1558643494852\",\n \"start_time\": \"06\/26\/19 10:10:24\",\n \"start_time_long\": \"2019-06-26T10:10:24.015Z\",\n \"duration_long\": 17154,\n \"predDuration_long\": 0,\n \"io_long\": 0,\n \"read_long\": 0,\n \"write_long\": 0,\n \"resource\": 0,\n \"service\": 0,\n \"events\": 1,\n \"numApps\": 0,\n \"numSparkApps\": 10,\n \"numMRJobs\": 10,\n \"numEvents\": 1,\n \"mrJobIds\": [\n \"stage-0\",\n \"stage-1\",\n \"stage-3\",\n \"stage-4\",\n \"stage-7\",\n \"stage-8\",\n \"stage-12\",\n \"stage-13\",\n \"stage-18\",\n \"stage-19\"\n ],\n \"appIds\": [],\n \"sm\": 20,\n \"sr\": 100,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": null,\n \"ss\": 20,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": 20,\n \"totalReduceTasks\": null,\n \"totalSparkTasks\": 20,\n \"totalMapSlotDuration\": 4631,\n \"totalReduceSlotDuration\": null,\n \"totalSparkSlotDuration\": 4631,\n \"inputTables\": null,\n \"outputTables\": [],\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"jobId\": \"-\",\n \"runName\": \"-\",\n \"runId\": \"-\",\n \"db\": \"-\",\n \"output\": \"-\",\n \"aid\": \"-\",\n \"userType\": \"-\",\n \"cents\": 0,\n \"metrics\": null,\n \"totalProcessingTime\": 0,\n \"memorySeconds\": 44999,\n \"cpuTime\": 0,\n \"storageWaitTime\": 0,\n \"networkSendWaitTime\": 0,\n \"networkReceiveWaitTime\": 0,\n \"elastic\": true,\n \"kind\": \"spark\",\n \"kindLong\": \"Spark\",\n \"name_long\": \"multiple_jobs_stages.py\",\n \"kind_url\": \"spark\",\n \"kind_parent_url\": \"app\"\n },\n {\n \"appId\": null,\n \"appType\": null,\n \"gotoId\": null,\n \"gotoLevel\": null,\n \"id\": \"application_1558643494852_0908\",\n \"nick\": \"spark\",\n \"name\": \"multiple_jobs_stages.py\",\n \"queue\": \"root.users.unravel\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"unravel\",\n \"raw_user\": \"unravel\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"-\"\n ],\n \"aa2Badge\": false,\n \"inBadge\": false,\n \"clusterId\": \"QACDH601A\",\n \"clusterTag\": \"1558643494852\",\n \"start_time\": \"06\/26\/19 10:08:03\",\n \"start_time_long\": \"2019-06-26T10:08:03.004Z\",\n \"duration_long\": 19386,\n \"predDuration_long\": 0,\n \"io_long\": 0,\n \"read_long\": 0,\n \"write_long\": 0,\n \"resource\": 0,\n \"service\": 0,\n \"events\": 3,\n \"numApps\": 0,\n \"numSparkApps\": 10,\n \"numMRJobs\": 10,\n \"numEvents\": 3,\n \"mrJobIds\": [\n \"stage-0\",\n \"stage-1\",\n \"stage-3\",\n \"stage-4\",\n \"stage-7\",\n \"stage-8\",\n \"stage-12\",\n \"stage-13\",\n \"stage-18\",\n \"stage-19\"\n ],\n \"appIds\": [],\n \"sm\": 20,\n \"sr\": 100,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": null,\n \"ss\": 20,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": 20,\n \"totalReduceTasks\": null,\n \"totalSparkTasks\": 20,\n \"totalMapSlotDuration\": 5345,\n \"totalReduceSlotDuration\": null,\n \"totalSparkSlotDuration\": 5345,\n \"inputTables\": null,\n \"outputTables\": [],\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"jobId\": \"-\",\n \"runName\": \"-\",\n \"runId\": \"-\",\n \"db\": \"-\",\n \"output\": \"-\",\n \"aid\": \"-\",\n \"userType\": \"-\",\n \"cents\": 0,\n \"metrics\": null,\n \"totalProcessingTime\": 0,\n \"memorySeconds\": 55511,\n \"cpuTime\": 0,\n \"storageWaitTime\": 0,\n \"networkSendWaitTime\": 0,\n \"networkReceiveWaitTime\": 0,\n \"elastic\": true,\n \"kind\": \"spark\",\n \"kindLong\": \"Spark\",\n \"name_long\": \"multiple_jobs_stages.py\",\n \"kind_url\": \"spark\",\n \"kind_parent_url\": \"app\"\n },\n {\n \"appId\": null,\n \"appType\": null,\n \"gotoId\": null,\n \"gotoLevel\": null,\n \"id\": \"application_1558643494852_0907\",\n \"nick\": \"spark\",\n \"name\": \"multiple_jobs_stages.py\",\n \"queue\": \"root.users.unravel\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"unravel\",\n \"raw_user\": \"unravel\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"-\"\n ],\n \"aa2Badge\": false,\n \"inBadge\": false,\n \"clusterId\": \"QACDH601A\",\n \"clusterTag\": \"1558643494852\",\n \"start_time\": \"06\/26\/19 10:05:41\",\n \"start_time_long\": \"2019-06-26T10:05:41.564Z\",\n \"duration_long\": 19017,\n \"predDuration_long\": 0,\n \"io_long\": 0,\n \"read_long\": 0,\n \"write_long\": 0,\n \"resource\": 0,\n \"service\": 0,\n \"events\": 3,\n \"numApps\": 0,\n \"numSparkApps\": 10,\n \"numMRJobs\": 10,\n \"numEvents\": 3,\n \"mrJobIds\": [\n \"stage-0\",\n \"stage-1\",\n \"stage-3\",\n \"stage-4\",\n \"stage-7\",\n \"stage-8\",\n \"stage-12\",\n \"stage-13\",\n \"stage-18\",\n \"stage-19\"\n ],\n \"appIds\": [],\n \"sm\": 20,\n \"sr\": 100,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": null,\n \"ss\": 20,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": 20,\n \"totalReduceTasks\": null,\n \"totalSparkTasks\": 20,\n \"totalMapSlotDuration\": 5137,\n \"totalReduceSlotDuration\": null,\n \"totalSparkSlotDuration\": 5137,\n \"inputTables\": null,\n \"outputTables\": [],\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"jobId\": \"-\",\n \"runName\": \"-\",\n \"runId\": \"-\",\n \"db\": \"-\",\n \"output\": \"-\",\n \"aid\": \"-\",\n \"userType\": \"-\",\n \"cents\": 0,\n \"metrics\": null,\n \"totalProcessingTime\": 0,\n \"memorySeconds\": 53504,\n \"cpuTime\": 0,\n \"storageWaitTime\": 0,\n \"networkSendWaitTime\": 0,\n \"networkReceiveWaitTime\": 0,\n \"elastic\": true,\n \"kind\": \"spark\",\n \"kindLong\": \"Spark\",\n \"name_long\": \"multiple_jobs_stages.py\",\n \"kind_url\": \"spark\",\n \"kind_parent_url\": \"app\"\n },\n {\n \"appId\": null,\n \"appType\": null,\n \"gotoId\": null,\n \"gotoLevel\": null,\n \"id\": \"application_1558643494852_0906\",\n \"nick\": \"spark\",\n \"name\": \"multiple_jobs_stages.py\",\n \"queue\": \"root.users.unravel\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"unravel\",\n \"raw_user\": \"unravel\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"-\"\n ],\n \"aa2Badge\": false,\n \"inBadge\": false,\n \"clusterId\": \"QACDH601A\",\n \"clusterTag\": \"1558643494852\",\n \"start_time\": \"06\/26\/19 10:03:20\",\n \"start_time_long\": \"2019-06-26T10:03:20.433Z\",\n \"duration_long\": 20112,\n \"predDuration_long\": 0,\n \"io_long\": 0,\n \"read_long\": 0,\n \"write_long\": 0,\n \"resource\": 0,\n \"service\": 0,\n \"events\": 1,\n \"numApps\": 0,\n \"numSparkApps\": 25,\n \"numMRJobs\": 25,\n \"numEvents\": 1,\n \"mrJobIds\": [\n \"stage-0\",\n \"stage-1\",\n \"stage-2\",\n \"stage-3\",\n \"stage-4\",\n \"stage-9\",\n \"stage-10\",\n \"stage-11\",\n \"stage-12\",\n \"stage-13\",\n \"stage-22\",\n \"stage-23\",\n \"stage-24\",\n \"stage-25\",\n \"stage-26\",\n \"stage-39\",\n \"stage-40\",\n \"stage-41\",\n \"stage-42\",\n \"stage-43\",\n \"stage-60\",\n \"stage-61\",\n \"stage-62\",\n \"stage-63\",\n \"stage-64\"\n ],\n \"appIds\": [],\n \"sm\": 50,\n \"sr\": 100,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": null,\n \"ss\": 50,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": 50,\n \"totalReduceTasks\": null,\n \"totalSparkTasks\": 50,\n \"totalMapSlotDuration\": 6988,\n \"totalReduceSlotDuration\": null,\n \"totalSparkSlotDuration\": 6988,\n \"inputTables\": null,\n \"outputTables\": [],\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"jobId\": \"-\",\n \"runName\": \"-\",\n \"runId\": \"-\",\n \"db\": \"-\",\n \"output\": \"-\",\n \"aid\": \"-\",\n \"userType\": \"-\",\n \"cents\": 0,\n \"metrics\": null,\n \"totalProcessingTime\": 0,\n \"memorySeconds\": 58648,\n \"cpuTime\": 0,\n \"storageWaitTime\": 0,\n \"networkSendWaitTime\": 0,\n \"networkReceiveWaitTime\": 0,\n \"elastic\": true,\n \"kind\": \"spark\",\n \"kindLong\": \"Spark\",\n \"name_long\": \"multiple_jobs_stages.py\",\n \"kind_url\": \"spark\",\n \"kind_parent_url\": \"app\"\n },\n {\n \"appId\": null,\n \"appType\": null,\n \"gotoId\": null,\n \"gotoLevel\": null,\n \"id\": \"application_1558643494852_0905\",\n \"nick\": \"spark\",\n \"name\": \"multiple_jobs_stages.py\",\n \"queue\": \"root.users.unravel\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"unravel\",\n \"raw_user\": \"unravel\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"-\"\n ],\n \"aa2Badge\": false,\n \"inBadge\": false,\n \"clusterId\": \"QACDH601A\",\n \"clusterTag\": \"1558643494852\",\n \"start_time\": \"06\/26\/19 10:00:55\",\n \"start_time_long\": \"2019-06-26T10:00:55.666Z\",\n \"duration_long\": 20682,\n \"predDuration_long\": 0,\n \"io_long\": 0,\n \"read_long\": 0,\n \"write_long\": 0,\n \"resource\": 0,\n \"service\": 0,\n \"events\": 3,\n \"numApps\": 0,\n \"numSparkApps\": 25,\n \"numMRJobs\": 25,\n \"numEvents\": 3,\n \"mrJobIds\": [\n \"stage-0\",\n \"stage-1\",\n \"stage-2\",\n \"stage-3\",\n \"stage-4\",\n \"stage-9\",\n \"stage-10\",\n \"stage-11\",\n \"stage-12\",\n \"stage-13\",\n \"stage-22\",\n \"stage-23\",\n \"stage-24\",\n \"stage-25\",\n \"stage-26\",\n \"stage-39\",\n \"stage-40\",\n \"stage-41\",\n \"stage-42\",\n \"stage-43\",\n \"stage-60\",\n \"stage-61\",\n \"stage-62\",\n \"stage-63\",\n \"stage-64\"\n ],\n \"appIds\": [],\n \"sm\": 50,\n \"sr\": 100,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": null,\n \"ss\": 50,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": 50,\n \"totalReduceTasks\": null,\n \"totalSparkTasks\": 50,\n \"totalMapSlotDuration\": 7419,\n \"totalReduceSlotDuration\": null,\n \"totalSparkSlotDuration\": 7419,\n \"inputTables\": null,\n \"outputTables\": [],\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"jobId\": \"-\",\n \"runName\": \"-\",\n \"runId\": \"-\",\n \"db\": \"-\",\n \"output\": \"-\",\n \"aid\": \"-\",\n \"userType\": \"-\",\n \"cents\": 0,\n \"metrics\": null,\n \"totalProcessingTime\": 0,\n \"memorySeconds\": 62845,\n \"cpuTime\": 0,\n \"storageWaitTime\": 0,\n \"networkSendWaitTime\": 0,\n \"networkReceiveWaitTime\": 0,\n \"elastic\": true,\n \"kind\": \"spark\",\n \"kindLong\": \"Spark\",\n \"name_long\": \"multiple_jobs_stages.py\",\n \"kind_url\": \"spark\",\n \"kind_parent_url\": \"app\"\n },\n {\n \"appId\": null,\n \"appType\": null,\n \"gotoId\": null,\n \"gotoLevel\": null,\n \"id\": \"application_1558643494852_0904\",\n \"nick\": \"spark\",\n \"name\": \"multiple_jobs_stages.py\",\n \"queue\": \"root.users.unravel\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"unravel\",\n \"raw_user\": \"unravel\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"-\"\n ],\n \"aa2Badge\": false,\n \"inBadge\": false,\n \"clusterId\": \"QACDH601A\",\n \"clusterTag\": \"1558643494852\",\n \"start_time\": \"06\/26\/19 09:58:33\",\n \"start_time_long\": \"2019-06-26T09:58:33.028Z\",\n \"duration_long\": 20967,\n \"predDuration_long\": 0,\n \"io_long\": 0,\n \"read_long\": 0,\n \"write_long\": 0,\n \"resource\": 0,\n \"service\": 0,\n \"events\": 3,\n \"numApps\": 0,\n \"numSparkApps\": 25,\n \"numMRJobs\": 25,\n \"numEvents\": 3,\n \"mrJobIds\": [\n \"stage-0\",\n \"stage-1\",\n \"stage-2\",\n \"stage-3\",\n \"stage-4\",\n \"stage-9\",\n \"stage-10\",\n \"stage-11\",\n \"stage-12\",\n \"stage-13\",\n \"stage-22\",\n \"stage-23\",\n \"stage-24\",\n \"stage-25\",\n \"stage-26\",\n \"stage-39\",\n \"stage-40\",\n \"stage-41\",\n \"stage-42\",\n \"stage-43\",\n \"stage-60\",\n \"stage-61\",\n \"stage-62\",\n \"stage-63\",\n \"stage-64\"\n ],\n \"appIds\": [],\n \"sm\": 50,\n \"sr\": 100,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": null,\n \"ss\": 50,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": 50,\n \"totalReduceTasks\": null,\n \"totalSparkTasks\": 50,\n \"totalMapSlotDuration\": 7688,\n \"totalReduceSlotDuration\": null,\n \"totalSparkSlotDuration\": 7688,\n \"inputTables\": null,\n \"outputTables\": [],\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"jobId\": \"-\",\n \"runName\": \"-\",\n \"runId\": \"-\",\n \"db\": \"-\",\n \"output\": \"-\",\n \"aid\": \"-\",\n \"userType\": \"-\",\n \"cents\": 0,\n \"metrics\": null,\n \"totalProcessingTime\": 0,\n \"memorySeconds\": 63076,\n \"cpuTime\": 0,\n \"storageWaitTime\": 0,\n \"networkSendWaitTime\": 0,\n \"networkReceiveWaitTime\": 0,\n \"elastic\": true,\n \"kind\": \"spark\",\n \"kindLong\": \"Spark\",\n \"name_long\": \"multiple_jobs_stages.py\",\n \"kind_url\": \"spark\",\n \"kind_parent_url\": \"app\"\n },\n {\n \"appId\": null,\n \"appType\": null,\n \"gotoId\": null,\n \"gotoLevel\": null,\n \"id\": \"application_1558643494852_0903\",\n \"nick\": \"spark\",\n \"name\": \"multiple_jobs_stages.py\",\n \"queue\": \"root.users.unravel\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"unravel\",\n \"raw_user\": \"unravel\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"-\"\n ],\n \"aa2Badge\": false,\n \"inBadge\": false,\n \"clusterId\": \"QACDH601A\",\n \"clusterTag\": \"1558643494852\",\n \"start_time\": \"06\/26\/19 09:56:10\",\n \"start_time_long\": \"2019-06-26T09:56:10.706Z\",\n \"duration_long\": 20906,\n \"predDuration_long\": 0,\n \"io_long\": 0,\n \"read_long\": 0,\n \"write_long\": 0,\n \"resource\": 0,\n \"service\": 0,\n \"events\": 2,\n \"numApps\": 0,\n \"numSparkApps\": 0,\n \"numMRJobs\": 0,\n \"numEvents\": 2,\n \"mrJobIds\": [],\n \"appIds\": [],\n \"sm\": null,\n \"sr\": 100,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": null,\n \"ss\": null,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": null,\n \"totalReduceTasks\": null,\n \"totalSparkTasks\": null,\n \"totalMapSlotDuration\": null,\n \"totalReduceSlotDuration\": null,\n \"totalSparkSlotDuration\": null,\n \"inputTables\": null,\n \"outputTables\": [],\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"jobId\": \"-\",\n \"runName\": \"-\",\n \"runId\": \"-\",\n \"db\": \"-\",\n \"output\": \"-\",\n \"aid\": \"-\",\n \"userType\": \"-\",\n \"cents\": 0,\n \"metrics\": null,\n \"totalProcessingTime\": 0,\n \"memorySeconds\": 63967,\n \"cpuTime\": 0,\n \"storageWaitTime\": 0,\n \"networkSendWaitTime\": 0,\n \"networkReceiveWaitTime\": 0,\n \"elastic\": true,\n \"kind\": \"spark\",\n \"kindLong\": \"Spark\",\n \"name_long\": \"multiple_jobs_stages.py\",\n \"kind_url\": \"spark\",\n \"kind_parent_url\": \"app\"\n },\n {\n \"appId\": null,\n \"appType\": null,\n \"gotoId\": null,\n \"gotoLevel\": null,\n \"id\": \"application_1558643494852_0902\",\n \"nick\": \"spark\",\n \"name\": \"multiple_jobs_stages.py\",\n \"queue\": \"root.users.unravel\",\n \"status\": \"S\",\n \"status_long\": \"Success\",\n \"user\": \"unravel\",\n \"raw_user\": \"unravel\",\n \"submitHost\": \"-\",\n \"remarks\": [\n \"-\"\n ],\n \"aa2Badge\": false,\n \"inBadge\": false,\n \"clusterId\": \"QACDH601A\",\n \"clusterTag\": \"1558643494852\",\n \"start_time\": \"06\/26\/19 09:53:46\",\n \"start_time_long\": \"2019-06-26T09:53:46.591Z\",\n \"duration_long\": 19830,\n \"predDuration_long\": 0,\n \"io_long\": 0,\n \"read_long\": 0,\n \"write_long\": 0,\n \"resource\": 0,\n \"service\": 0,\n \"events\": 0,\n \"numApps\": 0,\n \"numSparkApps\": 0,\n \"numMRJobs\": 0,\n \"numEvents\": 0,\n \"mrJobIds\": [],\n \"appIds\": [],\n \"sm\": null,\n \"sr\": 100,\n \"fm\": null,\n \"fr\": null,\n \"km\": null,\n \"kr\": null,\n \"ss\": null,\n \"fs\": null,\n \"ks\": null,\n \"fsu\": null,\n \"ksu\": null,\n \"totalMapTasks\": null,\n \"totalReduceTasks\": null,\n \"totalSparkTasks\": null,\n \"totalMapSlotDuration\": null,\n \"totalReduceSlotDuration\": null,\n \"totalSparkSlotDuration\": null,\n \"inputTables\": null,\n \"outputTables\": [],\n \"wi\": null,\n \"wn\": null,\n \"wt\": null,\n \"type\": \"REGULAR\",\n \"type_regular\": true,\n \"jobId\": \"-\",\n \"runName\": \"-\",\n \"runId\": \"-\",\n \"db\": \"-\",\n \"output\": \"-\",\n \"aid\": \"-\",\n \"userType\": \"-\",\n \"cents\": 0,\n \"metrics\": null,\n \"totalProcessingTime\": 0,\n \"memorySeconds\": 56824,\n \"cpuTime\": 0,\n \"storageWaitTime\": 0,\n \"networkSendWaitTime\": 0,\n \"networkReceiveWaitTime\": 0,\n \"elastic\": true,\n \"kind\": \"spark\",\n \"kindLong\": \"Spark\",\n \"name_long\": \"multiple_jobs_stages.py\",\n \"kind_url\": \"spark\",\n \"kind_parent_url\": \"app\"\n }\n ]\n} " }, 
{ "title" : "\/apps\/status\/finished", 
"url" : "102295-apps-status-finished.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Apps endpoints \/ \/apps\/status\/finished", 
"snippet" : "Gets a time series of finished apps. Request GET http:\/\/ unravel-host :3000\/api\/v1\/apps\/status\/finished Path parameters None. Query parameters None. Response body Name Type Description date string Time stamp. SUCCEEDED integer Number of apps with sucessful run. FAILED integer Number of failed apps. ...", 
"body" : "Gets a time series of finished apps. Request GET http:\/\/ unravel-host :3000\/api\/v1\/apps\/status\/finished Path parameters None. Query parameters None. Response body Name Type Description date string Time stamp. SUCCEEDED integer Number of apps with sucessful run. FAILED integer Number of failed apps. KILLED integer Number of killed apps. Examples Request: curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/apps\/status\/finished\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: \n[\n {\n \"date\": 1561482000000,\n \"SUCCEEDED\": 2\n },\n {\n \"date\": 1561485600000,\n \"SUCCEEDED\": 2\n },\n {\n \"date\": 1561489200000\n },\n {\n \"date\": 1561492800000\n },\n {\n \"date\": 1561496400000\n },\n {\n \"date\": 1561500000000\n },\n {\n \"date\": 1561503600000\n },\n {\n \"date\": 1561507200000\n },\n {\n \"date\": 1561510800000\n },\n {\n \"date\": 1561514400000\n },\n {\n \"date\": 1561518000000\n },\n {\n \"date\": 1561521600000\n },\n {\n \"date\": 1561525200000\n },\n {\n \"date\": 1561528800000\n },\n {\n \"date\": 1561532400000\n },\n {\n \"date\": 1561536000000\n },\n {\n \"date\": 1561539600000,\n \"SUCCEEDED\": 11,\n \"FAILED\": 5\n },\n {\n \"date\": 1561543200000,\n \"SUCCEEDED\": 12\n }\n] " }, 
{ "title" : "\/apps\/status\/running", 
"url" : "102296-apps-status-running.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Apps endpoints \/ \/apps\/status\/running", 
"snippet" : "Gets a list of running apps. Request GET http:\/\/ unravel-host :3000\/api\/v1\/apps\/status\/running Path parameters None. Query parameters None. Response body Name Type Description date string Timestamp in Unix epoch format appsRunning integer Number of apps running appsPending integer Number of apps pen...", 
"body" : "Gets a list of running apps. Request GET http:\/\/ unravel-host :3000\/api\/v1\/apps\/status\/running Path parameters None. Query parameters None. Response body Name Type Description date string Timestamp in Unix epoch format appsRunning integer Number of apps running appsPending integer Number of apps pending Examples Request: curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/apps\/status\/running\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response body: {\n \"date\": 1561619001184,\n \"appsRunning\": 0,\n \"appsPending\": 0\n} " }, 
{ "title" : "AutoAction endpoints", 
"url" : "102297-resource-autoactions.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ AutoAction endpoints", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "\/autoactions", 
"url" : "102298-autoactions.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ AutoAction endpoints \/ \/autoactions", 
"snippet" : "Gets a list of active and inactive auto actions. Path parameters None. Query parameters Required parameters are highlighted . Name Type Description enabled array Filters by active or inactive status. Valid values: true : Show only active auto actions false : Show only inactive auto actions Default: ...", 
"body" : "Gets a list of active and inactive auto actions. Path parameters None. Query parameters Required parameters are highlighted . Name Type Description enabled array Filters by active or inactive status. Valid values: true : Show only active auto actions false : Show only inactive auto actions Default: false . Response body Name Type Description [no name] array JSON structure containing details about each auto action. Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/autoactions?enabled=false\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response body: [\n {\n \"enabled\": false,\n \"policy_name\": \"AutoAction2\",\n \"policy_id\": 10,\n \"instance_id\": \"5346557784540652874\",\n \"name_by_user\": \"Long running YARN application Automation\",\n \"description_by_user\": \"\",\n \"created_by\": \"admin\",\n \"last_edited_by\": \"admin\",\n \"created_at\": 1551125124967,\n \"updated_at\": 1551861204577,\n \"rules\": [\n {\n \"SAME\": [\n {\n \"scope\": \"apps\",\n \"metric\": \"elapsedTime\",\n \"compare\": \">\",\n \"state\": \"*\",\n \"type\": \"mapreduce\",\n \"value\": 10000\n }\n ]\n }\n ],\n \"actions\": [\n {\n \"action\": \"send_email\",\n \"to\": [\n \"ravi@unraveldata.com\"\n ],\n \"to_owner\": false\n },\n {\n \"action\": \"kill_app\"\n }\n ],\n \"cluster_mode\": 0,\n \"cluster_list\": [\n \n ],\n \"cluster_transform\": \"\",\n \"queue_mode\": 0,\n \"queue_list\": [\n \n ],\n \"queue_transform\": \"\",\n \"user_mode\": 2,\n \"user_list\": [\n \"user7\"\n ],\n \"user_transform\": \"\",\n \"app_mode\": 2,\n \"app_list\": [\n \"QuasiMonteCarlo\"\n ],\n \"app_transform\": \"\",\n \"sustain_mode\": 0,\n \"sustain_time\": 0,\n \"time_mode\": 1,\n \"start_hour\": 13,\n \"start_min\": 59,\n \"end_hour\": 13,\n \"end_min\": 59,\n \"dt_zone\": \"America\\\/Los_Angeles\",\n \"broker_mode\": 0,\n \"broker_list\": [\n \n ],\n \"broker_transform\": \"\",\n \"topic_mode\": 0,\n \"topic_list\": [\n \n ],\n \"topic_transform\": \"\"\n },\n {\n \"enabled\": false,\n \"policy_name\": \"AutoAction2\",\n \"policy_id\": 10,\n \"instance_id\": \"7944134729015777055\",\n \"name_by_user\": \"Long running Hive query Automation\",\n \"description_by_user\": \"\",\n \"created_by\": \"admin\",\n \"last_edited_by\": \"admin\",\n \"created_at\": 1551125155192,\n \"updated_at\": 1551787684684,\n \"rules\": [\n {\n \"SAME\": [\n {\n \"scope\": \"apps\",\n \"metric\": \"duration\",\n \"compare\": \">=\",\n \"state\": \"*\",\n \"type\": \"hive\",\n \"value\": 100\n }\n ]\n }\n ],\n \"actions\": [\n {\n \"action\": \"http_post\",\n \"urls\": [\n \"https:\\\/\\\/unraveldata.slack.com\\\/messages\\\/CA2RX1M35\\\/\"\n ]\n }\n ],\n \"cluster_mode\": 0,\n \"cluster_list\": [\n \n ],\n \"cluster_transform\": \"\",\n \"queue_mode\": 0,\n \"queue_list\": [\n \n ],\n \"queue_transform\": \"\",\n \"user_mode\": 0,\n \"user_list\": [\n \n ],\n \"user_transform\": \"\",\n \"app_mode\": 0,\n \"app_list\": [\n \n ],\n \"app_transform\": \"\",\n \"sustain_mode\": 0,\n \"sustain_time\": 0,\n \"time_mode\": 0,\n \"start_hour\": 12,\n \"start_min\": 5,\n \"end_hour\": 12,\n \"end_min\": 5,\n \"dt_zone\": \"America\\\/Los_Angeles\",\n \"broker_mode\": 0,\n \"broker_list\": [\n \n ],\n \"broker_transform\": \"\",\n \"topic_mode\": 0,\n \"topic_list\": [\n \n ],\n \"topic_transform\": \"\"\n },\n {\n \"enabled\": false,\n \"policy_name\": \"AutoAction2\",\n \"policy_id\": 10,\n \"instance_id\": \"2556543518905386312\",\n \"name_by_user\": \"Resource contention in cluster allocated memory Automation\",\n \"description_by_user\": \"\",\n \"created_by\": \"admin\",\n \"last_edited_by\": \"admin\",\n \"created_at\": 1551125186605,\n \"updated_at\": 1551787687869,\n \"rules\": [\n {\n \"SAME\": [\n {\n \"scope\": \"clusters\",\n \"metric\": \"allocatedMB\",\n \"compare\": \">\",\n \"value\": 1024\n },\n {\n \"scope\": \"clusters\",\n \"metric\": \"appCount\",\n \"compare\": \">\",\n \"state\": \"*\",\n \"value\": 2\n }\n ]\n }\n ],\n \"actions\": [\n {\n \"action\": \"send_email\",\n \"to\": [\n \"ravi@unraveldata.com\"\n ],\n \"to_owner\": false\n }\n ],\n \"cluster_mode\": 1,\n \"cluster_list\": [\n \n ],\n \"cluster_transform\": \"\",\n \"queue_mode\": 0,\n \"queue_list\": [\n \n ],\n \"queue_transform\": \"\",\n \"user_mode\": 2,\n \"user_list\": [\n \"user1\"\n ],\n \"user_transform\": \"\",\n \"app_mode\": 0,\n \"app_list\": [\n \n ],\n \"app_transform\": \"\",\n \"sustain_mode\": 0,\n \"sustain_time\": 0,\n \"time_mode\": 0,\n \"start_hour\": 12,\n \"start_min\": 6,\n \"end_hour\": 12,\n \"end_min\": 6,\n \"dt_zone\": \"America\\\/Los_Angeles\",\n \"broker_mode\": 0,\n \"broker_list\": [\n \n ],\n \"broker_transform\": \"\",\n \"topic_mode\": 0,\n \"topic_list\": [\n \n ],\n \"topic_transform\": \"\"\n },\n {\n \"enabled\": false,\n \"policy_name\": \"AutoAction2\",\n \"policy_id\": 10,\n \"instance_id\": \"8761136669870544897\",\n \"name_by_user\": \"Resource contention in cluster allocated vcores Automation\",\n \"description_by_user\": \"\",\n \"created_by\": \"admin\",\n \"last_edited_by\": \"admin\",\n \"created_at\": 1551125217281,\n \"updated_at\": 1551787691083,\n \"rules\": [\n {\n \"SAME\": [\n {\n \"scope\": \"clusters\",\n \"metric\": \"allocatedVCores\",\n \"compare\": \">=\",\n \"value\": 2\n },\n {\n \"scope\": \"clusters\",\n \"metric\": \"appCount\",\n \"compare\": \">=\",\n \"state\": \"*\",\n \"value\": 2\n }\n ]\n }\n ],\n \"actions\": [\n {\n \"action\": \"send_email\",\n \"to\": [\n \"ravi@unraveldata.com\"\n ],\n \"to_owner\": false\n }\n ],\n \"cluster_mode\": 1,\n \"cluster_list\": [\n \n ],\n \"cluster_transform\": \"\",\n \"queue_mode\": 0,\n \"queue_list\": [\n \n ],\n \"queue_transform\": \"\",\n \"user_mode\": 2,\n \"user_list\": [\n \"user2\"\n ],\n \"user_transform\": \"\",\n \"app_mode\": 0,\n \"app_list\": [\n \n ],\n \"app_transform\": \"\",\n \"sustain_mode\": 0,\n \"sustain_time\": 0,\n \"time_mode\": 0,\n \"start_hour\": 12,\n \"start_min\": 6,\n \"end_hour\": 12,\n \"end_min\": 6,\n \"dt_zone\": \"America\\\/Los_Angeles\",\n \"broker_mode\": 0,\n \"broker_list\": [\n \n ],\n \"broker_transform\": \"\",\n \"topic_mode\": 0,\n \"topic_list\": [\n \n ],\n \"topic_transform\": \"\"\n }\n] " }, 
{ "title" : "Request", 
"url" : "102298-autoactions.html#UUID-8d8cb5e9-22da-e8a9-2c88-8b92288c57bc_UUID-e4f12fc5-163b-93bf-2247-ee894881a017", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ AutoAction endpoints \/ \/autoactions \/ Request", 
"snippet" : "GET http:\/\/ unravel-host :3000\/api\/v1\/autoactions?enabled={status}...", 
"body" : "GET http:\/\/ unravel-host :3000\/api\/v1\/autoactions?enabled={status} " }, 
{ "title" : "\/autoactions\/metrics", 
"url" : "102299-autoactions-metrics.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ AutoAction endpoints \/ \/autoactions\/metrics", 
"snippet" : "Gets a list of auto action metrics. Path parameters None. Query parameters None. Response body Name Type Description [no name] array JSON structure containing a list of metrics. Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/autoactions\/metrics\" -H \"accept: application\/json\" -H \"Auth...", 
"body" : "Gets a list of auto action metrics. Path parameters None. Query parameters None. Response body Name Type Description [no name] array JSON structure containing a list of metrics. Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/autoactions\/metrics\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response body: [\n \"appCount\",\n \"elapsedTime\",\n \"allocatedMB\",\n \"allocatedVCores\",\n \"runningContainers\",\n \"memorySeconds\",\n \"vcoreSeconds\",\n \"jobCount\",\n \"elapsedAppTime\",\n \"mapsTotal\",\n \"mapsCompleted\",\n \"reducesTotal\",\n \"reducesCompleted\",\n \"mapsPending\",\n \"mapsRunning\",\n \"reducesPending\",\n \"reducesRunning\",\n \"newReduceAttempts\",\n \"runningReduceAttempts\",\n \"failedReduceAttempts\",\n \"killedReduceAttempts\",\n \"successfulReduceAttempts\",\n \"newMapAttempts\",\n \"runningMapAttempts\",\n \"failedMapAttempts\",\n \"killedMapAttempts\",\n \"successfulMapAttempts\",\n \"badId\",\n \"connection\",\n \"ioError\",\n \"wrongLength\",\n \"wrongMap\",\n \"wrongReduce\",\n \"fileBytesRead\",\n \"fileBytesWritten\",\n \"fileReadOps\",\n \"fileLargeReadOps\",\n \"fileWriteOps\",\n \"hdfsBytesRead\",\n \"hdfsBytesWritten\",\n \"hdfsReadOps\",\n \"hdfsLargeReadOps\",\n \"hdfsWriteOps\",\n \"mapInputRecords\",\n \"mapOutputRecords\",\n \"mapOutputBytes\",\n \"mapOutputMaterializedBytes\",\n \"splitRawBytes\",\n \"combineInputRecords\",\n \"combineOutputRecords\",\n \"reduceInputGroups\",\n \"reduceShuffleBytes\",\n \"reduceInputRecords\",\n \"reduceOutputRecords\",\n \"spilledRecords\",\n \"shuffledMaps\",\n \"failedShuffle\",\n \"mergedMapOutputs\",\n \"gcTimeMillis\",\n \"cpuMilliseconds\",\n \"physicalMemoryBytes\",\n \"virtualMemoryBytes\",\n \"committedHeapBytes\",\n \"totalLaunchedMaps\",\n \"totalLaunchedReduces\",\n \"dataLocalMaps\",\n \"slotsMillisMaps\",\n \"slotsMillisReduces\",\n \"millisMaps\",\n \"millisReduces\",\n \"vcoresMillisMaps\",\n \"vcoresMillisReduces\",\n \"mbMillisMaps\",\n \"mbMillisReduces\",\n \"bytesRead\",\n \"bytesWritten\",\n \"duration\",\n \"totalDfsBytesRead\",\n \"totalDfsBytesWritten\",\n \"inputRecords\",\n \"outputRecords\",\n \"outputToInputRecordsRatio\",\n \"totalJoinInputRowCount\",\n \"totalJoinOutputRowCount\",\n \"inputPartitions\",\n \"outputPartitions\",\n \"joinInputRowCount\",\n \"joinOutputRowCount\",\n \"joinOutputToInputRowRatio\"\n] " }, 
{ "title" : "Request", 
"url" : "102299-autoactions-metrics.html#UUID-720b866c-036c-1760-1fa4-f3c912231c70_UUID-e4f12fc5-163b-93bf-2247-ee894881a017", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ AutoAction endpoints \/ \/autoactions\/metrics \/ Request", 
"snippet" : "GET http:\/\/ unravel-host :3000\/api\/v1\/autoactions\/metrics...", 
"body" : "GET http:\/\/ unravel-host :3000\/api\/v1\/autoactions\/metrics " }, 
{ "title" : "\/autoactions\/recent_violations", 
"url" : "102300-autoactions-recent-violations.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ AutoAction endpoints \/ \/autoactions\/recent_violations", 
"snippet" : "Gets a list of recent auto action violations. Path parameters None. Query parameters Required parameters are highlighted . Name Type Description start_time string Starting date of date range. Format YYYY-MM-DD . end_time string Ending date of date range. Format YYYY-MM-DD . limit integer Maximum num...", 
"body" : "Gets a list of recent auto action violations. Path parameters None. Query parameters Required parameters are highlighted . Name Type Description start_time string Starting date of date range. Format YYYY-MM-DD . end_time string Ending date of date range. Format YYYY-MM-DD . limit integer Maximum number of violations to return. Response body Name Type Description entityType integer Application type: 1: Hive 2: MapReduce 3: Spark clusterName string Name of the cluster on which the auto action occurred. appId string Application ID eventTime integer Time of the violation in Unix epoch format. eventName string Event name. entityId string Entity ID. user string User name. queue string Queue name. Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/autoactions\/recent_violations?start_time=2019-09-10&end_time=2019-09-12&limit=50\" \" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response body: [\n {\n \"_index\": \"ev-20190210_16\",\n \"_type\": \"event\",\n \"_id\": \"4555334246238766772\",\n \"_score\": null,\n \"_source\": {\n \"eventName\": \"Resource contention in queue allocated vcores\",\n \"eventType\": \"AV\",\n \"eventNumber\": 80002,\n \"eventTime\": 1550138634145,\n \"entityGroup\": 0,\n \"entityType\": 0,\n \"entityId\": \"2655444101301604058\",\n \"user\": \"N\/A\",\n \"queue\": \"N\/A\",\n \"clusterName\": \"N\/A\",\n \"staticRank\": 80,\n \"dynamicRank\": -1,\n \"appId\": \"http:\/\/fargo22.unraveldata.com:3000\/#\/app\/operations\/charts\/resources?from=1550138334145&to=1550138934145&at=1550138634145&interval=1m\"\n },\n \"sort\": [\n 1550138634145\n ]\n },\n {\n \"_index\": \"ev-20190210_16\",\n \"_type\": \"event\",\n \"_id\": \"7697129336747736945\",\n \"_score\": null,\n \"_source\": {\n \"eventName\": \"Resource contention in queue allocated vcores\",\n \"eventType\": \"AV\",\n \"eventNumber\": 80002,\n \"eventTime\": 1550138177941,\n \"entityGroup\": 0,\n \"entityType\": 0,\n \"entityId\": \"2655444101301604058\",\n \"user\": \"N\/A\",\n \"queue\": \"N\/A\",\n \"clusterName\": \"N\/A\",\n \"staticRank\": 80,\n \"dynamicRank\": -1,\n \"appId\": \"http:\/\/fargo22.unraveldata.com:3000\/#\/app\/operations\/charts\/resources?from=1550137877941&to=1550138477941&at=1550138177941&interval=1m\"\n },\n \"sort\": [\n 1550138177941\n ]\n },\n {\n \"_index\": \"ev-20190210_16\",\n \"_type\": \"event\",\n \"_id\": \"2534921656421818156\",\n \"_score\": null,\n \"_source\": {\n \"eventName\": \"Resource contention in queue allocated vcores\",\n \"eventType\": \"AV\",\n \"eventNumber\": 80002,\n \"eventTime\": 1550138082308,\n \"entityGroup\": 0,\n \"entityType\": 0,\n \"entityId\": \"2655444101301604058\",\n \"user\": \"N\/A\",\n \"queue\": \"N\/A\",\n \"clusterName\": \"N\/A\",\n \"staticRank\": 80,\n \"dynamicRank\": -1,\n \"appId\": \"http:\/\/fargo22.unraveldata.com:3000\/#\/app\/operations\/charts\/resources?from=1550137782308&to=1550138382308&at=1550138082308&interval=1m\"\n },\n \"sort\": [\n 1550138082308\n ]\n }\n] " }, 
{ "title" : "Request", 
"url" : "102300-autoactions-recent-violations.html#UUID-9f309483-16d6-bea9-8567-e3ceb040d1ed_UUID-e4f12fc5-163b-93bf-2247-ee894881a017", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ AutoAction endpoints \/ \/autoactions\/recent_violations \/ Request", 
"snippet" : "GET http:\/\/ unravel-host :3000\/api\/v1\/autoactions\/recent_violations?start_time={timestamp}&end_time={timestamp}&limit={integer}\"...", 
"body" : "GET http:\/\/ unravel-host :3000\/api\/v1\/autoactions\/recent_violations?start_time={timestamp}&end_time={timestamp}&limit={integer}\" " }, 
{ "title" : "Celery endpoints", 
"url" : "102301-resource-celery.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Celery endpoints", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "\/celery\/analyze-queue-data", 
"url" : "102302-celery-analyze-queue-data.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Celery endpoints \/ \/celery\/analyze-queue-data", 
"snippet" : "Generates a Queue Analysis report. Path parameters None. Query parameters Required parameters are highlighted . Name Type Description start_time string Start date. Format YYYY-MM-DD end_time string End date. Format YYYY-MM-DD Response body Name Type Description [no name] array JSON structure contain...", 
"body" : "Generates a Queue Analysis report. Path parameters None. Query parameters Required parameters are highlighted . Name Type Description start_time string Start date. Format YYYY-MM-DD end_time string End date. Format YYYY-MM-DD Response body Name Type Description [no name] array JSON structure containing a Queue Analysis report. Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/v1\/celery\/analyze-queue-data?max_size=1000&version=2\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: " }, 
{ "title" : "Request", 
"url" : "102302-celery-analyze-queue-data.html#UUID-0375abf6-6c9e-712f-81c5-0b4aace8d9cd_UUID-e4f12fc5-163b-93bf-2247-ee894881a017", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Celery endpoints \/ \/celery\/analyze-queue-data \/ Request", 
"snippet" : "GET http:\/\/ unravel-host :3000\/api\/v1\/celery\/analyze-queue-data?max_size=1000&version=2...", 
"body" : "GET http:\/\/ unravel-host :3000\/api\/v1\/celery\/analyze-queue-data?max_size=1000&version=2 " }, 
{ "title" : "\/celery\/cloud-mappings-grouped-reports", 
"url" : "102303-celery-cloud-mappings-grouped-reports.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Celery endpoints \/ \/celery\/cloud-mappings-grouped-reports", 
"snippet" : "Generates a Cloud Mapping grouped report. Request GET http:\/\/ unravel-host :3000\/api\/v1\/celery\/cloud-mappings-grouped-reports Path parameters None. Query parameters Required parameters are highlighted . Name Type Description from string instance_type string Response body Name Type Description task_i...", 
"body" : "Generates a Cloud Mapping grouped report. Request GET http:\/\/ unravel-host :3000\/api\/v1\/celery\/cloud-mappings-grouped-reports Path parameters None. Query parameters Required parameters are highlighted . Name Type Description from string instance_type string Response body Name Type Description task_id integer Task ID Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/cloud-mappings-grouped-reports\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response body: {\n \"task_id\": \"c097620d-b0c5-434c-8a94-76c83fcf28da\"\n} " }, 
{ "title" : "\/celery\/cloud-mappings-reports", 
"url" : "102304-celery-cloud-mappings-reports.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Celery endpoints \/ \/celery\/cloud-mappings-reports", 
"snippet" : "Generates a Cloud Mappping report. Request GET http:\/\/ unravel-host :3000\/api\/v1\/celery\/cloud-mappings-reports Path parameters None. Query parameters None. Response body Name Type Description [no name] array JSON structure containing the report. Examples Request: curl -X GET \"http:\/\/myserver.com:300...", 
"body" : "Generates a Cloud Mappping report. Request GET http:\/\/ unravel-host :3000\/api\/v1\/celery\/cloud-mappings-reports Path parameters None. Query parameters None. Response body Name Type Description [no name] array JSON structure containing the report. Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/celery\/cloud-mappings-reports\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Request body: " }, 
{ "title" : "\/celery\/cloud_reports\/cloud_providers", 
"url" : "102305-celery-cloud-reports-cloud-providers.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Celery endpoints \/ \/celery\/cloud_reports\/cloud_providers", 
"snippet" : "Generates a report on cloud providers. Request GET http:\/\/ unravel-host :3000\/api\/v1\/celery\/cloud_reports\/cloud_providers Path parameters None. Query parameters None. Response body Name Type Description [no name] array JSON structure containing the report. Examples Request: curl -X GET \"http:\/\/myser...", 
"body" : "Generates a report on cloud providers. Request GET http:\/\/ unravel-host :3000\/api\/v1\/celery\/cloud_reports\/cloud_providers Path parameters None. Query parameters None. Response body Name Type Description [no name] array JSON structure containing the report. Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/celery\/cloud_reports\/cloud_providers\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Request body: " }, 
{ "title" : "\/celery\/cluster-discovery", 
"url" : "102306-celery-cluster-discovery.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Celery endpoints \/ \/celery\/cluster-discovery", 
"snippet" : "Generates a Cluster Discovery report. Request GET http:\/\/ unravel-host :3000\/api\/v1\/celery\/cluster-discovery Path parameters None. Query parameters None. Response body Name Type Description [no name] array JSON structure containing information about the cluster(s). Examples Request: curl -X GET \"htt...", 
"body" : "Generates a Cluster Discovery report. Request GET http:\/\/ unravel-host :3000\/api\/v1\/celery\/cluster-discovery Path parameters None. Query parameters None. Response body Name Type Description [no name] array JSON structure containing information about the cluster(s). Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/celery\/cluster-discovery\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Request body: " }, 
{ "title" : "\/celery\/generate_cluster_app_param_report", 
"url" : "102307-celery-generate-cluster-app-param-report.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Celery endpoints \/ \/celery\/generate_cluster_app_param_report", 
"snippet" : "Generates a cluster app parameter report. Request GET http:\/\/ unravel-host :3000\/api\/v1\/celery\/generate_cluster_app_param_report Path parameters None. Query parameters None. Response body Name Type Description [no name] array JSON structure containing the report. Examples Request: curl -X GET \"http:...", 
"body" : "Generates a cluster app parameter report. Request GET http:\/\/ unravel-host :3000\/api\/v1\/celery\/generate_cluster_app_param_report Path parameters None. Query parameters None. Response body Name Type Description [no name] array JSON structure containing the report. Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/celery\/generate_cluster_app_param_report\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Request body: " }, 
{ "title" : "\/celery\/predict_capacity", 
"url" : "102308-celery-predict-capacity.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Celery endpoints \/ \/celery\/predict_capacity", 
"snippet" : "Generates a Forecasting report. Request GET http:\/\/ unravel-host :3000\/api\/v1\/celery\/predict_capacity Path parameters None. Query parameters None. Response body Name Type Description [no name] array JSON structure containing a Forecasting report. Examples Request: curl -X GET \"http:\/\/myserver.com:30...", 
"body" : "Generates a Forecasting report. Request GET http:\/\/ unravel-host :3000\/api\/v1\/celery\/predict_capacity Path parameters None. Query parameters None. Response body Name Type Description [no name] array JSON structure containing a Forecasting report. Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/celery\/predict_capacity\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Request body: " }, 
{ "title" : "\/celery\/small-files-report", 
"url" : "102309-celery-small-files-report.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Celery endpoints \/ \/celery\/small-files-report", 
"snippet" : "Gets the Small Files report. Request GET http:\/\/ unravel-host :3000\/api\/v1\/celery\/small-files-report?avg_size_threshold={sizethreshold}&num_files_threshold={countthreshold}&top_n_small_files={topx} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description avg...", 
"body" : "Gets the Small Files report. Request GET http:\/\/ unravel-host :3000\/api\/v1\/celery\/small-files-report?avg_size_threshold={sizethreshold}&num_files_threshold={countthreshold}&top_n_small_files={topx} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description avg_size_threshold avg_size_threshold string File size threshold in bytes num_files_threshold num_files_threshold string File count threshold top_n_small_files top_n_small_files string \"Top x\" small files drill_down_subdirs_flag string Need to drill down subdirectories? (true or false) use_avg_file_size_flag string Need to use average file size? (true or false) max_parent_dir_depth string Maximum depth of parent directories. min_parent_dir_depth string Minimum depth of parent directories. Response Body Name Type Description [no name] array JSON structure containing the Small Files report. Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/celery\/small-files-report?avg_size_threshold=456&num_files_threshold=22&top_n_small_files=5\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Request body: {\n \"task_id\": \"831e5966-3233-4400-ab0f-ed8a459859e3\"\n} " }, 
{ "title" : "Chargeback reports", 
"url" : "102310-resource-chargeback.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Chargeback reports", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "\/chargeback\/cb\/app?gte={date}&lte={date}", 
"url" : "102311-chargeback-cb-app.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Chargeback reports \/ \/chargeback\/cb\/app?gte={date}&lte={date}", 
"snippet" : "Gets chargeback reports by application type. Gives the count of all applications in all queues for all users across all clusters. Compatibility This endpoint is available on Unravel 4.5.1.0+. If you're using an older version of Unravel, see . Request GET http:\/\/ unravel-host :3000\/api\/v1\/chargeback\/...", 
"body" : "Gets chargeback reports by application type. Gives the count of all applications in all queues for all users across all clusters. Compatibility This endpoint is available on Unravel 4.5.1.0+. If you're using an older version of Unravel, see . Request GET http:\/\/ unravel-host :3000\/api\/v1\/chargeback\/cb\/app?gte={date}&lte={date} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description gte string Start date. Format: YYYY-MM-DD or Unix epoch value. lte string End date. Format: YYYY-MM-DD or Unix epoch value. Response body The response body contains one array per application type. The table below describes the contents of the array. Name Type Description ms integer Memory usage in seconds. count integer Application count. v1 string Application type ( mr , spark ). vs integer Vcore usage in seconds. Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/chargeback\/cb\/appt?gte=2019-01-02&lte=2019-04-02\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response body: [\n {\n \"type\": \"mr\",\n \"appCount\": 39,\n \"cpuHours\": \"4.14\",\n \"memoryHours\": \"2Y 11M 3D 14h 27m 33s\"\n },\n {\n \"type\": \"tez\",\n \"appCount\": 39,\n \"cpuHours\": \"87.16\",\n \"memoryHours\": \"82Y 8M 26D 9h 14m 12s\"\n },\n {\n \"type\": \"spark\",\n \"appCount\": 29,\n \"cpuHours\": \"2.14\",\n \"memoryHours\": \"5M 13D 5h 9m 26s\"\n }\n] " }, 
{ "title" : "\/chargeback\/cb\/appt\/queue", 
"url" : "102312-chargeback-cb-appt-queue.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Chargeback reports \/ \/chargeback\/cb\/appt\/queue", 
"snippet" : "Gets chargeback reports by queue. Compatibility This endpoint is available on Unravel 4.5.0.9+. If you're using an older version of Unravel, see . Request GET http:\/\/ unravel-host :3000\/api\/v1\/chargeback\/cb\/appt\/queue?gte={date}&lte={date} Path parameters None. Query parameters Required parameters a...", 
"body" : "Gets chargeback reports by queue. Compatibility This endpoint is available on Unravel 4.5.0.9+. If you're using an older version of Unravel, see . Request GET http:\/\/ unravel-host :3000\/api\/v1\/chargeback\/cb\/appt\/queue?gte={date}&lte={date} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description gte string Start date. Format: YYYY-MM-DD or Unix epoch value. lte string End date. Format: YYYY-MM-DD or Unix epoch value. Response body The response body contains one array per application type. The table below describes the contents of the array. Name Type Description count integer Application count. v1 string Application type ( mr , spark ). cb array Array of users or queues (depending on command) containing: ms : Memory usage in milliseconds. count : Application count. v2 : username or queue name. vs : Vcore usage in seconds. Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/chargeback\/cb\/appt\/queue?gte=2019-01-02&lte=2019-04-02\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response body: [\n {\n \"type\": \"mr\",\n \"appCount\": 37,\n \"cpuHours\": \"3.66\",\n \"queue\": \"root.hdfs\",\n \"memoryHours\": \"2Y 8M 8D 16h 44m 45s\"\n },\n {\n \"type\": \"mr\",\n \"appCount\": 2,\n \"cpuHours\": \"0.48\",\n \"queue\": \"root.hbase\",\n \"memoryHours\": \"2M 24D 21h 42m 48s\"\n },\n {\n \"type\": \"tez\",\n \"appCount\": 38,\n \"cpuHours\": \"87.14\",\n \"queue\": \"root.hdfs\",\n \"memoryHours\": \"82Y 8M 18D 13h 30m 38s\"\n },\n {\n \"type\": \"tez\",\n \"appCount\": 1,\n \"cpuHours\": \"0.02\",\n \"queue\": \"root.unravel\",\n \"memoryHours\": \"7D 19h 43m 34s\"\n },\n {\n \"type\": \"spark\",\n \"appCount\": 24,\n \"cpuHours\": \"1.60\",\n \"queue\": \"root.hdfs\",\n \"memoryHours\": \"3M 27D 22h 4m 31s\"\n },\n {\n \"type\": \"spark\",\n \"appCount\": 3,\n \"cpuHours\": \"0.52\",\n \"queue\": \"root.user1\",\n \"memoryHours\": \"1M 14D 10h 47m 48s\"\n },\n {\n \"type\": \"spark\",\n \"appCount\": 2,\n \"cpuHours\": \"0.01\",\n \"queue\": \"root.unravel\",\n \"memoryHours\": \"20h 17m 7s\"\n }\n] " }, 
{ "title" : "\/chargeback\/cb\/appt\/user?gte={date}&lte={date}", 
"url" : "102313-chargeback-cb-appt-user.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Chargeback reports \/ \/chargeback\/cb\/appt\/user?gte={date}&lte={date}", 
"snippet" : "Gets chargeback reports by user. Compatibility This endpoint is available on Unravel 4.5.1.0+. If you're using an older version of Unravel, see . Request GET http:\/\/ unravel-host :3000\/api\/v1\/chargeback\/cb\/appt\/user?gte={date}&lte={date} Path parameters None. Query parameters Required parameters are...", 
"body" : "Gets chargeback reports by user. Compatibility This endpoint is available on Unravel 4.5.1.0+. If you're using an older version of Unravel, see . Request GET http:\/\/ unravel-host :3000\/api\/v1\/chargeback\/cb\/appt\/user?gte={date}&lte={date} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description gte string Start date. Format: YYYY-MM-DD or Unix epoch value. lte string End date. Format: YYYY-MM-DD or Unix epoch value. Response body The response body contains one array per application type. The table below describes the contents of the array. Name Type Description count integer Application count. v1 string Application type ( mr , spark ). cb array Array of users or queues (depending on command) containing: ms : Memory usage in milliseconds. count : Application count. v2 : username or queue name. vs : Vcore usage in seconds. Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/chargeback\/cb\/appt\/user?gte=2019-01-02&lte=2019-04-02\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response body: [\n {\n \"type\": \"mr\",\n \"appCount\": 37,\n \"cpuHours\": \"3.66\",\n \"user\": \"hdfs\",\n \"memoryHours\": \"2Y 8M 8D 16h 44m 45s\"\n },\n {\n \"type\": \"mr\",\n \"appCount\": 2,\n \"cpuHours\": \"0.48\",\n \"user\": \"hbase\",\n \"memoryHours\": \"2M 24D 21h 42m 48s\"\n },\n {\n \"type\": \"tez\",\n \"appCount\": 38,\n \"cpuHours\": \"87.14\",\n \"user\": \"hdfs\",\n \"memoryHours\": \"82Y 8M 18D 13h 30m 38s\"\n },\n {\n \"type\": \"tez\",\n \"appCount\": 1,\n \"cpuHours\": \"0.02\",\n \"user\": \"unravel\",\n \"memoryHours\": \"7D 19h 43m 34s\"\n },\n {\n \"type\": \"spark\",\n \"appCount\": 24,\n \"cpuHours\": \"1.60\",\n \"user\": \"hdfs\",\n \"memoryHours\": \"3M 27D 22h 4m 31s\"\n },\n {\n \"type\": \"spark\",\n \"appCount\": 3,\n \"cpuHours\": \"0.52\",\n \"user\": \"user1\",\n \"memoryHours\": \"1M 14D 10h 47m 48s\"\n },\n {\n \"type\": \"spark\",\n \"appCount\": 2,\n \"cpuHours\": \"0.01\",\n \"user\": \"unravel\",\n \"memoryHours\": \"20h 17m 7s\"\n }\n] " }, 
{ "title" : "Cluster reports", 
"url" : "102314-resource-clusters.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Cluster reports", 
"snippet" : "These endpoints provide detailed data on various ongoing cluster activities, memory and vcore usage by cluster, queue, application, and user, job status, and recent alerts....", 
"body" : "These endpoints provide detailed data on various ongoing cluster activities, memory and vcore usage by cluster, queue, application, and user, job status, and recent alerts. " }, 
{ "title" : "\/clusters", 
"url" : "102315-clusters.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Cluster reports \/ \/clusters", 
"snippet" : "Gets a list of clusters. Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters Path parameters None. Query parameters None. Response body Name Type Description cluster-name string Cluster name. Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/clusters\" -H \"accept: application\/json\" -H...", 
"body" : "Gets a list of clusters. Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters Path parameters None. Query parameters None. Response body Name Type Description cluster-name string Cluster name. Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/clusters\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: [\n \"QACDH601A\"\n] " }, 
{ "title" : "\/clusters\/{cluster_name}\/nodes", 
"url" : "102316-clusters-clustername-nodes.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Cluster reports \/ \/clusters\/{cluster_name}\/nodes", 
"snippet" : "Gets a time series of all the nodes of given cluster grouped by node state. Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters\/{cluster_name}\/nodes?from={timestamp}&to={timestamp} Path parameters Name Description cluster_name Cluster name. Query parameters Required parameters are highlighted . N...", 
"body" : "Gets a time series of all the nodes of given cluster grouped by node state. Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters\/{cluster_name}\/nodes?from={timestamp}&to={timestamp} Path parameters Name Description cluster_name Cluster name. Query parameters Required parameters are highlighted . Name Type Description from string Start date. Format YYYY-MM-DD to string End date. Format YYYY-MM-DD Response body {\n \"total\": [\n {\n \"ts\": 0\n }\n ],\n \"active\": [\n {\n \"ts\": 0\n }\n ],\n \"unhealthy\": [\n {\n \"ts\": 0\n }\n ],\n \"lost\": [\n {\n \"ts\": 0\n }\n ],\n \"rebooted\": [\n {\n \"ts\": 0\n }\n ],\n \"decommissioned\": [\n {\n \"ts\": 0\n }\n ]\n} Examples Request: curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/clusters\/QACDH601A\/nodes?from=2019-03-28&to=2019-06-28\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response body: {\n \"date\": [\n 1561536000000,\n 1561539600000,\n 1561543200000,\n 1561546800000,\n 1561550400000,\n 1561554000000,\n 1561557600000,\n 1561561200000,\n 1561564800000,\n 1561568400000,\n 1561572000000,\n 1561575600000,\n 1561579200000,\n 1561582800000,\n 1561586400000,\n 1561590000000,\n 1561593600000,\n 1561597200000,\n 1561600800000,\n 1561604400000,\n 1561608000000,\n 1561611600000,\n 1561615200000\n ],\n \"total\": {\n \"1561536000000\": 1,\n \"1561539600000\": 1,\n \"1561543200000\": 1,\n \"1561546800000\": 1,\n \"1561550400000\": 1,\n \"1561554000000\": 1,\n \"1561557600000\": 1,\n \"1561561200000\": 1,\n \"1561564800000\": 1,\n \"1561568400000\": 1,\n \"1561572000000\": 1,\n \"1561575600000\": 1,\n \"1561579200000\": 1,\n \"1561582800000\": 1,\n \"1561586400000\": 1,\n \"1561590000000\": 1,\n \"1561593600000\": 1,\n \"1561597200000\": 1,\n \"1561600800000\": 1,\n \"1561604400000\": 1,\n \"1561608000000\": 1,\n \"1561611600000\": 1,\n \"1561615200000\": 1\n },\n \"active\": {\n \"1561536000000\": 1,\n \"1561539600000\": 1,\n \"1561543200000\": 1,\n \"1561546800000\": 1,\n \"1561550400000\": 1,\n \"1561554000000\": 1,\n \"1561557600000\": 1,\n \"1561561200000\": 1,\n \"1561564800000\": 1,\n \"1561568400000\": 1,\n \"1561572000000\": 1,\n \"1561575600000\": 1,\n \"1561579200000\": 1,\n \"1561582800000\": 1,\n \"1561586400000\": 1,\n \"1561590000000\": 1,\n \"1561593600000\": 1,\n \"1561597200000\": 1,\n \"1561600800000\": 1,\n \"1561604400000\": 1,\n \"1561608000000\": 1,\n \"1561611600000\": 1,\n \"1561615200000\": 1\n },\n \"lost\": {\n \"1561536000000\": 0,\n \"1561539600000\": 0,\n \"1561543200000\": 0,\n \"1561546800000\": 0,\n \"1561550400000\": 0,\n \"1561554000000\": 0,\n \"1561557600000\": 0,\n \"1561561200000\": 0,\n \"1561564800000\": 0,\n \"1561568400000\": 0,\n \"1561572000000\": 0,\n \"1561575600000\": 0,\n \"1561579200000\": 0,\n \"1561582800000\": 0,\n \"1561586400000\": 0,\n \"1561590000000\": 0,\n \"1561593600000\": 0,\n \"1561597200000\": 0,\n \"1561600800000\": 0,\n \"1561604400000\": 0,\n \"1561608000000\": 0,\n \"1561611600000\": 0,\n \"1561615200000\": 0\n },\n \"unhealthy\": {\n \"1561536000000\": 0,\n \"1561539600000\": 0,\n \"1561543200000\": 0,\n \"1561546800000\": 0,\n \"1561550400000\": 0,\n \"1561554000000\": 0,\n \"1561557600000\": 0,\n \"1561561200000\": 0,\n \"1561564800000\": 0,\n \"1561568400000\": 0,\n \"1561572000000\": 0,\n \"1561575600000\": 0,\n \"1561579200000\": 0,\n \"1561582800000\": 0,\n \"1561586400000\": 0,\n \"1561590000000\": 0,\n \"1561593600000\": 0,\n \"1561597200000\": 0,\n \"1561600800000\": 0,\n \"1561604400000\": 0,\n \"1561608000000\": 0,\n \"1561611600000\": 0,\n \"1561615200000\": 0\n },\n \"decommissioned\": {\n \"1561536000000\": 0,\n \"1561539600000\": 0,\n \"1561543200000\": 0,\n \"1561546800000\": 0,\n \"1561550400000\": 0,\n \"1561554000000\": 0,\n \"1561557600000\": 0,\n \"1561561200000\": 0,\n \"1561564800000\": 0,\n \"1561568400000\": 0,\n \"1561572000000\": 0,\n \"1561575600000\": 0,\n \"1561579200000\": 0,\n \"1561582800000\": 0,\n \"1561586400000\": 0,\n \"1561590000000\": 0,\n \"1561593600000\": 0,\n \"1561597200000\": 0,\n \"1561600800000\": 0,\n \"1561604400000\": 0,\n \"1561608000000\": 0,\n \"1561611600000\": 0,\n \"1561615200000\": 0\n },\n \"rebooted\": {\n \"1561536000000\": 0,\n \"1561539600000\": 0,\n \"1561543200000\": 0,\n \"1561546800000\": 0,\n \"1561550400000\": 0,\n \"1561554000000\": 0,\n \"1561557600000\": 0,\n \"1561561200000\": 0,\n \"1561564800000\": 0,\n \"1561568400000\": 0,\n \"1561572000000\": 0,\n \"1561575600000\": 0,\n \"1561579200000\": 0,\n \"1561582800000\": 0,\n \"1561586400000\": 0,\n \"1561590000000\": 0,\n \"1561593600000\": 0,\n \"1561597200000\": 0,\n \"1561600800000\": 0,\n \"1561604400000\": 0,\n \"1561608000000\": 0,\n \"1561611600000\": 0,\n \"1561615200000\": 0\n }\n} " }, 
{ "title" : "\/clusters\/{cluster_name}\/resources\/cpu\/allocated", 
"url" : "102317-clusters-clustername-resources-cpu-allocated.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Cluster reports \/ \/clusters\/{cluster_name}\/resources\/cpu\/allocated", 
"snippet" : "Gets a time series for allocated CPU in a given cluster. Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters\/{cluster_name}\/resources\/cpu\/allocated?from={timestamp}&to={timestamp} Name Description cluster_name Cluster name. Query parameters Required parameters are highlighted . Name Type Descript...", 
"body" : "Gets a time series for allocated CPU in a given cluster. Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters\/{cluster_name}\/resources\/cpu\/allocated?from={timestamp}&to={timestamp} Name Description cluster_name Cluster name. Query parameters Required parameters are highlighted . Name Type Description from string Start date. Format YYYY-MM-DD to string End date. Format YYYY-MM-DD Response body The response contains time series of CPU usage in the given cluster. Examples Request: curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/clusters\/QACDH601A\/resources\/cpu\/allocated?from=2019-03-28&to=2019-06-28\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response body: {\n \"1561539600000\": \"2.875\",\n \"1561543200000\": \"1.5\"\n} " }, 
{ "title" : "\/clusters\/{cluster_name}\/resources\/cpu\/total", 
"url" : "102318-clusters-clustername-resources-cpu-total.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Cluster reports \/ \/clusters\/{cluster_name}\/resources\/cpu\/total", 
"snippet" : "Gets a time series for total CPU usage in a given cluster. Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters\/{cluster_name}\/resources\/cpu\/total?from={timestamp}&to={timestamp} Path parameters Name Description cluster_name Cluster name. Query parameters Required parameters are highlighted . Name...", 
"body" : "Gets a time series for total CPU usage in a given cluster. Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters\/{cluster_name}\/resources\/cpu\/total?from={timestamp}&to={timestamp} Path parameters Name Description cluster_name Cluster name. Query parameters Required parameters are highlighted . Name Type Description from string Start date. Format YYYY-MM-DD to string End date. Format YYYY-MM-DD Response body Examples Request: curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/clusters\/QACDH601A\/resources\/cpu\/total?from=2019-03-28&to=2019-06-28\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response body: {\n \"1561536000000\": 24,\n \"1561539600000\": 24,\n \"1561543200000\": 24,\n \"1561546800000\": 24,\n \"1561550400000\": 24,\n \"1561554000000\": 24,\n \"1561557600000\": 24,\n \"1561561200000\": 24,\n \"1561564800000\": 24,\n \"1561568400000\": 24,\n \"1561572000000\": 24,\n \"1561575600000\": 24,\n \"1561579200000\": 24,\n \"1561582800000\": 24,\n \"1561586400000\": 24,\n \"1561590000000\": 24,\n \"1561593600000\": 24,\n \"1561597200000\": 24,\n \"1561600800000\": 24,\n \"1561604400000\": 24,\n \"1561608000000\": 24,\n \"1561611600000\": 24,\n \"1561615200000\": 24\n} " }, 
{ "title" : "\/clusters\/{cluster_name}\/resources\/memory\/allocated", 
"url" : "102319-clusters-clustername-resources-memory-allocated.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Cluster reports \/ \/clusters\/{cluster_name}\/resources\/memory\/allocated", 
"snippet" : "Gets a time series for allocated memory in a given cluster. Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters\/{cluster_name}\/resources\/memory\/allocated?from={timestamp}&to={timestamp} Path parameters Name Description cluster_name Cluster name. Query parameters Required parameters are highlighte...", 
"body" : "Gets a time series for allocated memory in a given cluster. Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters\/{cluster_name}\/resources\/memory\/allocated?from={timestamp}&to={timestamp} Path parameters Name Description cluster_name Cluster name. Query parameters Required parameters are highlighted . Name Type Description from string Start date. Format YYYY-MM-DD to string End date. Format YYYY-MM-DD Response body Examples Request: curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/clusters\/QACDH601A\/resources\/memory\/allocated?from=2019-03-28&to=2019-06-28\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response body: {\n \"1561539600000\": \"4416\",\n \"1561543200000\": \"2304\"\n} " }, 
{ "title" : "\/clusters\/{cluster_name}\/resources\/memory\/total", 
"url" : "102320-clusters-clustername-resources-memory-total.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Cluster reports \/ \/clusters\/{cluster_name}\/resources\/memory\/total", 
"snippet" : "Gets a time series for total memory usage in a given cluster. Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters\/{cluster_name}\/resources\/memory\/total?from={timestamp}&to={timestamp} Path parameters Name Description cluster_name Cluster name. Query parameters Required parameters are highlighted ...", 
"body" : "Gets a time series for total memory usage in a given cluster. Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters\/{cluster_name}\/resources\/memory\/total?from={timestamp}&to={timestamp} Path parameters Name Description cluster_name Cluster name. Query parameters Required parameters are highlighted . Name Type Description from string Start date. Format YYYY-MM-DD to string End date. Format YYYY-MM-DD Response body Examples Request: curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/clusters\/QACDH601A\/resources\/memory\/total?from=2019-03-28&to=2019-06-28\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response body: {\n \"1561536000000\": 14663,\n \"1561539600000\": 14663,\n \"1561543200000\": 14663,\n \"1561546800000\": 14663,\n \"1561550400000\": 14663,\n \"1561554000000\": 14663,\n \"1561557600000\": 14663,\n \"1561561200000\": 14663,\n \"1561564800000\": 14663,\n \"1561568400000\": 14663,\n \"1561572000000\": 14663,\n \"1561575600000\": 14663,\n \"1561579200000\": 14663,\n \"1561582800000\": 14663,\n \"1561586400000\": 14663,\n \"1561590000000\": 14663,\n \"1561593600000\": 14663,\n \"1561597200000\": 14663,\n \"1561600800000\": 14663,\n \"1561604400000\": 14663,\n \"1561608000000\": 14663,\n \"1561611600000\": 14663,\n \"1561615200000\": 14663\n} " }, 
{ "title" : "\/clusters\/nodes", 
"url" : "102321-clusters-nodes.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Cluster reports \/ \/clusters\/nodes", 
"snippet" : "Gets a summary of cluster nodes. Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters\/nodes Path parameters None. Query parameters None. Response body { \"total\": [ { \"ts\": 0 } ], \"active\": [ { \"ts\": 0 } ], \"unhealthy\": [ { \"ts\": 0 } ], \"lost\": [ { \"ts\": 0 } ], \"rebooted\": [ { \"ts\": 0 } ], \"decommi...", 
"body" : "Gets a summary of cluster nodes. Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters\/nodes Path parameters None. Query parameters None. Response body {\n \"total\": [\n {\n \"ts\": 0\n }\n ],\n \"active\": [\n {\n \"ts\": 0\n }\n ],\n \"unhealthy\": [\n {\n \"ts\": 0\n }\n ],\n \"lost\": [\n {\n \"ts\": 0\n }\n ],\n \"rebooted\": [\n {\n \"ts\": 0\n }\n ],\n \"decommissioned\": [\n {\n \"ts\": 0\n }\n ]\n} Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/clusters\/nodes\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: {\n \"date\": [\n 1561611600000,\n 1561615200000\n ],\n \"total\": {\n \"1561611600000\": 1,\n \"1561615200000\": 1\n },\n \"active\": {\n \"1561611600000\": 1,\n \"1561615200000\": 1\n },\n \"lost\": {\n \"1561611600000\": 0,\n \"1561615200000\": 0\n },\n \"unhealthy\": {\n \"1561611600000\": 0,\n \"1561615200000\": 0\n },\n \"decommissioned\": {\n \"1561611600000\": 0,\n \"1561615200000\": 0\n },\n \"rebooted\": {\n \"1561611600000\": 0,\n \"1561615200000\": 0\n }\n} " }, 
{ "title" : "\/clusters\/resources\/cpu\/allocated", 
"url" : "102322-clusters-resources-cpu-allocated.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Cluster reports \/ \/clusters\/resources\/cpu\/allocated", 
"snippet" : "Gets a time series of CPU allocations across all clusters. Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters\/resources\/cpu\/allocated?from={timestamp}&to={timestamp} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description from string Start date. Format ...", 
"body" : "Gets a time series of CPU allocations across all clusters. Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters\/resources\/cpu\/allocated?from={timestamp}&to={timestamp} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description from string Start date. Format YYYY-MM-DD to string End date. Format YYYY-MM-DD Response body Examples Request: curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/clusters\/resources\/cpu\/allocated?from=2019-03-28&to=2019-06-28\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: {\n \"1561539600000\": \"2.875\",\n \"1561543200000\": \"1.5\"\n} " }, 
{ "title" : "\/clusters\/resources\/cpu\/total", 
"url" : "102323-clusters-resources-cpu-total.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Cluster reports \/ \/clusters\/resources\/cpu\/total", 
"snippet" : "Gets total CPU usage across all clusters. Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters\/resources\/cpu\/total?from={timestamp}&to={timestamp} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description from string Start date. Format YYYY-MM-DD to string ...", 
"body" : "Gets total CPU usage across all clusters. Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters\/resources\/cpu\/total?from={timestamp}&to={timestamp} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description from string Start date. Format YYYY-MM-DD to string End date. Format YYYY-MM-DD Response body Examples Request: curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/clusters\/resources\/cpu\/total?from=2019-03-28&to=2019-06-28\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response body: {\n \"1561536000000\": 24,\n \"1561539600000\": 24,\n \"1561543200000\": 24,\n \"1561546800000\": 24,\n \"1561550400000\": 24,\n \"1561554000000\": 24,\n \"1561557600000\": 24,\n \"1561561200000\": 24,\n \"1561564800000\": 24,\n \"1561568400000\": 24,\n \"1561572000000\": 24,\n \"1561575600000\": 24,\n \"1561579200000\": 24,\n \"1561582800000\": 24,\n \"1561586400000\": 24,\n \"1561590000000\": 24,\n \"1561593600000\": 24,\n \"1561597200000\": 24,\n \"1561600800000\": 24,\n \"1561604400000\": 24,\n \"1561608000000\": 24,\n \"1561611600000\": 24,\n \"1561615200000\": 24\n} " }, 
{ "title" : "\/clusters\/resources\/memory\/allocated", 
"url" : "102324-clusters-resources-memory-allocated.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Cluster reports \/ \/clusters\/resources\/memory\/allocated", 
"snippet" : "Gets a time series of memory allocations across all clusters. Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters\/resources\/memory\/allocated?from={timestamp}&to={timestamp} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description from string Start date. F...", 
"body" : "Gets a time series of memory allocations across all clusters. Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters\/resources\/memory\/allocated?from={timestamp}&to={timestamp} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description from string Start date. Format YYYY-MM-DD to string End date. Format YYYY-MM-DD Response body Examples Request: curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/clusters\/resources\/memory\/allocated?from=2019-03-28&to=2019-06-28\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response body: {\n \"1561539600000\": \"4416\",\n \"1561543200000\": \"2304\"\n} " }, 
{ "title" : "\/clusters\/resources\/memory\/total", 
"url" : "102325-clusters-resources-memory-total.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Cluster reports \/ \/clusters\/resources\/memory\/total", 
"snippet" : "Gets total memory usage across all clusters. Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters\/resources\/memory\/total?from={timestamp}&to={timestamp} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description from string Start date. Format YYYY-MM-DD to s...", 
"body" : "Gets total memory usage across all clusters. Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters\/resources\/memory\/total?from={timestamp}&to={timestamp} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description from string Start date. Format YYYY-MM-DD to string End date. Format YYYY-MM-DD Response body Examples Request: curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/clusters\/resources\/memory\/total?from=2019-03-28&to=2019-06-28\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: {\n \"1561536000000\": 14663,\n \"1561539600000\": 14663,\n \"1561543200000\": 14663,\n \"1561546800000\": 14663,\n \"1561550400000\": 14663,\n \"1561554000000\": 14663,\n \"1561557600000\": 14663,\n \"1561561200000\": 14663,\n \"1561564800000\": 14663,\n \"1561568400000\": 14663,\n \"1561572000000\": 14663,\n \"1561575600000\": 14663,\n \"1561579200000\": 14663,\n \"1561582800000\": 14663,\n \"1561586400000\": 14663,\n \"1561590000000\": 14663,\n \"1561593600000\": 14663,\n \"1561597200000\": 14663,\n \"1561600800000\": 14663,\n \"1561604400000\": 14663,\n \"1561608000000\": 14663,\n \"1561611600000\": 14663,\n \"1561615200000\": 14663\n} " }, 
{ "title" : "\/clusters\/resources\/tagged\/{query_type}?applicationType={value}", 
"url" : "102326-clusters-resources-tagged.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Cluster reports \/ \/clusters\/resources\/tagged\/{query_type}?applicationType={value}", 
"snippet" : "\/clusters\/resources\/tagged\/{query_type}?applicationType={value} Gets total job count per status in a given time window (at one hour intervals). Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters\/resources\/tagged\/{query_type}?applicationType={value}&from={timestamp}&to={timestamp}&groupBy={type}&...", 
"body" : "\/clusters\/resources\/tagged\/{query_type}?applicationType={value} Gets total job count per status in a given time window (at one hour intervals). Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters\/resources\/tagged\/{query_type}?applicationType={value}&from={timestamp}&to={timestamp}&groupBy={type}&interval={polling_interval} Path parameters Name Type Description query_type string Query type. Valid values are: cpu : Returns vcores as count memory : Returns memory in bytes Query parameters Required parameters are highlighted . Name Type Description from string Start time in Unix epoch format. to string End time in Unix epoch format. applicationType integer Application type. Valid values are: cascading , impala , hive , mr , pig , spark , tez . interval integer Polling interval. Valid values are: 1m , 5m , 10m , 30m , 1h . Response body {\n epoch-timestamp : count\n} Examples Get vcore usage for MapReduce: curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/clusters\/resources\/tagged\/cpu?to=1536275883&groupBy=%7B%22type%22:%22applicationType%22,%22value%22:%5B%22MAPREDUCE%22%5D%7D&interval=1h&from=1536189483\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Get memory usage for Spark: curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/clusters\/resources\/tagged\/memory?to=1536275883&groupBy=%7B%22type%22:%22applicationType%22,%22value%22:%5B%22SPARK%22%5D%7D&interval=1h&from=1536189483\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" \/clusters\/resources\/tagged\/{query_type}?UserName={list} Returns vcore\/memory usage per user. Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters\/resources\/tagged\/{query_type}?UserName={list}&from={timestamp}&to={timestamp}&groupBy={type}&interval={polling_interval} Path Parameters Name Type Description query_type string Query type. Valid values are: cpu : Returns vcores as count memory : Returns memory in bytes Query Parameters Name Type Description from string Start time in Unix epoch format. to string End time in Unix epoch format. UserName integer List of usernames separated by commas. interval integer Polling interval. Valid values are: 1m , 5m , 10m , 30m , 1h . Response Fields {\n epoch-timestamp : count\n} Examples Get vcore usage for certain user(s): curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/clusters\/resources\/tagged\/cpu?to=1536276842&groupBy=%7B%22type%22:%22user%22,%22value%22:%5B%22root%22%5D%7D&interval=1h&from=1536190442\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Get memory usage for certain user(s): curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/clusters\/resources\/tagged\/memory?to=1536276842&groupBy=%7B%22type%22:%22user%22,%22value%22:%5B%22root%22%5D%7D&interval=1h&from=1536190442\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" \/clusters\/resources\/tagged\/{query_type}?QueueName={list} Returns vcore\/memory usage per queue. Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters\/resources\/tagged\/{query_type}?QueueName={list}&from={timestamp}&to={timestamp}&groupBy={type}&interval={polling_interval} Path Parameters Name Type Description query_type string Query type. Valid values are: cpu : Returns vcores as count memory : Returns memory in bytes Query Parameters Name Type Description from string Start time in Unix epoch format. to string End time in Unix epoch format. QueueName integer List of queue names separated by commas. interval integer Polling interval. Valid values are: 1m , 5m , 10m , 30m , 1h . Response Fields {\n epoch-timestamp : count\n} Examples Get vcore usage for certain queue(s): curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/clusters\/resources\/tagged\/cpu?to=1536277362&groupBy=%7B%22type%22:%22queue%22,%22value%22:%5B%22root.users.root%22%5D%7D&interval=1h&from=1536190962\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Get memory usage for certain queue(s): curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/clusters\/resources\/tagged\/memory?to=1536277362&groupBy=%7B%22type%22:%22queue%22,%22value%22:%5B%22root.users.root%22%5D%7D&interval=1h&from=1536190962\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" " }, 
{ "title" : "\/clusters\/resources\/{resource_type}\/{query_type}?to={time}&from={time}&interval={polling_interval}", 
"url" : "102327-clusters-resources-type-query.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Cluster reports \/ \/clusters\/resources\/{resource_type}\/{query_type}?to={time}&from={time}&interval={polling_interval}", 
"snippet" : "Gets average number of jobs by type. Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters\/resources\/{resource_type}\/{query_type}?to={time}&from={time}&interval={polling_interval} Path parameters Name Type Description resource_type string Resource type. Valid values: cpu : vcore usage in bytes memo...", 
"body" : "Gets average number of jobs by type. Request GET http:\/\/ unravel-host :3000\/api\/v1\/clusters\/resources\/{resource_type}\/{query_type}?to={time}&from={time}&interval={polling_interval} Path parameters Name Type Description resource_type string Resource type. Valid values: cpu : vcore usage in bytes memory : memory usage in bytes query_type string Query type. Valid values: allocated : total Query parameters Required parameters are highlighted . Name Type Description interval interval string Polling interval. Valid values are 1m, 5m, 10m, 30m, |1hk from from string Start time in Unix epoch format. to to string End time in Unix epoch format. Response body {\n epoch-timestamp : count \n} Examples Allocated vcores Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/clusters\/resources\/cpu\/allocated?to=1538666160&interval=30m&from=1536273311\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: {\n \"1535360400000\": \"12\", \n \"1535364000000\": \"3.125\", \n \"1535367600000\": \"5.0303030303\"\n} Total memory Request: curl -X GET \"http:\/\/localhost:3000\/api\/v1\/clusters\/resources\/memory\/total?to=1536273841&interval=1h&from=1536187441\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: {\n \"1535364000000\": 47460,\n \"1535367600000\": 45087,\n \"1535371200000\": 37968\n} " }, 
{ "title" : "Common endpoints", 
"url" : "102328-resource-common.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Common endpoints", 
"snippet" : "Like the Applications page, these endpoints provide detailed information about your applications. You can collect data based on application name, type, user, queues, and tags....", 
"body" : "Like the Applications page, these endpoints provide detailed information about your applications. You can collect data based on application name, type, user, queues, and tags. " }, 
{ "title" : "\/common\/app\/{app_id}\/errors", 
"url" : "102329-common-app-appid-errors.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Common endpoints \/ \/common\/app\/{app_id}\/errors", 
"snippet" : "Gets the errors associated with a given app. Request GET http:\/\/ unravel-host :3000\/api\/v1\/common\/app\/{app_id}\/errors Path parameters Name Description app_id App ID. Query parameters None. Response body Name Type Description integer Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/comm...", 
"body" : "Gets the errors associated with a given app. Request GET http:\/\/ unravel-host :3000\/api\/v1\/common\/app\/{app_id}\/errors Path parameters Name Description app_id App ID. Query parameters None. Response body Name Type Description integer Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/common\/app\/parwinder_20190228180000_db0d2b83-849f-45d2-bd64-e9224e4b4388-u_SOyF\/errors\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response body: No errors found " }, 
{ "title" : "\/common\/app\/{app_id}\/extendedsummary", 
"url" : "102330-common-app-appid-extendedsummary.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Common endpoints \/ \/common\/app\/{app_id}\/extendedsummary", 
"snippet" : "Gets a detailed summary of a given app. Request GET http:\/\/ unravel-host :3000\/api\/v1\/common\/app\/{app_id}\/extendedsummary Path parameters Name Description app_id App ID. Query parameters None. Response body Name Type Description integer Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/...", 
"body" : "Gets a detailed summary of a given app. Request GET http:\/\/ unravel-host :3000\/api\/v1\/common\/app\/{app_id}\/extendedsummary Path parameters Name Description app_id App ID. Query parameters None. Response body Name Type Description integer Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/common\/app\/parwinder_20190228180000_db0d2b83-849f-45d2-bd64-e9224e4b4388-u_SOyF\/extendedsummary\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response body: " }, 
{ "title" : "\/common\/app\/{app_id}\/logs", 
"url" : "102331-common-app-appid-logs.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Common endpoints \/ \/common\/app\/{app_id}\/logs", 
"snippet" : "Gets the logs associated with a given app. Request GET http:\/\/ unravel-host :3000\/api\/v1\/common\/app\/{app_id}\/logs Path parameters Name Description app_id App ID. Query parameters None. Response body Name Type Description integer Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/common\/a...", 
"body" : "Gets the logs associated with a given app. Request GET http:\/\/ unravel-host :3000\/api\/v1\/common\/app\/{app_id}\/logs Path parameters Name Description app_id App ID. Query parameters None. Response body Name Type Description integer Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/common\/app\/parwinder_20190228180000_db0d2b83-849f-u_SOyF\/logs\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response body: No log view found for app hive_20190126160707_1d3f4e51-55eb-4cc7-8a8b-bca05d598920 " }, 
{ "title" : "\/common\/app\/{app_id}\/recommendation", 
"url" : "102332-common-app-appid-recommendation.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Common endpoints \/ \/common\/app\/{app_id}\/recommendation", 
"snippet" : "Gets Unravel's recommendations for the given application. Request GET http:\/\/ unravel-host :3000\/api\/v1\/common\/app\/{app_id}\/recommendation Path parameters Name Description app_id App ID. Query parameters None. Response body Name Type Description parameter string App parameter. current_value string C...", 
"body" : "Gets Unravel's recommendations for the given application. Request GET http:\/\/ unravel-host :3000\/api\/v1\/common\/app\/{app_id}\/recommendation Path parameters Name Description app_id App ID. Query parameters None. Response body Name Type Description parameter string App parameter. current_value string Current value of the app's parameter. recommended string Recommended value for the app's parameter. Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/common\/app\/job_1543784013107_1631\/recommendation\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: [\n {\n \"parameter\": \"mapreduce.map.memory.mb\",\n \"current_value\": \"7596\",\n \"recommended_value\": \"3896\"\n },\n {\n \"parameter\": \"mapreduce.map.java.opts\",\n \"current_value\": \"-Xmx8192m\",\n \"recommended_value\": \"-Xmx3117m\"\n }\n] " }, 
{ "title" : "\/common\/app\/{app_id}\/status", 
"url" : "102333-common-app-appid-status.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Common endpoints \/ \/common\/app\/{app_id}\/status", 
"snippet" : "Gets the status of a given app. Request GET http:\/\/ unravel-host :3000\/api\/v1\/common\/app\/{app_id}\/status Path parameters Name Description app_id App ID. Query parameters None. Response body Name Type Description status string The app's current status. message string The app's status message. Example...", 
"body" : "Gets the status of a given app. Request GET http:\/\/ unravel-host :3000\/api\/v1\/common\/app\/{app_id}\/status Path parameters Name Description app_id App ID. Query parameters None. Response body Name Type Description status string The app's current status. message string The app's status message. Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/common\/app\/parwinder_20190228180000_db0d2b83-849f-45d2-bd64-e9224e4b4388-u_SOyF\/status\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: {\n \"status\": \"Success\",\n \"message\": \"The app status of parwinder_20190228180000_db0d2b83-849f-45d2-bd64-e9224e4b4388-u_SOyF is Success\"\n} " }, 
{ "title" : "\/common\/app\/{app_id}\/summary", 
"url" : "102334-common-app-appid-summary.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Common endpoints \/ \/common\/app\/{app_id}\/summary", 
"snippet" : "Gets a summary of a given app. Request GET http:\/\/ unravel-host :3000\/api\/v1\/common\/app\/{app_id}\/summary Path parameters Name Description app_id App ID. Query parameters None. Response body Name Type Description @class string vcoreSeconds integeger memorySeconds integer cents integer version integer...", 
"body" : "Gets a summary of a given app. Request GET http:\/\/ unravel-host :3000\/api\/v1\/common\/app\/{app_id}\/summary Path parameters Name Description app_id App ID. Query parameters None. Response body Name Type Description @class string vcoreSeconds integeger memorySeconds integer cents integer version integer source string kind string id string App ID. nick string status string user string mrJobIds array List of MapReduce job IDs. duration integer startTime integer numMRJobs integer totalMRJobs integer totalMapTasks integer sm integer km integer kmu integer fm integer fmu integer totalReduceTasks integer sr integer kr integer kru integer fr integer fru integer totalMapSlotDuration integer totalReduceSlotDuration integer totalDfsBytesRead integer totalDfsBytesWritten integer queryString string type string numEvents integer Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/common\/app\/parwinder_20190228180000_db0d2b83-849f-45d2-bd64-e9224e4b4388-u_SOyF\/summary\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: {\n \"@class\": \"com.unraveldata.annotation.HiveQueryAnnotation\",\n \"vcoreSeconds\": 0,\n \"memorySeconds\": 0,\n \"cents\": 0,\n \"version\": 1,\n \"source\": \"post-db\",\n \"kind\": \"hive\",\n \"id\": \"parwinder_20190228180000_db0d2b83-849f-45d2-bd64-e9224e4b4388-u_SOyF\",\n \"nick\": \"Hive Query\",\n \"status\": \"S\",\n \"user\": \"parwinder\",\n \"mrJobIds\": [\n \"job_1550559654567_8334\"\n ],\n \"duration\": 82120,\n \"startTime\": 1551405626487,\n \"numMRJobs\": 0,\n \"totalMRJobs\": 1,\n \"totalMapTasks\": 0,\n \"sm\": 0,\n \"km\": 0,\n \"kmu\": 0,\n \"fm\": 0,\n \"fmu\": 0,\n \"totalReduceTasks\": 0,\n \"sr\": 0,\n \"kr\": 0,\n \"kru\": 0,\n \"fr\": 0,\n \"fru\": 0,\n \"totalMapSlotDuration\": 0,\n \"totalReduceSlotDuration\": 0,\n \"totalDfsBytesRead\": 0,\n \"totalDfsBytesWritten\": 0,\n \"queryString\": \"\\nINSERT OVERWRITE DIRECTORY '\\\/user\\\/benchmark-user\\\/benchmarks\\\/oozie\\\/workflows\\\/road_accident_db\\\/output\\\/'\\nROW FORMAT DELIMITED\\nFIELDS TERMINATED BY '\\\\t'\\nSTORED AS TEXTFILE\\nselect reflect(\\\"java.lang.Thread\\\", \\\"sleep\\\", bigint(60000))\",\n \"type\": \"DML\",\n \"numEvents\": 0\n} " }, 
{ "title" : "Job endpoints", 
"url" : "102335-resource-jobs.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Job endpoints", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "\/jobs", 
"url" : "102336-jobs.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Job endpoints \/ \/jobs", 
"snippet" : "Gets total job count, at one hour intervals. Request GET http:\/\/ unravel-host :3000\/api\/v1\/jobs?from={timestamp}&to={timestamp} Path parameters None. Query parameters Name Type Description from string Start date. Format YYYY-MM-DD to string End date. Format YYYY-MM-DD Response body [ \"string\" ] Exam...", 
"body" : "Gets total job count, at one hour intervals. Request GET http:\/\/ unravel-host :3000\/api\/v1\/jobs?from={timestamp}&to={timestamp} Path parameters None. Query parameters Name Type Description from string Start date. Format YYYY-MM-DD to string End date. Format YYYY-MM-DD Response body [\n \"string\"\n] Examples Request: curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/jobs\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" " }, 
{ "title" : "\/jobs\/bystatus", 
"url" : "102337-jobs-bystatus.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Job endpoints \/ \/jobs\/bystatus", 
"snippet" : "Gets the total job count per job type, at one hour intervals, in a given time window. Request GET http:\/\/ unravel-host :3000\/api\/v1\/jobs\/bystatus?from={timestamp}&to={timestamp} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description from string Start date....", 
"body" : "Gets the total job count per job type, at one hour intervals, in a given time window. Request GET http:\/\/ unravel-host :3000\/api\/v1\/jobs\/bystatus?from={timestamp}&to={timestamp} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description from string Start date. Format YYYY-MM-DD to string End date. Format YYYY-MM-DD Response body [\n \"string\"\n] Examples Request: curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/jobs\/bystatus\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" " }, 
{ "title" : "\/jobs\/bytype", 
"url" : "102338-jobs-bytype.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Job endpoints \/ \/jobs\/bytype", 
"snippet" : "Gets the total job count per job type, at one hour intervals, in a given time window. Request GET http:\/\/ unravel-host :3000\/api\/v1\/jobs\/bytype?from={timestamp}&to={timestamp} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description from string Start date. F...", 
"body" : "Gets the total job count per job type, at one hour intervals, in a given time window. Request GET http:\/\/ unravel-host :3000\/api\/v1\/jobs\/bytype?from={timestamp}&to={timestamp} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description from string Start date. Format YYYY-MM-DD to string End date. Format YYYY-MM-DD Response body [\n \"string\"\n] Examples Request: curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/jobs\/bytype\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" " }, 
{ "title" : "\/jobs\/count", 
"url" : "102339-jobs-count.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Job endpoints \/ \/jobs\/count", 
"snippet" : "Gets the number of jobs in a given timeframe, grouped by state, app type, user, or queue. Request GET http:\/\/ unravel-host :3000\/api\/v1\/jobs\/count?from={time}&to={time}&groupBy={category}&interval={polling_interval} Path parameters None. Query parameters Required parameters are highlighted . Name Ty...", 
"body" : "Gets the number of jobs in a given timeframe, grouped by state, app type, user, or queue. Request GET http:\/\/ unravel-host :3000\/api\/v1\/jobs\/count?from={time}&to={time}&groupBy={category}&interval={polling_interval} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description from string Start time in Unix epoch format. to string End time in Unix epoch format. groupBy string Job category. Valid values are state , applicationType , user , queue . interval string Polling interval. Valid values are 1m , 5m , 10m , 30m , 1h , 1d , 1w . Response body JSON schema when groupBy is state : {\n \"date\": [\n epoch-timestamp\n ],\n \"RUNNING\": {\n epoch-timestamp\n : average\n },\n \"ACCEPTED\": {\n epoch-timestamp\n : average\n }\n} JSON schema when groupBy is applicationType : {\n { \n \"date\" : [ \n \/\/ array of polling epoch-timestamp\n epoch-timestamp \n ],\n\n APP_TYPE : { \n \/\/ APP_TYPE mr | hive | spark | pig | cascading | impala | tez \n \/\/ for each polling interval where the app type is running\n epoch-timestamp : count\n }\n} JSON schema when groupBy is user : {\n \"date\": [\n epoch-timestamp \n ],\n userName: {\n epoch-timestamp : average\n }\n} JSON schema when groupBy is queue : {\n \"date\": [\n epoch-timestamp \n ],\n queue Name: {\n epoch-timestamp : average\n }\n} Examples Request with groupBy set to state : Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/jobs\/count?to=1536275822&groupBy=state&interval=1h&from=1535671022\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: {\n \"date\": [\n 1535626800000\n ],\n \"RUNNING\": {\n \"1535626800000\": \"1.3333333333\"\n },\n \"ACCEPTED\": {\n \"1535695200000\": \"1\"\n }\n} Request with groupBy set to applicationType : curl -X GET \"http:\/\/localhost:3000\/api\/v1\/jobs\/count?to=1536280789&groupBy=applicationType&interval=1h&from=1535675989\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: {\n \"date\": [\n 1535644800000,\n 1535695200000,\n 1535698800000,\n 1535702400000,\n 1535706000000,\n 1535709600000,\n 1535731200000,\n 1535745600000,\n 1535814000000,\n 1535817600000,\n 1535904000000,\n 1535961600000,\n 1535990400000,\n 1536030000000,\n 1536037200000,\n 1536040800000\n ],\n \"MAPREDUCE\": {\n \"1535644800000\": \"1\",\n \"1535695200000\": \"1\",\n \"1535698800000\": \"1\",\n \"1535702400000\": \"1\",\n \"1535706000000\": \"1\",\n \"1535709600000\": \"1\",\n \"1535731200000\": \"1\",\n \"1535745600000\": \"1\",\n \"1535814000000\": \"1\",\n \"1535817600000\": \"1\",\n \"1535904000000\": \"1\",\n \"1535961600000\": \"1.5\",\n \"1535990400000\": \"1\",\n \"1536030000000\": \"1\",\n \"1536037200000\": \"1\",\n \"1536040800000\": \"1.1538461539\"\n },\n \"SPARK\": {\n \"1535698800000\": \"1\"\n }\n} Request with groupBy set to user : curl -X GET \"http:\/\/localhost:3000\/api\/v1\/jobs\/count?to=1536281730&groupBy=user&interval=1h&from=1535676930\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: {\n \"date\": [\n 1535695200000,\n 1535698800000,\n 1535702400000,\n 1535706000000,\n 1535709600000,\n 1535731200000,\n 1535745600000,\n 1535814000000,\n 1535817600000,\n 1535904000000\n ],\n \"root\": {\n \"1535695200000\": \"1\",\n \"1535709600000\": \"1\",\n \"1535731200000\": \"1\",\n \"1535745600000\": \"1\",\n \"1535814000000\": \"1\",\n \"1535817600000\": \"1\",\n \"1535904000000\": \"1\"\n },\n \"hdfs\": {\n \"1535698800000\": \"1\",\n \"1535702400000\": \"1\",\n \"1535706000000\": \"1\"\n }\n} Request with groupBy set to queue : curl -X GET \"http:\/\/localhost:3000\/api\/v1\/jobs\/count?to=1536339111&groupBy=queue&interval=1h&from=1535734311\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: {\n \"date\": [\n 1535695200000,\n 1535698800000,\n 1535702400000,\n 1535706000000,\n 1535709600000,\n 1535731200000,\n 1535745600000,\n 1535814000000,\n 1535817600000,\n 1535904000000,\n 1535961600000,\n 1535990400000,\n 1536030000000,\n 1536037200000,\n 1536040800000,\n 1536044400000,\n 1536048000000,\n 1536051600000,\n 1536055200000,\n 1536058800000,\n 1536076800000\n ],\n \"root.users.root\": {\n \"1535695200000\": \"1\",\n \"1535709600000\": \"1\",\n \"1535731200000\": \"1\",\n \"1535745600000\": \"1\",\n \"1535814000000\": \"1\",\n \"1535817600000\": \"1\",\n \"1535904000000\": \"1\",\n \"1535961600000\": \"1\",\n \"1535990400000\": \"1\",\n \"1536030000000\": \"1\",\n \"1536037200000\": \"1\",\n \"1536040800000\": \"1\",\n \"1536076800000\": \"1\"\n },\n \"root.users.hdfs\": {\n \"1535698800000\": \"1\",\n \"1535702400000\": \"1\",\n \"1535706000000\": \"1\",\n \"1535961600000\": \"2\",\n \"1536040800000\": \"1.1304347826\",\n \"1536044400000\": \"1\",\n \"1536055200000\": \"1.3333333333\",\n \"1536058800000\": \"1\"\n },\n \"root.users.user11\": {\n \"1535698800000\": \"1\"\n },\n \"root.abcdefghijklmnopqrstuvwxyz\": {\n \"1536037200000\": \"1\",\n \"1536040800000\": \"1\",\n \"1536044400000\": \"1\"\n }\n} " }, 
{ "title" : "Kafka endpoints", 
"url" : "102340-resource-kafka.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Kafka endpoints", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "\/kafka\/brokers?cluster_id={cluster_name}&start_time={timestamp}&end_time={timestamp}", 
"url" : "102341-kafka-brokers.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Kafka endpoints \/ \/kafka\/brokers?cluster_id={cluster_name}&start_time={timestamp}&end_time={timestamp}", 
"snippet" : "Gets a list of Kafka brokers active on a given cluster during a given timeframe. Request GET http:\/\/ unravel-host :3000\/api\/v1\/kafka\/brokers?cluster_id={cluster_name}&start_time={timestamp}&end_time={timestamp} Path parameters None. Query parameters Required parameters are highlighted . Name Type De...", 
"body" : "Gets a list of Kafka brokers active on a given cluster during a given timeframe. Request GET http:\/\/ unravel-host :3000\/api\/v1\/kafka\/brokers?cluster_id={cluster_name}&start_time={timestamp}&end_time={timestamp} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description cluster_id string Cluster name. start_time string Start time. Format: YYYY-MM-DDTHH:MM:SS.NNNZ or Unix epoch time. end_time string End time. Format: YYYY-MM-DDTHH:MM:SS.NNNZ or Unix epoch time. Response body The response body contains an array of metrics per broker. Name Type Description broker_name array Array of broker metrics. Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/kafka\/brokers?cluster_id=QAHDP26B&end_time=2019-06-03T16:08:52%2B05:30&start_time=2019-06-03T15:08:52%2B05:30\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: {\"QAHDP26B\": {\n \"ActiveControllerCount-Value\":1,\n \"OfflinePartitionsCount-Value\":43,\n \"BytesInPerSec_newTopic1-OneMinuteRate\":8.893181625e-314,\n \"BytesOutPerSec___consumer_offsets-OneMinuteRate\":5549.999999999995,\n \"MessagesInPerSec___consumer_offsets-OneMinuteRate\":49.99999999999996,\n \"TotalFetchRequestsPerSec___consumer_offsets-OneMinuteRate\":514.6005947658281,\n \"UnderReplicatedPartitions-Value\":0,\n }\n} " }, 
{ "title" : "\/kafka\/{cluster_name}\/kpi?broker={broker_name}?start_time={timestamp}&end_time={timestamp}&interval={minutes}&prefix={metrics}", 
"url" : "102342-kafka-clustername-kpi.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Kafka endpoints \/ \/kafka\/{cluster_name}\/kpi?broker={broker_name}?start_time={timestamp}&end_time={timestamp}&interval={minutes}&prefix={metrics}", 
"snippet" : "Gets KPIs for a given Kafka broker. Request GET http:\/\/ unravel-host :3000\/api\/v1\/kafka\/{cluster_name}\/kpi?broker={broker_name}?start_time={timestamp}&end_time={timestamp}&interval={minutes}&prefix={metrics} Path parameters Name Description cluuster_name Cluster name. Query parameters Required param...", 
"body" : "Gets KPIs for a given Kafka broker. Request GET http:\/\/ unravel-host :3000\/api\/v1\/kafka\/{cluster_name}\/kpi?broker={broker_name}?start_time={timestamp}&end_time={timestamp}&interval={minutes}&prefix={metrics} Path parameters Name Description cluuster_name Cluster name. Query parameters Required parameters are highlighted . Name Type Description broker string Broker name. start_time string Start time. Format: YYYY-MM-DDTHH:MM:SS.NNNZ or Unix epoch time. end_time string End time. Format: YYYY-MM-DDTHH:MM:SS.NNNZ or Unix epoch time. interval string Interval size, in minutes. Valid values are 1m , 5m , 10m , 30m . prefix string List of metrics to graph. Separate each metric with + . Valid values are: BytesInPerSec BytesOutPerSec MessagesInPerSec TotalFetchRequestsPerSec UnderReplicatedPartitions ActiveControllerCount RequestHandlerAvgIdlePercent-OneMinuteRate PartitionCount-Value LeaderCount-Value OfflinePartitionsCount TotalTimeMs_Fetch-99thPercentile RequestsPerSec_Fetch-OneMinuteRate RequestsPerSec_Produce-OneMinuteRate PurgatorySize_Produce-Value PurgatorySize_Fetch-Value Response body The response is one array per broker, with the following KPIs: Name Type Description ts string Timestamp. avg_vd string Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/kafka\/QAHDP26B\/kpi?broker=QAHDP26B.kafka1\n&end_time=2019-06-03T16:56:09%2B05:30&interval=1m&prefix=BytesInPerSec&start_time=2019-06-03T15:56:09%2B05:30\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: {\"QAHDP26B.kafka2\":[{\"ts\":1559559780000,\"avg_vd\":\"504.710000000000\"}]} " }, 
{ "title" : "\/kafka\/{cluster_name}\/kpi_clusters?start_time={timestamp}&end_time={timestamp}&interval={minutes}&prefix={metrics}", 
"url" : "102343-kafka-clustername-kpi-clusters.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Kafka endpoints \/ \/kafka\/{cluster_name}\/kpi_clusters?start_time={timestamp}&end_time={timestamp}&interval={minutes}&prefix={metrics}", 
"snippet" : "Gets KPIs for a given Kafka cluster. Request GET http:\/\/ unravel-host :3000\/api\/v1\/kafka\/{cluster_name}\/kpi_clusters?start_time={timestamp}&end_time={timestamp}&interval={interval}&prefix={metrics} Path parameters Name Description cluster_name Cluster name. Query parameters Required parameters are h...", 
"body" : "Gets KPIs for a given Kafka cluster. Request GET http:\/\/ unravel-host :3000\/api\/v1\/kafka\/{cluster_name}\/kpi_clusters?start_time={timestamp}&end_time={timestamp}&interval={interval}&prefix={metrics} Path parameters Name Description cluster_name Cluster name. Query parameters Required parameters are highlighted . Name Type Description start_time string Start time. Format: YYYY-MM-DDTHH:MM:SS.NNNZ or Unix epoch time. end_time string End time. Format: YYYY-MM-DDTHH:MM:SS.NNNZ or Unix epoch time. interval string Interval size, in minutes. Valid values are 1m , 5m , 10m , 30m . prefix string Metric to graph. Valid values are: BytesInPerSec BytesOutPerSec MessagesInPerSec TotalFetchRequestsPerSec UnderReplicatedPartitions ActiveControllerCount RequestHandlerAvgIdlePercent-OneMinuteRate PartitionCount-Value LeaderCount-Value OfflinePartitionsCount TotalTimeMs_Fetch-99thPercentile RequestsPerSec_Fetch-OneMinuteRate RequestsPerSec_Produce-OneMinuteRate PurgatorySize_Produce-Value PurgatorySize_Fetch-Value Response body The response is one array per cluster, with the following KPIs: Name Type Description Ts string Timestamp. avg_vd string Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/kafka\/QAHDP26B\/kpi_clusters?end_time=2019-06-03T16:56:09%2B05:30&interval=1m&prefix=BytesInPerSec&start_time=2019-06-03T15:56:09%2B05:30\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: { \"QAHDP26B\": [ { \"Ts\":1559557560000, \"avg_vd\":\"0.000000000000\" } ] } " }, 
{ "title" : "\/kafka\/{cluster_name}\/kpi_topics?start_time={timestamp}&end_time={timestamp}&interval={minutes}&prefix={metrics}", 
"url" : "102344-kafka-clustername-kpi-topics.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Kafka endpoints \/ \/kafka\/{cluster_name}\/kpi_topics?start_time={timestamp}&end_time={timestamp}&interval={minutes}&prefix={metrics}", 
"snippet" : "Gets KPIs for the Kafka topics within a given timespan. Request GET http:\/\/ unravel-host :3000\/api\/v1\/kafka\/{cluster_name}\/kpi_topics?start_time={timestamp}&end_time={timestamp}&interval={minutes}&prefix={metrics+topic_OneMinuteRate} Path parameters Name Description cluster_name Cluster name. Query ...", 
"body" : "Gets KPIs for the Kafka topics within a given timespan. Request GET http:\/\/ unravel-host :3000\/api\/v1\/kafka\/{cluster_name}\/kpi_topics?start_time={timestamp}&end_time={timestamp}&interval={minutes}&prefix={metrics+topic_OneMinuteRate} Path parameters Name Description cluster_name Cluster name. Query parameters Required parameters are highlighted . Name Type Description start_time string Start time. Format: YYYY-MM-DDTHH:MM:SS.NNNZ or Unix epoch time. end_time string End time. Format: YYYY-MM-DDTHH:MM:SS.NNNZ or Unix epoch time. interval string Size of interval in minutes . prefix string List of topics names to graph. Separate each metric with + . Response body The response body contains one array per topic, with the following fields. Name Type Description ts string Timestamp. avg_vd string Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/kafka\/QAHDP26B\/kpi_topics?end_time=2019-06-04T10:21:55%2B05:30&interval=1m&prefix=BytesInPerSec___consumer_offsets-OneMinuteRate&start_time=2019-06-04T09:21:55%2B05:30\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: {\"__consumer_offsets\":[{\"ts\":1559620320000,\"avg_vd\":\"4310.999986671694\"}]} " }, 
{ "title" : "\/kafka\/clusters?gte={timestamp}&lt={timestamp}", 
"url" : "102345-kafka-clusters.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Kafka endpoints \/ \/kafka\/clusters?gte={timestamp}&lt={timestamp}", 
"snippet" : "Gets Kafka cluster metrics. Request GET http:\/\/ unravel-host :3000\/api\/v1\/kafka\/clusters?gte={timestamp}&lt={timestamp} Path parameters None. Query parameters Name Type Description gte string Start time. Format: YYYY-MM-DDTHH:MM:SS.NNNZ or Unix epoch time. Required . lt string End time. Format: YYYY...", 
"body" : "Gets Kafka cluster metrics. Request GET http:\/\/ unravel-host :3000\/api\/v1\/kafka\/clusters?gte={timestamp}&lt={timestamp} Path parameters None. Query parameters Name Type Description gte string Start time. Format: YYYY-MM-DDTHH:MM:SS.NNNZ or Unix epoch time. Required . lt string End time. Format: YYYY-MM-DDTHH:MM:SS.NNNZ or Unix epoch time. Required . Response body The response body is a JSON structure identified by cluster name containing cluster metrics. Name Type Description ActiveControllerCount-Value integer OfflinePartitionsCount-Value integer BytesInPerSec_newTopic1-OneMinuteRate float BytesOutPerSec___consumer_offsets-OneMinuteRate float MessagesInPerSec___consumer_offsets-OneMinuteRate float TotalFetchRequestsPerSec___consumer_offsets-OneMinuteRate float UnderReplicatedPartitions-Value integer Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/kafka\/clusters?gte=2019-06-02T14:14:18%2B05:30&lt=2019-06-03T14:14:18%2B05:30\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: {\n\"QAHDP26B\": { \n \"ActiveControllerCount-Value\":1,\n \"OfflinePartitionsCount-Value\":43,\n \"BytesInPerSec_newTopic1-OneMinuteRate\":8.893181625e-314,\n \"BytesOutPerSec___consumer_offsets-OneMinuteRate\":5549.999999999995,\n \"MessagesInPerSec___consumer_offsets-OneMinuteRate\":49.99999999999996,\n \"TotalFetchRequestsPerSec___consumer_offsets-OneMinuteRate\":514.6005947658281,\n \"UnderReplicatedPartitions-Value\":0,\n }\n} " }, 
{ "title" : "\/kafka\/topics\/partions\/ts?cluster_id={cluster_name}?topic_id={topic_name}&start_time={timestamp}&end_time={timestamp}&interval={minutes}&metric_pattern={metric}", 
"url" : "102346-kafka-topics-partitions-ts.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Kafka endpoints \/ \/kafka\/topics\/partions\/ts?cluster_id={cluster_name}?topic_id={topic_name}&start_time={timestamp}&end_time={timestamp}&interval={minutes}&metric_pattern={metric}", 
"snippet" : "Gets details about a given partition. Request GET http:\/\/ unravel-host :3000\/api\/v1\/kafka\/topics\/partions\/ts?cluster_id={cluster_name}?topic_id={topic_name}&start_time={timestamp}&end_time={timestamp}&interval={minutes}&metric_pattern={metric} Path parameters None. Query parameters Name Type Descrip...", 
"body" : "Gets details about a given partition. Request GET http:\/\/ unravel-host :3000\/api\/v1\/kafka\/topics\/partions\/ts?cluster_id={cluster_name}?topic_id={topic_name}&start_time={timestamp}&end_time={timestamp}&interval={minutes}&metric_pattern={metric} Path parameters None. Query parameters Name Type Description cluster_id string Cluster name. Required . topic_id string Topic name. Required . start_time string Start time. Format: YYYY-MM-DDTHH:MM:SS.NNNZ or Unix epoch time. Required . end_time string End time. Format: YYYY-MM-DDTHH:MM:SS.NNNZ or Unix epoch time. Required . interval string Size of interval in minutes . Required . metric_pattern string List of metrics to graph. Separate each metric with + . Required . Valid values are: Offset: LogEndOffset___consumer_offsets_ partition-number -Value Consumer Lag: CgLag_*_topic___consumer_offsets_ partition-number where partition-number corresponds to the partition number (0-any number). Response body The response body contains details about the given partition. Name Type Description key_as_string string Date in UTC time. Key string Date in Unix epoch time. Doc_count string Document count. Avg_vl string Array of key average value pairs. Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/kafka\/topics\/partions\/ts?cluster_id=QAHDP26B&end_time=2019-06-04T12:24:49%2B05:30&interval=1m&metric_pattern=CgLag_*_topic___consumer_offsets_5&start_time=2019-06-04T11:24:49%2B05:30&topic_id=__consumer_offsets\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: {\n\"key_as_string\":\"2019-06-04T05:55:00.000Z\",\n\"Key\":1559627700000,\n\"Doc_count\":3,\n\"Avg_vl\":{\"value\":2016}\n} " }, 
{ "title" : "Report endpoints", 
"url" : "102347-resource-reports.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Report endpoints", 
"snippet" : "These endpoints return key performance indicators (KPIs) for clusters, the small files report , chargeback reports (by user, queue, app), and cluster workload reports ....", 
"body" : "These endpoints return key performance indicators (KPIs) for clusters, the small files report , chargeback reports (by user, queue, app), and cluster workload reports . " }, 
{ "title" : "Cluster reports", 
"url" : "102347-resource-reports.html#UUID-f07aa885-b348-c7fa-6837-62a455605ecf_section-5d04449b79fa5-idm45764277806112", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Report endpoints \/ Cluster reports", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Chargeback reports", 
"url" : "102347-resource-reports.html#UUID-f07aa885-b348-c7fa-6837-62a455605ecf_section-5d04489231e26-idm45764277695264", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Report endpoints \/ Chargeback reports", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "For Unravel versions before 4.5.1.0", 
"url" : "102347-resource-reports.html#UUID-f07aa885-b348-c7fa-6837-62a455605ecf_section-5d044373eb664-idm45764277776960", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Report endpoints \/ Chargeback reports \/ For Unravel versions before 4.5.1.0", 
"snippet" : "MOVED...", 
"body" : "MOVED " }, 
{ "title" : "For Unravel 4.5.1.0 and later", 
"url" : "102347-resource-reports.html#UUID-f07aa885-b348-c7fa-6837-62a455605ecf_section-5d044393b2f69-idm45764277604752", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Report endpoints \/ Chargeback reports \/ For Unravel 4.5.1.0 and later", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "\/reports\/archives\/get_report_archives", 
"url" : "102348-reports-archives-get-report-archives.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Report endpoints \/ \/reports\/archives\/get_report_archives", 
"snippet" : "Under construction....", 
"body" : "Under construction. " }, 
{ "title" : "\/reports\/celeryreports", 
"url" : "102349-reports-celeryreports.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Report endpoints \/ \/reports\/celeryreports", 
"snippet" : "Under construction....", 
"body" : "Under construction. " }, 
{ "title" : "\/reports\/celeryreports\/metadata", 
"url" : "102350-reports-celeryreports-metadata.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Report endpoints \/ \/reports\/celeryreports\/metadata", 
"snippet" : "Under construction....", 
"body" : "Under construction. " }, 
{ "title" : "\/reports\/clusteroptimization\/metadata", 
"url" : "102351-reports-clusteroptimization-metadata.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Report endpoints \/ \/reports\/clusteroptimization\/metadata", 
"snippet" : "Under construction....", 
"body" : "Under construction. " }, 
{ "title" : "\/reports\/data\/diskusage\/capacity_task_details", 
"url" : "102352-reports-data-diskusage-capacity-task-details.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Report endpoints \/ \/reports\/data\/diskusage\/capacity_task_details", 
"snippet" : "Under construction....", 
"body" : "Under construction. " }, 
{ "title" : "\/reports\/data\/diskusage\/get_latest_reports", 
"url" : "102353-reports-data-diskusage-get-latest-reports.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Report endpoints \/ \/reports\/data\/diskusage\/get_latest_reports", 
"snippet" : "\/reports\/data\/diskusage\/get_latest_reports?task_name=small_files_report Gets the latest small file report. Request GET http:\/\/ unravel-host :3000\/api\/v1\/reports\/data\/diskusage\/get_latest_reports?task_name=small_files_report Path Parameters None. Query Parameters Name Type Description task_name strin...", 
"body" : "\/reports\/data\/diskusage\/get_latest_reports?task_name=small_files_report Gets the latest small file report. Request GET http:\/\/ unravel-host :3000\/api\/v1\/reports\/data\/diskusage\/get_latest_reports?task_name=small_files_report Path Parameters None. Query Parameters Name Type Description task_name string Task name. Set this to small_files_report . Response Fields {\n \"date\": date created EPOCH_timestamp,\n \"isSuccess\": report generation sucess\/failure,\n\n\/\/ report parameters\n \"avg_size_threshold\": small file size in bytes,\n \"num_files_threshold\": minimum number of small files,\n \"top_n_small_files\": # of directories to show,\n\n \"report_id\": report name\",\n \"root\": [\n \/\/ array[top_n_small files] directory\n {\n \"MaxFilesize\": maximum size file in directory,\n \"MinFilesize\": minimum size file in directory,\n \"NumFiles\": # of small files in the directory,\n \"DirPath\": directory path,\n \"TotalFilesize\": sum of file size in directory,\n \"AvgFilesize\": average file size\n }\n ]\n} Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/reports\/data\/diskusage\/get_latest_reports?task_name=small_files_report\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: {\n \"date\": 1536899695,\n \"isSuccess\": true,\n \"avg_size_threshold\": 100000,\n \"num_files_threshold\": 100,\n \"top_n_small_files\": 5,\n \"report_id\": \"small_files_1536927781_5674\",\n \"root\": [\n {\n \"MaxFilesize\": \"0\",\n \"MinFilesize\": \"0\",\n \"NumFiles\": \"21523359\",\n \"DirPath\": \"\\\/97ovg\",\n \"TotalFilesize\": \"0\",\n \"AvgFilesize\": \"0\"\n },\n {\n \"MaxFilesize\": \"0\",\n \"MinFilesize\": \"0\",\n \"NumFiles\": \"11302185\",\n \"DirPath\": \"\\\/98esm\",\n \"TotalFilesize\": \"0\",\n \"AvgFilesize\": \"0\"\n },\n {\n \"MaxFilesize\": \"0\",\n \"MinFilesize\": \"0\",\n \"NumFiles\": \"7174452\",\n \"DirPath\": \"\\\/99acx\",\n \"TotalFilesize\": \"0\",\n \"AvgFilesize\": \"0\"\n },\n {\n \"MaxFilesize\": \"0\",\n \"MinFilesize\": \"0\",\n \"NumFiles\": \"7174452\",\n \"DirPath\": \"\\\/97ovg\\\/97zqu\",\n \"TotalFilesize\": \"0\",\n \"AvgFilesize\": \"0\"\n },\n {\n \"MaxFilesize\": \"0\",\n \"MinFilesize\": \"0\",\n \"NumFiles\": \"7174452\",\n \"DirPath\": \"\\\/97ovg\\\/99kgg\",\n \"TotalFilesize\": \"0\",\n \"AvgFilesize\": \"0\"\n }\n ]\n} " }, 
{ "title" : "\/reports\/data\/diskusage\/get_latest_success_reports", 
"url" : "102354-reports-data-diskusage-get-latest-success-reports.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Report endpoints \/ \/reports\/data\/diskusage\/get_latest_success_reports", 
"snippet" : "Under construction....", 
"body" : "Under construction. " }, 
{ "title" : "\/reports\/data\/kpis", 
"url" : "102355-reports-data-kpis.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Report endpoints \/ \/reports\/data\/kpis", 
"snippet" : "Generates a Key Performance Indicator (KPI) report. Request GET http:\/\/ unravel-host :3000\/api\/v1\/reports\/data\/kpis?numDays= num-days Path parameters None. Query parameters Required parameters are highlighted . Name Type Description numDays integer Number of days to include. Response body Name Type ...", 
"body" : "Generates a Key Performance Indicator (KPI) report. Request GET http:\/\/ unravel-host :3000\/api\/v1\/reports\/data\/kpis?numDays= num-days Path parameters None. Query parameters Required parameters are highlighted . Name Type Description numDays integer Number of days to include. Response body Name Type Description st integer Start time in Unix epoch format. et integer End time in Unix epoch format. nlaTb integer Number of tables accessed. nlaPr integer Number of partitions accessed. nlaQr integer Number of queries accessing the table. nlaRi integer Total Read I\/O due to accessing the tables. nlcTb integer Number of tables created. nlcPr integer Number of partitions created. nlcTz integer Size of tables created. nlcPz integer Size of partitions created. ntoTb integer Total number of tables in the system. ntoPr integer Total number of partitions in the system. nhtTb integer nwaTb integer ncoTb integer nhtPr integer nwaPr integer ncoPr integer rp integer rs integer fs integer users array Comma-separated list of users. Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/reports\/data\/kpis?numDays=1\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: [\n {\n \"st\": 1536795705,\n \"et\": 1536882105,\n \"nlaTb\": 15,\n \"nlaPr\": 0,\n \"nlaQr\": 57,\n \"nlaRi\": 101489411821,\n \"nlcTb\": 7,\n \"nlcPr\": 0,\n \"nlcTz\": 304768890,\n \"nlcPz\": 0,\n \"ntoTb\": 378,\n \"ntoPr\": 30061,\n \"nhtTb\": 19,\n \"nwaTb\": 0,\n \"ncoTb\": 359,\n \"nhtPr\": 1823,\n \"nwaPr\": 0,\n \"ncoPr\": 28238,\n \"rp\": 30061,\n \"rs\": 92544293666,\n \"fs\": 92544293666,\n \"users\": [\n \"root\",\n \"hdfs\"\n ]\n }\n] " }, 
{ "title" : "\/reports\/data\/small_file_report_details", 
"url" : "102356-reports-data-small-file-report-details.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Report endpoints \/ \/reports\/data\/small_file_report_details", 
"snippet" : "Gets details about the small file report. Request GET http:\/\/ unravel-host :3000\/api\/v1\/reports\/data\/small_file_report_details Path parameters None. Query parameters None. Response body Name Type Description [no name] array JSON structure containing details about the small file report. Examples Requ...", 
"body" : "Gets details about the small file report. Request GET http:\/\/ unravel-host :3000\/api\/v1\/reports\/data\/small_file_report_details Path parameters None. Query parameters None. Response body Name Type Description [no name] array JSON structure containing details about the small file report. Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/reports\/data\/small_file_report_details\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" " }, 
{ "title" : "\/reports\/files\/reports\/latest", 
"url" : "102357-reports-files-reports-latest.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Report endpoints \/ \/reports\/files\/reports\/latest", 
"snippet" : "Under construction....", 
"body" : "Under construction. " }, 
{ "title" : "\/reports\/operational\/clusterstats?st={timestamp}&et={timestamp}&mode={summary_type}", 
"url" : "102358-reports-operational-clusterstats.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Report endpoints \/ \/reports\/operational\/clusterstats?st={timestamp}&et={timestamp}&mode={summary_type}", 
"snippet" : "Gets a summary of the cluster's users, apps, or queues. Request GET http:\/\/ unravel-host :3000\/api\/v1\/reports\/operational\/clusterstats?st={timestamp}&et={timestamp}&mode={summary_type} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description st string Start ...", 
"body" : "Gets a summary of the cluster's users, apps, or queues. Request GET http:\/\/ unravel-host :3000\/api\/v1\/reports\/operational\/clusterstats?st={timestamp}&et={timestamp}&mode={summary_type} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description st string Start time, in Unix epoch format. et string End time, in Unix epoch format. mode string Summary type. Valid values are user , app , queue . Response body JSON schema when mode is user or queue : {\n \"userStats\": [\n\/\/ array of users | queues currently on cluster\n {\n \"root\": {\n \"running\": {\n \"min\": minimum applications running,,\n \"max\": minimum applications running,\n \"mean\": average appication number running,\n \"stddev\": standard deviation\n },\n \"memory\": { \/\/ see running above },\n \"pending\": { \/\/ see running above },\n \"vcores\": { \/\/ see running above }\n }\n }\n ]\n} Examples Request with mode=user : curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/reports\/operational\/clusterstats?st=1536658189000&et=1536744589000&mode=user\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response for mode=user : {\n \"userStats\": [\n {\n \"root\": {\n \"running\": {\n \"min\": 1,\n \"max\": 1,\n \"mean\": 1,\n \"stddev\": 0\n },\n \"memory\": {\n \"min\": 1024,\n \"max\": 16384,\n \"mean\": 7040,\n \"stddev\": 7099.0974074174\n },\n \"pending\": {\n \"min\": 0,\n \"max\": 0,\n \"mean\": 0,\n \"stddev\": 0\n },\n \"vcores\": {\n \"min\": 1,\n \"max\": 3,\n \"mean\": 2,\n \"stddev\": 0.81649658092773\n }\n }\n }\n ]\n} Request with mode=queue : curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/reports\/operational\/clusterstats?st=1536658189000&et=1536744589000&mode=queue\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response for mode=queue : {\n \"queueStats\": [\n {\n \"root.users.root\": {\n \"running\": {\n \"min\": 1,\n \"max\": 1,\n \"mean\": 1,\n \"stddev\": 0\n },\n \"memory\": {\n \"min\": 1024,\n \"max\": 16384,\n \"mean\": 7040,\n \"stddev\": 7099.0974074174\n },\n \"pending\": {\n \"min\": 0,\n \"max\": 0,\n \"mean\": 0,\n \"stddev\": 0\n },\n \"vcores\": {\n \"min\": 1,\n \"max\": 3,\n \"mean\": 2,\n \"stddev\": 0.81649658092773\n }\n }\n }\n ]\n} Response for mode=app : {\n [\n appID : application id,\n type : memorySeconds\/vcoreSeconds\n vcore : vcore value\n memory : memory value\n ]\n} " }, 
{ "title" : "\/reports\/operational\/clusterworkload?gte={timestamp}&lte={timestamp}&reportBy={interval}", 
"url" : "102359-reports-operational-clusterworkload.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Report endpoints \/ \/reports\/operational\/clusterworkload?gte={timestamp}&lte={timestamp}&reportBy={interval}", 
"snippet" : "Gets a summary of the cluster's workloads. Request GET http:\/\/ unravel-host :3000\/api\/v1\/reports\/operational\/clusterworkload?gte={timestamp}&lte={timestamp}&reportBy={interval} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description gte string Start time, i...", 
"body" : "Gets a summary of the cluster's workloads. Request GET http:\/\/ unravel-host :3000\/api\/v1\/reports\/operational\/clusterworkload?gte={timestamp}&lte={timestamp}&reportBy={interval} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description gte string Start time, in Unix epoch format. lte string End time, in Unix epoch format. reportBy string Report interval. Valid values are month , hour , hourday . Response body \/\/ work load by month one of more months with the application count for month\n\/\/ minimum of one month\n { timestamp: appcount[,timestamp: appcount] }\n\n\/\/ work load by hour array of 25 hours\n [\n { timestamp: appcount }, ... { timestamp: appcount }\n ]\n\n\/\/ work load by day - array for each days contained with the time period (Mon - Sun)\n\/\/ minimum of one day\n [\n { timestamp: appcount }\n ]\n\n\/\/ work load by hour\/day:array for each day (Mon - Sun) by hour\n\/\/ minimum of 24 hours for one day\n [\n { timestamp: appcount }\n ]\n\n Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/reports\/operational\/clusterworkload?gte=1536777000Z&lte=1536863400Z&reportBy=month\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: {\"1536777000\":96,\"1536863400\":4} \n[\n{\"1536813000000\":0},{\"1536816600000\":0},{\"1536820200000\":0},{\"1536823800000\":10},{\"1536827400000\":7},{\"1536831000000\":13},{\"1536834600000\":10},{\"1536838200000\":31},{\"1536841800000\":0},{\"1536845400000\":0},{\"1536849000000\":4},{\"1536852600000\":2},{\"1536856200000\":0},{\"1536859800000\":0},{\"1536863400000\":0},{\"1536867000000\":0},{\"1536870600000\":0},{\"1536874200000\":0},{\"1536877800000\":0},{\"1536881400000\":0},{\"1536885000000\":0},{\"1536888600000\":0},{\"1536892200000\":0},{\"1536895800000\":6},{\"1536899400000\":2}]\n[\n{\"1536777000000\":96},{\"1536863400000\":7}\n]\n[{\"1536813000000\":0},{\"1536816600000\":0},{\"1536820200000\":0},{\"1536823800000\":10},{\"1536827400000\":7},{\"1536831000000\":13},{\"1536834600000\":10},{\"1536838200000\":31},{\"1536841800000\":0},{\"1536845400000\":0},{\"1536849000000\":4},{\"1536852600000\":2},{\"1536856200000\":0},{\"1536859800000\":0},{\"1536863400000\":0},{\"1536867000000\":0},{\"1536870600000\":0},{\"1536874200000\":0},{\"1536877800000\":0},{\"1536881400000\":0},{\"1536885000000\":0},{\"1536888600000\":0},{\"1536892200000\":0},{\"1536895800000\":6},{\"1536899400000\":2}]\n " }, 
{ "title" : "\/reports\/queueanalysis\/get_latest_report_by_queue_names", 
"url" : "102360-reports-queueanalysis-get-latest-report-by-queue-names.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Report endpoints \/ \/reports\/queueanalysis\/get_latest_report_by_queue_names", 
"snippet" : "Under construction....", 
"body" : "Under construction. " }, 
{ "title" : "\/reports\/queueanalysis\/get_latest_report_by_queue_name", 
"url" : "102361-reports-queueanalysis-get-latest-report-by-queue-name1.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Report endpoints \/ \/reports\/queueanalysis\/get_latest_report_by_queue_name", 
"snippet" : "Under construction....", 
"body" : "Under construction. " }, 
{ "title" : "\/reports\/schedules", 
"url" : "102362-reports-schedules.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Report endpoints \/ \/reports\/schedules", 
"snippet" : "Under construction....", 
"body" : "Under construction. " }, 
{ "title" : "Search endpoints", 
"url" : "102363-resource-search.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Search endpoints", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "\/search\/cb\/appt?from={date}&to={date}", 
"url" : "102364-search-cb-appt.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Search endpoints \/ \/search\/cb\/appt?from={date}&to={date}", 
"snippet" : "Gets chargeback reports per app type and the number of apps in all queues for all users across all clusters. Request GET http:\/\/ unravel-host :3000\/api\/v1\/search\/cb\/appt?from={date}&to={date} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description from stri...", 
"body" : "Gets chargeback reports per app type and the number of apps in all queues for all users across all clusters. Request GET http:\/\/ unravel-host :3000\/api\/v1\/search\/cb\/appt?from={date}&to={date} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description from string Start date. Format: YYYY-MM-DD or Unix epoch value. to string End date. Format: YYYY-MM-DD or Unix epoch value. Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/search\/cb\/appt?from=1536670860&to=1536757260\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response body: {\n \"cb\": [\n {\n \"ms\": 2324825154,\n \"count\": 4332,\n \"v1\": \"mr\",\n \"vs\": 1512734\n },\n {\n \"ms\": 298989641,\n \"count\": 75,\n \"v1\": \"spark\",\n \"vs\": 120404\n }\n ]\n} " }, 
{ "title" : "Response body", 
"url" : "102364-search-cb-appt.html#UUID-725ef47f-4ac6-4535-af17-fe92dd841a79_chargebackReportContents", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Search endpoints \/ \/search\/cb\/appt?from={date}&to={date} \/ Response body", 
"snippet" : "The response body contains one chargeback report per app type. Each report contains the fields shown in the table below. Name Type Description ms integer Memory usage in seconds. count integer App count. v1 string App type ( mr or spark ). vs integer Vcore usage in seconds....", 
"body" : "The response body contains one chargeback report per app type. Each report contains the fields shown in the table below. Name Type Description ms integer Memory usage in seconds. count integer App count. v1 string App type ( mr or spark ). vs integer Vcore usage in seconds. " }, 
{ "title" : "\/search\/cb\/appt\/queue", 
"url" : "102365-search-cb-appt-queue.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Search endpoints \/ \/search\/cb\/appt\/queue", 
"snippet" : "Gets chargeback reports per queue. Request GET http:\/\/ unravel-host :3000\/api\/v1\/search\/cb\/appt\/queue?from={date}&to={date} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description from string Start date. Format: YYYY-MM-DD or Unix epoch value. to string End...", 
"body" : "Gets chargeback reports per queue. Request GET http:\/\/ unravel-host :3000\/api\/v1\/search\/cb\/appt\/queue?from={date}&to={date} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description from string Start date. Format: YYYY-MM-DD or Unix epoch value. to string End date. Format: YYYY-MM-DD or Unix epoch value. Response body The response body contains one chargeback report per queue. Each report contains the fields shown in the table below. Name Type Description count string App count. v1 string App type ( mr or spark ). cb array Chargeback report . ms integer Memory usage in seconds. count integer App count. v2 string App type ( mr or spark ). vs integer Vcore usage in seconds. Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/search\/cb\/appt\/queue?from=1536676860&to=1536763260\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: {\n \"cb\": [\n {\n \"count\": 27,\n \"v1\": \"mr\",\n \"cb\": [\n {\n \"ms\": 1974235,\n \"count\": 27,\n \"v2\": \"root.users.root\",\n \"vs\": 798\n }\n ]\n }\n ]\n} " }, 
{ "title" : "\/search\/cb\/appt\/user?from={date}&to={date}", 
"url" : "102366-search-cb-appt-user.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Search endpoints \/ \/search\/cb\/appt\/user?from={date}&to={date}", 
"snippet" : "Gets chargeback reports by application type for a specific queue. Request GET http:\/\/ unravel-host :3000\/api\/v1\/search\/cb\/appt\/user?from={date}&to={date} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description from string Start date. Format: YYYY-MM-DD or U...", 
"body" : "Gets chargeback reports by application type for a specific queue. Request GET http:\/\/ unravel-host :3000\/api\/v1\/search\/cb\/appt\/user?from={date}&to={date} Path parameters None. Query parameters Required parameters are highlighted . Name Type Description from string Start date. Format: YYYY-MM-DD or Unix epoch value. to string End date. Format: YYYY-MM-DD or Unix epoch value. Response body The response body contains one chargeback report per queue. Each report contains the fields shown in the table below. Name Type Description count string App count. v1 string App type ( mr or spark ). cb array Chargeback report . ms integer Memory usage in seconds. count integer App count. v2 string App type ( mr or spark ). vs integer Vcore usage in seconds. Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/search\/cb\/appt\/user?from=1536676860&to=1536763260\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: {\n \"cb\": [\n {\n \"count\": 27,\n \"v1\": \"mr\",\n \"cb\": [\n {\n \"ms\": 1974235,\n \"count\": 27,\n \"v2\": \"root.users.root\",\n \"vs\": 798\n }\n ]\n }\n ]\n} " }, 
{ "title" : "Authentication API", 
"url" : "102367-resource-signin.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Authentication API", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "\/signIn", 
"url" : "102368-signin.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Authentication API \/ \/signIn", 
"snippet" : "Authenticates your API session and gets a token which you include in all requests in this session. Whenever you start a new session, first send this request to generate a new token. To reach Unravel Server you might need to open an SSH session to your cluster, cluster-name . ssh@root cluster-name...", 
"body" : "Authenticates your API session and gets a token which you include in all requests in this session. Whenever you start a new session, first send this request to generate a new token. To reach Unravel Server you might need to open an SSH session to your cluster, cluster-name . ssh@root cluster-name " }, 
{ "title" : "Request", 
"url" : "102368-signin.html#UUID-80a6a237-c6a6-b32c-b4c0-b7f740710ae9_section-idm13128377942222", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Authentication API \/ \/signIn \/ Request", 
"snippet" : "POST http:\/\/ unravel-host :3000\/api\/v1\/signIn...", 
"body" : "POST http:\/\/ unravel-host :3000\/api\/v1\/signIn " }, 
{ "title" : "Path parameters", 
"url" : "102368-signin.html#UUID-80a6a237-c6a6-b32c-b4c0-b7f740710ae9_section-idm13128377985934", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Authentication API \/ \/signIn \/ Path parameters", 
"snippet" : "None....", 
"body" : "None. " }, 
{ "title" : "Query parameters", 
"url" : "102368-signin.html#UUID-80a6a237-c6a6-b32c-b4c0-b7f740710ae9_section-idm13128378235278", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Authentication API \/ \/signIn \/ Query parameters", 
"snippet" : "Name Type Description username string Your username password string Your password...", 
"body" : "Name Type Description username string Your username password string Your password " }, 
{ "title" : "Response body", 
"url" : "102368-signin.html#UUID-80a6a237-c6a6-b32c-b4c0-b7f740710ae9_section-idm13128378269960", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Authentication API \/ \/signIn \/ Response body", 
"snippet" : "Name Type Description message string Indicates success or failure. token string The token which you will include in the Authorization header on all subsequent requests in this session. role string   readOnly string   tags string id string username string...", 
"body" : "Name Type Description message string Indicates success or failure. token string The token which you will include in the Authorization header on all subsequent requests in this session. role string   readOnly string   tags string id string username string " }, 
{ "title" : "Examples", 
"url" : "102368-signin.html#UUID-80a6a237-c6a6-b32c-b4c0-b7f740710ae9_section-idm13128378316048", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Authentication API \/ \/signIn \/ Examples", 
"snippet" : "Request: curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/signIn\" -H \"accept: application\/json\" -H \"content-type: application\/x-www-form-urlencoded\" -d \"username=admin&password= your-password \" Response body: { \"message\": \"ok\", \"token\": \" long-string \", \"role\": \"admin\", \"readOnly\": false, \"tags\":...", 
"body" : "Request: curl -X GET \"http:\/\/http:\/\/myserver.com:3000\/api\/v1\/signIn\" -H \"accept: application\/json\" -H \"content-type: application\/x-www-form-urlencoded\" -d \"username=admin&password= your-password \" Response body: {\n \"message\": \"ok\",\n \"token\": \" long-string \",\n \"role\": \"admin\",\n \"readOnly\": false,\n \"tags\": \"\",\n \"id\": \"admin\",\n \"username\": \"admin\"\n} " }, 
{ "title" : "Workflow endpoints", 
"url" : "102369-resource-workflows.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Workflow endpoints", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "\/workflows", 
"url" : "102370-workflows.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Workflow endpoints \/ \/workflows", 
"snippet" : "Lists all workflows. Request GET http:\/\/ unravel-host :3000\/api\/v1\/workflows Path parameters None. Query parameters None. Response body Name Type Description [No name] array JSON structure containing a JSON structure for each workflow Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/wo...", 
"body" : "Lists all workflows. Request GET http:\/\/ unravel-host :3000\/api\/v1\/workflows Path parameters None. Query parameters None. Response body Name Type Description [No name] array JSON structure containing a JSON structure for each workflow Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/workflows\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: {\n \"workflow_spark_test_20190626T032742Z\": {\n \"key\": \"workflow_spark_test_20190626T032742Z\",\n \"doc_count\": 1,\n \"duration_stats\": {\n \"count\": 1,\n \"min\": 12524,\n \"max\": 12524,\n \"avg\": 12524,\n \"sum\": 12524,\n \"sum_of_squares\": 156850576,\n \"variance\": 0,\n \"std_deviation\": 0,\n \"std_deviation_bounds\": {\n \"upper\": 12524,\n \"lower\": 12524\n }\n }\n }\n} " }, 
{ "title" : "\/workflows\/missing_sla", 
"url" : "102371-workflows-missing-sla.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Workflow endpoints \/ \/workflows\/missing_sla", 
"snippet" : "Lists workflows with missed SLA. Request GET http:\/\/ unravel-host :3000\/api\/v1\/workflows\/missing_sla?from={date}&to={date} Path parameters None. Query parameters Name Type Description from string Start date. Format YYYY-MM-DD to string End date. Format YYYY-MM-DD Response body { \"duration\": 0, \"avgD...", 
"body" : "Lists workflows with missed SLA. Request GET http:\/\/ unravel-host :3000\/api\/v1\/workflows\/missing_sla?from={date}&to={date} Path parameters None. Query parameters Name Type Description from string Start date. Format YYYY-MM-DD to string End date. Format YYYY-MM-DD Response body {\n \"duration\": 0,\n \"avgDuration\": 0,\n \"user\": \"string\",\n \"name\": \"string\"\n} Name Type Description [No name] array JSON structure containing a JSON structure for each missed SLA Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/workflows\/missing_sla?from=2019-02-01&to=2019-03-06\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response where there are no missed SLAs: [ ] Response where there's one missed SLA: {\n \"Benchmark: Road_Accident_2005-2016\": {\n \"key\": \"Benchmark: Road_Accident_2005-2016\",\n \"doc_count\": 565,\n \"duration_stats\": {\n \"count\": 565,\n \"min\": 21000,\n \"max\": 772000,\n \"avg\": 174877.8761061947,\n \"sum\": 98806000,\n \"sum_of_squares\": 17972790000000,\n \"variance\": 1227976236.197041,\n \"std_deviation\": 35042.49186626204,\n \"std_deviation_bounds\": {\n \"upper\": 244962.85983871878,\n \"lower\": 104792.8923736706\n }\n }\n }\n} " }, 
{ "title" : "\/workflows\/{workflow_id}\/annotation", 
"url" : "102372-workflows-workflow-id-annotation.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ Workflow endpoints \/ \/workflows\/{workflow_id}\/annotation", 
"snippet" : "Lists a specific workflow. Request GET http:\/\/ unravel-host :3000\/api\/v1\/workflows\/{workflow_id}\/annotation Path parameters Name Description workflow_id ID of workflow Query parameters None. Response body Return code upon success: 200 Name Type Description instances array JSON structure containing d...", 
"body" : "Lists a specific workflow. Request GET http:\/\/ unravel-host :3000\/api\/v1\/workflows\/{workflow_id}\/annotation Path parameters Name Description workflow_id ID of workflow Query parameters None. Response body Return code upon success: 200 Name Type Description instances array JSON structure containing details of each run of the workflow instanceCompare array JSON structure containing a summary of metrics for each run All timestamps are in Unix epoch time. Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/workflows\/annotation\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: {\n instanceCompare: {\n\t\t\t\t\tworkflowIds: [\"20180825T003618Z-8434469366206475039\", \"20180825T003706Z-8434469366206503807\"}\n\t\t\t\t\tavgDuration: 27056.8\n\t\t\t\t\tavgService: 0.1554950102167776\n\t\t\t\t\tavgTotalDfsBytesRead: 298227334.8\n\t\t\t\t\tavgTotalDfsBytesWritten: 1524.1\n\t\t\t\t\tdtimes: [1535157378000, 1535157426000, 1535157475000, 1535157524000, 1535157572000, 1535157621000,]\n\t\t\t\t\tduration: [10881, 34708, 11218, 33382, 11173, 37431, 77868, 9720, 9671, 34516]\n\t\t\t\t\tmaxDuration: 77868\n\t\t\t\t\t......\n},\ninstances: [\n\t\t\t{\n\t\t\tannotation: {@class: \"com.unraveldata.annotation.WorkflowInstanceAnnotation\", vcoreSeconds: 0, memorySeconds: 0,}\n\t\t\tdt: 1535157378000\n\t\t\thi: 0\n\t\t\tid: \"20180825T003618Z-8434469366206475039\"\n\t\t\tnm: \"wf-sla-mr-spark\"\n\t\t\tsignature: \"unravel.workflow.name=wf-sla-mr-spark\"\n\t\t\tut: \"20180825T003618Z\"\n\t\t\twc: [{ci: \"job_1534794873154_4973\", ct: \"mapred\", st: 1535157384745, et: 1535157373534}\n\t\t\t\t0: {ci: \"job_1534794873154_4973\", ct: \"mapred\", st: 1535157384745, et: 1535157373534}\n\t\t\t\t1: {ci: \"application_1534794873154_4974\", ct: \"spark\", st: 1535157384020, et: 1535157394901}\n\t\t\t\t],\n\t\t\t}....\n\n\t\t\t]\n} " }, 
{ "title" : "YARN Resource Manager endpoints", 
"url" : "102373-resource-yarn-rm.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ YARN Resource Manager endpoints", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "\/yarn_rm\/kill_app", 
"url" : "102374-yarn-rm-kill-app.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ YARN Resource Manager endpoints \/ \/yarn_rm\/kill_app", 
"snippet" : "Gets the status code for a killed YARN app. Request GET http:\/\/ unravel-host :3000\/api\/v1\/yarn_rm\/move_app?clusterid={cluster_id}&appid={app_id}&queue={queue_name} Path parameters None. Query parameters Name Type Description cluserid string Cluster ID. Required . appid string App ID. Required . Resp...", 
"body" : "Gets the status code for a killed YARN app. Request GET http:\/\/ unravel-host :3000\/api\/v1\/yarn_rm\/move_app?clusterid={cluster_id}&appid={app_id}&queue={queue_name} Path parameters None. Query parameters Name Type Description cluserid string Cluster ID. Required . appid string App ID. Required . Response body Name Type Description duration integer avgDuration integer user string name string Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/yarn_rm\/kill_app?clusterid=clusterid=ignite1&appid=application_1550764666755_0668\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: 200 " }, 
{ "title" : "\/yarn_rm\/move_app", 
"url" : "102375-yarn-rm-move-app.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ REST API \/ All endpoints by resource name \/ YARN Resource Manager endpoints \/ \/yarn_rm\/move_app", 
"snippet" : "Gets the status code for a moved YARN app. Request GET http:\/\/ unravel-host :3000\/api\/api\/v1\/yarn_rm\/move_app?clusterid={cluster_id}&appid={app_id}&queue={queue_name} Path parameters None. Query parameters Name Type Description cluserid string Cluster ID. Required . appid string App ID. Required . q...", 
"body" : "Gets the status code for a moved YARN app. Request GET http:\/\/ unravel-host :3000\/api\/api\/v1\/yarn_rm\/move_app?clusterid={cluster_id}&appid={app_id}&queue={queue_name} Path parameters None. Query parameters Name Type Description cluserid string Cluster ID. Required . appid string App ID. Required . queue string Queue name. Required . Response body Name Type Description duration integer avgDuration integer user string name string Examples Request: curl -X GET \"http:\/\/myserver.com:3000\/api\/v1\/yarn_rm\/move_app?clusterid=ignite1&appid=application_1550764666755_0668&queue=root.users.root\" -H \"accept: application\/json\" -H \"Authorization: JWT token \" Response: 200 " }, 
{ "title" : "Roles and Role Based Access Control (RBAC)", 
"url" : "102376-rbac.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Roles and Role Based Access Control (RBAC)", 
"snippet" : "Unravel supports three roles. Admin – has complete access to UI with read\/write permissions. Read-only admin – has complete access to the UI but cannot write. End-user - has access to the UI but not the Manage page, e.g., can't see AutoActions. RBAC lets the admin to restrict an end-users view to ce...", 
"body" : "Unravel supports three roles. Admin – has complete access to UI with read\/write permissions. Read-only admin – has complete access to the UI but cannot write. End-user - has access to the UI but not the Manage page, e.g., can't see AutoActions. RBAC lets the admin to restrict an end-users view to certain pages and apps. Admins and Read-only Admins views and abilities are not affected by RBAC, i.e., RBAC is irrelevant. If you are not familiar with the concept of tagging, see What is tagging? . " }, 
{ "title" : "RBAC roles", 
"url" : "102377-rbac-roles.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Roles and Role Based Access Control (RBAC) \/ RBAC roles", 
"snippet" : "RBAC lets admins restrict the pages a specific end-user can view and how those pages are populated. Application tagging is intertwined with RBAC. While it is possible to use RBAC without defining applications tags, its usefulness is limited. See What is tagging? if you are not familiar with the conc...", 
"body" : "RBAC lets admins restrict the pages a specific end-user can view and how those pages are populated. Application tagging is intertwined with RBAC. While it is possible to use RBAC without defining applications tags, its usefulness is limited. See What is tagging? if you are not familiar with the concept of tagging and how to generate tags for Unravel to use. The end-user's access is restricted based upon three factors Tags for the end-user You can create tags for Applications - See Tagging applications . Workflows - See Tagging workflows . End-users are then associated with the tags via LDAP or SAML. See com.unraveldata.login.mode . When RBAC is turned on an end-user's view is filtered based upon their tags. For instance, if a user only has the defined tag dept:marketing they can only see applications tagged with dept:marketing. Unravel default tag com.unraveldata.rbac.default is always used to filter the end-user's view. It is set to Username by default. Mode com.unraveldata.ngui.user.mode extended a user can access Application > Applications Operations > Usage Details > Infrastructure Operations > Usage Details > Impala Usage Reports > Operational Insights > Chargeback restricted a user can only access Application > Applications What the end-user sees when RBAC is turned on The available pages (as defined by com.unraveldata.ngui.user.mode ) display applications contained in (filtered end-user tags) ∪ (filtered by com.unraveldata.rbac.default ) If com.unraveldata.rbac.default is not set and an end-user has no tags, the viewable pages are unpopulated (blank). How to exempt an end-user from RBAC To exempt an end-user from RBAC, you must make them a read-only admin . " }, 
{ "title" : "Configuring RBAC general properties", 
"url" : "102378-rbac-configure.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Roles and Role Based Access Control (RBAC) \/ Configuring RBAC general properties", 
"snippet" : "Open \/usr\/local\/unravel\/etc\/unravel.properties . Search for and update the following properties; if you can't find them, add them. See RBAC properties for descriptions. Set com.unraveldata.login.mode to open, ldap or saml. com.unraveldata.login.mode= mode com.unraveldata.rbac.enabled=true com.unrave...", 
"body" : "Open \/usr\/local\/unravel\/etc\/unravel.properties . Search for and update the following properties; if you can't find them, add them. See RBAC properties for descriptions. Set com.unraveldata.login.mode to open, ldap or saml. com.unraveldata.login.mode= mode \ncom.unraveldata.rbac.enabled=true \ncom.unraveldata.rbac.default=userName \ncom.unraveldata.ngui.user.mode=extended \ncom.unraveldata.rbac.tagcmd= path_of_tag_file If you are upgrading from 4.3, you must replace\/redefine the following properties. Here we are assuming login.mode = ldap . If login.mode = saml , you use saml in the following properties instead of ldap . 4.3 property Replacement com.unraveldata.rbac.mode =ldap com.unraveldata.login.mode =ldap com.unraveldata.rbac.prefix =dept- com.unraveldata.rbac.ldap.tag.dept.regex.find =dept-(.*) com.unraveldata.rbac.tag =dept com.unraveldata.rbac.ldap.tags =dept com.unraveldata.rbac.user.operations.enabled =true com.unraveldata.ngui.user.mode =(extended | restricted) You can exempt specific end-users from RBAC effects by adding them to the read-only admin group. Modify the following property, based upon com.unraveldata.login.mode . Open com.unraveldata.login.admins.readonly=user1,user2,user3 LDAP com.unraveldata.login.admins.ldap.readonly=user1,user2,user3 SAML com.unraveldata.login.admins.saml.readonly=user1,user2,user3 " }, 
{ "title" : "Configuring LDAP or SAML RBAC properties", 
"url" : "102379-rbac-configure-ldap-saml.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Roles and Role Based Access Control (RBAC) \/ Configuring LDAP or SAML RBAC properties", 
"snippet" : "RBAC uses tags, if you are not familiar with tagging please see What is tagging for an explanation of tags and their creation. Configure the following properties for either SAML and LDAP based upon the value of com.unraveldata.login.mode . You can exempt end-users from RBAC by adding them to the rea...", 
"body" : "RBAC uses tags, if you are not familiar with tagging please see What is tagging for an explanation of tags and their creation. Configure the following properties for either SAML and LDAP based upon the value of com.unraveldata.login.mode . You can exempt end-users from RBAC by adding them to the read-only admin group as shown. See LDAP and SAML for property definitions. LDAP \/\/ Required com.unraveldata.login.admins.ldap.groups=admin1,admin2,admin3 \ncom.unraveldata.rbac.ldap.tags.find=proj,dept \ncom.unraveldata.rbac.ldap.proj.regex.find=proj-(.*)\ncom.unraveldata.rbac.ldap.dept.regex.find=dept-(.*) \/\/ Optional com.unraveldata.login.admins.readonly.ldap.groups=RO-admin4,RO-admin5,RO=admin6 SAML \/\/ Required com.unraveldata.login.admins.saml.groups=admin1,admin2,admin3 \ncom.unraveldata.rbac.saml.tags.find=proj,dept \ncom.unraveldata.rbac.saml.proj.regex.find=proj-(.*)\ncom.unraveldata.rbac.saml.dept.regex.find=dept-(.*) \/\/ Optional com.unraveldata.login.admins.readonly.saml.groups=RO-admin4,RO-admin5,RO=admin6 Example When a user logs on, their LDAP or SAML group is read and used to create their tags, if any. This example uses the LDAP definitions above to parse the LDAP groups each user belongs to. User LDAP Groups Tags Key Value user1 [\"dept-hr,\"dept-sale\",\"dept-finance\"] {\"dept\":[\"hr\",\"sale\",\"finance\"]} dept hr, sales, finance user2 [\"proj-group1\",\"proj-group2\", \"proj-group3\"] {\"proj\":[\"group1\",\"group2\", \"group3\"]} proj group1, group2, group3 user3 [\"proj-group1\",\"proj-group2\", \"proj-group3\", \"dept-hr,\"dept-sale\",\"dept-finance\"] {\"proj\":[\"group1\",\"group2\", \"group3\"]} proj group01, group02, group03 user4 [\"div-div1\",\"div-div2\", \"div-div3\"] n\/a n\/a n\/a user1 and user2 LDAP groups each have one valid key with three values. user3 LDAP groups has two valid keys, but Unravel stops parsing the when it finds a match. In this case the key proj which has three values is used to generate the RBAC tags. user4 LDAP groups has one key, div which has not been specified therefore no RBAC tags are created for them. " }, 
{ "title" : "Example RBAC configurations", 
"url" : "102380-rbac-example-config.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Roles and Role Based Access Control (RBAC) \/ Example RBAC configurations", 
"snippet" : "Admins and read-only admins are always exempt from RBAC restrictions. To use RBAC, set these properties: com.unraveldata.rbac.enabled =true com.unraveldata.ngui.user.mode =[extended | restricted] com.unraveldata.login.admins [.readyonly] are irrelevant if mode is LDAP or SAML. [empty] In the example...", 
"body" : "Admins and read-only admins are always exempt from RBAC restrictions. To use RBAC, set these properties: com.unraveldata.rbac.enabled =true com.unraveldata.ngui.user.mode =[extended | restricted] com.unraveldata.login.admins [.readyonly] are irrelevant if mode is LDAP or SAML. [empty] In the examples below use LDAP, for SAML just substitute saml for ldap. Replace your local values for text . Set admin access To set admin and not read-only admin access, set and comment out: com.unraveldata.login.admins=L772417,K228680\n#com.unraveldata.login.admins.readonly= For LDAP or SAML, set and comment out: com.unraveldata.login.mode=LDAP\ncom.unraveldata.login.admins.ldap.groups=LDAP_Users,,,,\n#com.unraveldata.login.admins.readonly.ldap.groups=LDAP_Users,,,, Set only read-only admin access To set only read-only admin access, set and comment out: com.unraveldata.login.admins.readonly=RO-L772417,RO-K228680\n#com.unraveldata.login.admins=L772417,K22868 For LDAP or SAML, set and comment out: com.unraveldata.login.mode=LDAP \ncom.unraveldata.login.admins.readonly.ldap.groups= LDAP_Users ,,,,, \n#com.unraveldata.login.admins.ldap.groups= LDAP_Users Set admin and read-only admin access For admin and read-only admin access, set: com.unraveldata.login.admins=L772417,K228680\ncom.unraveldata.login.admins.readonly=RO-L772417,RO-K228680 For LDAP or SAML set: com.unraveldata.login.mode=LDAP \ncom.unraveldata.login.admins.readonly.ldap.groups= LDAP_Users ,,,,\ncom.unraveldata.login.admins.ldap.groups= LDAP_Users \n Exempt select end-users from RBAC To exempt end-users from RBAC add them to the read-only admin property: com.unraveldata.login.admins.readonly=RO-L772417,RO-K228680 For LDAP or SAML add them to: com.unraveldata.login.admins.ldap.groups= LDAP_Users \n " }, 
{ "title" : "Manage page", 
"url" : "102381-rbac-manage-page.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Roles and Role Based Access Control (RBAC) \/ Manage page", 
"snippet" : "This page is only available to admins and read-only admins....", 
"body" : "This page is only available to admins and read-only admins. " }, 
{ "title" : "RBAC UI", 
"url" : "102382-rbac-ui.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Roles and Role Based Access Control (RBAC) \/ RBAC UI", 
"snippet" : "If com.unraveldata.login.mode is set to LDAP or SAML, you can only toggle the RBAC Access status. The tags used for RBAC end-users must also be loaded as Application and Workflows tags. If they are not, the pages are filtered only on com.unraveldata.rbac.default . (See RBAC Configuration .) Go to Ma...", 
"body" : "If com.unraveldata.login.mode is set to LDAP or SAML, you can only toggle the RBAC Access status. The tags used for RBAC end-users must also be loaded as Application and Workflows tags. If they are not, the pages are filtered only on com.unraveldata.rbac.default . (See RBAC Configuration .) Go to Manage | Role Manager to access the Role Manager. The RBAC default is set via com.unraveldata.rbac.enabled . You can toggle the status of RBAC; however, when the Unravel daemon is restarted RBAC resets. If you are not using LDAP\/SAML login mode, you can add filters for specific end-users. Any end-user roles you have previously set are displayed. If the Unravel daemon was restarted after you added end-user roles the entries are lost. You can add end-users one at at time via Add New Role . To add multiple users at a time create and upload a csv file. Adding roles. You limit end-user access through tags. In the example below only two tags are available, project and tenant . If a 3rd tag, department , had been defined it would be available. The end-user filters between the red brackets were loaded using a . csv file Clicking on Add New Role adds a row to the Roles table containing text boxes (1). You must define the User and at least one tag restriction. To add multiple tag names under a tag type separate the tag names with commas, be sure the string contains no spaces or special characters. To save the entry click . Click to delete your entry click without saving it. Adding one or more roles via a role file Click on Select role file to choose the . csv file. The format of a . csv is: first row is a header row defining the columns tags : user, tagKey[,tagKey]* tagKey : is a valid tag key, i.e., department, tenant. one or more rows defining user and tag values : user, tagValue[,tagValue]* tagValue : is empty, a valid tag value for tagKey , or tagString , tagString : is a series of tagValues separated by commas and enclosed in quotes, and * : means zero or more Note: The file must define at least one tagKey , one user and one tagValue for the user . After you add your last tagValue you can leave the rest of the row blank. See the userNew filter in the CSV file below for an example. tagValues must be ordered as defined in the header row. No special characters or spaces are allowed in file. The CSV file below was used to load filters within the red brackets. user,project,tenant \nuser72,\"group1,group2\",mm \nuser25,,\"3n,3m\" userNew,groupNew \nuser33,\"group3,group2\",\"3m,mm\"\n Editing or deleting roles. To edit a role, click the edit glyph ( ). You can add or delete tags, but not edit the end-user's name. To delete a role, click the delete glyph. Effect of RBAC control End-user's access with RBAC turned off The user has access to all the Unravel UI features and all applications. End-user's access with RBAC turned on. The user only has access to their applications or those matching their tags. " }, 
{ "title" : "Tagging", 
"url" : "102383-tagging.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Tagging", 
"snippet" : "See What is tagging? for an overview of what tagging is. Application and workflow tags allow you to: Filter the applications displayed. Group applications together (workflow). Filter applications for chargeback reports . Limit users UI access and applications they can see via Role Based Access Contr...", 
"body" : "See What is tagging? for an overview of what tagging is. Application and workflow tags allow you to: Filter the applications displayed. Group applications together (workflow). Filter applications for chargeback reports . Limit users UI access and applications they can see via Role Based Access Control . " }, 
{ "title" : "What is tagging?", 
"url" : "102384-tagging-what-is-tagging.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Tagging \/ What is tagging?", 
"snippet" : "Tagging is a method to apply more context to your applications so you can view and organize them with more specificity based upon your requirements and needs. There are two types of tags: Application tags provide ways to filter applications. Workflow tags logically group a set of applications. Appli...", 
"body" : "Tagging is a method to apply more context to your applications so you can view and organize them with more specificity based upon your requirements and needs. There are two types of tags: Application tags provide ways to filter applications. Workflow tags logically group a set of applications. Application tagging By the time your application reaches the cluster much of its context is lost, e.g., the dept which submitted it, what project it belongs to, etc. Available metadata by default All applications have the following metadata. (See how the metadata fields are populated .) App type App Id App Name Username Cluster (empty for server-less applications) Queue (empty for server-less applications) Configuration (accessible for some applications) As an admin you need to be able to organize\/view the applications running on the cluster with more granularity than this limited set allows. You will want to “slice and dice\" applications a myriad of ways for a variety of reasons. Unravel can not “magically” deduce which applications belong to what tenants, projects, teams, etc. to gain such granularity you must provide Unravel with this information. Tagging an app is how you tell Unravel about its context. What is a tag At the basic level a tag is simply a < key , value > pair which is associated with an application. A simple way to conceptualize tagging is to think of a spreadsheet. The key is the column header and the value an entry in the column. For each application you enter your value, if any, in the column. The following example has two keys, Tenant and Project, which can be used to filter applications. While an app can have multiple tags, it cannot have multiple values for a given key. In this example, an app cannot be tagged with proj1 and proj2 , App Type App ID App Name User Cluster Queue Tenant Project MR j_134 distc John def root.UM-proj1.print marketing proj1 Spark s_345 rate.spark.sim Jane def root.UM-proj2.print sales proj2 Spark s_456 over.spark.sim Jane def root.UM-proj3.print sales proj3 See the following Example for an explanation of how these tags were generated. Effective use of tags You can use tags to: Generate chargeback reports based upon specific criteria, e.g., project, dept, team, etc. Decide which apps a specific user can see, e.g., the marketing head can see all marketing apps, while user2 can only see specific marketing project apps. Group applications together. (See Tagging workflows .) In order to effectively use tags you need to understand your requirements for displaying or grouping your apps. For example: Do you need chargebacks reports for each tenant in a multi-tenant cluster? Then apps must be tagged with the tenant it belongs to. Do applications need to be billed back to departments and teams? Apps need department and team tags. Do you want to allow some users to see all projects and others just a subset of the projects (see Role Based Access Control )? Apps must have the project tags. A specific tag can be used for different purposes. You can use the project tag to generate chargeback reports and filter views specific users. Assigning tags You generate a tagging dictionary via a Python script. Unravel then uses the dictionary to apply tags to the applications as you prescribe. But like Unravel, the only information you have about the application running on the cluster is its metadata. So how can you develop a script to tag specific applications; how can you determine and generate your < key , value > pairs? Methods to generate\/create tags for an application Naming conventions By creating naming conventions for app, queues, and cluster names you can embed information to use for your tags, e.g., placing all apps belonging to project-1 in the root.UM-proj1.print queue lets you extract the project name from the queue the app is in. Using the metadata You can use the app’s metadata to create the tag values, for example: Directly, e.g., <team, username > Parse it to extract information, e.g., <project, {extracted from queue name }> Concatenate metadata with other metadata or strings, e.g., <dept, { username }+ {extracted from app name } + string> External mapping information A tagging script can access files that contain further mapping information. e.g., maps projects to tenants. You can download example tagging scripts . (This is currently private; please contact Unravel Support .) Workflow tagging Workflow tags are much simpler than application tags. You use preexisting Unravel tags to create workflows, specifically unravel.workflow.name and unravel.workflow.utctimestamp . See Tagging workflows for more information on creating workflows. " }, 
{ "title" : "Example", 
"url" : "102384-tagging-what-is-tagging.html#UUID-b7e75353-1b8b-00ce-b138-2d4654c4747d_N1554596243922", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Tagging \/ What is tagging? \/ Example", 
"snippet" : "The above table is simple example to help you understand tagging concepts. In your environment you will likely use a more complicated schema. In this example we show how the tags were created. Determining the tags The cluster is multi-tenant; we created the tags: <tenant, marketing> <tenant, sales> ...", 
"body" : "The above table is simple example to help you understand tagging concepts. In your environment you will likely use a more complicated schema. In this example we show how the tags were created. Determining the tags The cluster is multi-tenant; we created the tags: <tenant, marketing> <tenant, sales> There are three projects; we created the tags: <project, proj1> <project, proj2> <project, proj3> Finally, we have a file that maps the projects to the tenants: proj1 to marketing proj2 and proj3 to sales Assigning the tags We then told Unravel about the tags and how to assign them, i.e., developed the tagging dictionary. First, an app’s queue is parsed to extract the project it belongs to. The project name is encoded between \"UM-\" and \" . \". Once extracted, the name was used as the project value . Next, the script accessed a file which mapped the project name ( value ) to the tenant ( key ). The script resulted in three applications being assigned tenant and project tags. " }, 
{ "title" : "Tagging applications", 
"url" : "102385-tagging-applications.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Tagging \/ Tagging applications", 
"snippet" : "You can define tags for groups of applications using a python script. Unravel retrieves the script from the property com.unraveldata.app.tagging.script.path so you must define all your application tags in that file. You can also use this script to set workflow tags . You can think of the script as c...", 
"body" : "You can define tags for groups of applications using a python script. Unravel retrieves the script from the property com.unraveldata.app.tagging.script.path so you must define all your application tags in that file. You can also use this script to set workflow tags . You can think of the script as creating a database comprised of a list of keys , their associated values , and what applications are associated with a specific <key, value> . For example, You have three departments: finance, hr, and marketing. You would create the key department and give it three values finance, hr and marketing. You would then associate applications with one of more of <key, value> pairs. One hive query might be associated with dept:marketing while another with dept:finance . You can not associate an application with more than one value per key . Given the example above, an application cannot be associated with both dept:marketing and dept:finance . See What is tagging? for more information on tagging, its purpose and a more comprehensive description. Your Python script must be idempotent, i.e., it must produce the same result over multiple invocations with different input (metadata) for the same application. Application tags are immutable and once created they cannot be changed. Using a Python script See Writing a Python script and the example script for tips on how to write a script. Set the following properties in \/usr\/local\/unravel\/etc\/unravel.properties . com.unraveldata.tagging.script.enabled=true\ncom.unraveldata.app.tagging.script.path= python_script \ncom.unraveldata.app.tagging.script.method.name= method_name Restart the following daemons. You must restart these daemons after you reset the property values above or edit the script referenced. \/etc\/init.d\/unravel_all.sh stop-etl\n\/etc\/init.d\/unravel_all.sh start Running scripts The tags computed in the Python script feed into Unravel core ETL pipeline. The Python script is invoked in the ingestion pipeline and is set up to access application metadata to create tags on the fly. The first time an application is invoked and running it is not listed when applications are filtered by tags. Debug and print statements are logged multiple times as the script is invoked multiple times over a run. References You can download example tagging scripts . (This is currently private; please contact Unravel Support .) " }, 
{ "title" : "Writing a Python script", 
"url" : "102385-tagging-applications.html#UUID-a892142f-7675-778c-76d6-83462266b232_N1552802203691", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Tagging \/ Tagging applications \/ Writing a Python script", 
"snippet" : "You can add print\/debugging statements to the script, but they are logged each time the script is run. Consequently, there are numerous\/duplicated entries as the script is invoked multiple times during an application's run. You can also specify workflow tags in your script. Format In the Python scri...", 
"body" : "You can add print\/debugging statements to the script, but they are logged each time the script is run. Consequently, there are numerous\/duplicated entries as the script is invoked multiple times during an application's run. You can also specify workflow tags in your script. Format In the Python script, you set a tag_key to a tag_value . Your tag_value can be a string, the return value of a method, or a concatenation of both. tag[\"auth\"]=\"admin\" tag[\"scope\"]=app_obj.getAppQueue() tags[\"dept\"]=app_obj.getAppName() + \"_\" + app_obj.getQueue() Field\/Description Where generated Method app_id Application ID. Hadoop app_obj.getAppId() app_name Application name. Hadoop app_obj.getAppName() app_type Application type: mr, spark, hive, impala, or tez. Unravel app_obj.getAppType() cluster_id The cluster the the app is running on. Note : This is not fully supported. When Unravel cannot obtain the cluster_id it's set to default . Hadoop app_obj.getClusterId() queue The queue the app is running in. Hadoop app_obj.getQueue() username Application's owner. Hadoop app_obj.getUsername() The following four (4) methods are only valid for Hive related apps, i.e., Hive, Hive-on-Spark, Hive-on-Tez, and Hive-on-MR. For all other apps these are undefined. Returns the real user's name, as opposed to hdfs, etc. Hadoop app_obj.getRealUser() List of name of the database(s) connected while running SQL queries. Hadoop app_obj.getDb() List of input tables in SQL queries. This list can be empty. Hadoop app_obj.getInputTables() List of output tables in SQL queries. This list can be empty Hadoop app_obj.getOutputTables() For Hive on MapReduce applications the following property is available. (This method is currently unavailable for Spark or Tez.) getAppConf(\" parameter\" ) Any field which exists within the MR configuration object. e.g., app_obj.getAppConf (“hive.query.id”) returns hive_123490cad app_obj.getAppConf(\" parameter_name \") " }, 
{ "title" : "Example Python script", 
"url" : "102385-tagging-applications.html#UUID-a892142f-7675-778c-76d6-83462266b232_N1552802253349", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Tagging \/ Tagging applications \/ Example Python script", 
"snippet" : "The following script creates seven tag_keys for applications and then populates them, generating the tagging dictionary. hive_query_id dept team auth scope unravel.workflow.name , and unravel.workflow.utctimestamp (See tagged workflows .) The tagging properties are set to the script file and method ...", 
"body" : "The following script creates seven tag_keys for applications and then populates them, generating the tagging dictionary. hive_query_id dept team auth scope unravel.workflow.name , and unravel.workflow.utctimestamp (See tagged workflows .) The tagging properties are set to the script file and method name. com.unraveldata.app.tagging.script.path-=\/usr\/scripts\/Tagging.py\ncom.unraveldata.app.tagging.script.method.name-=get_tags # filename: \/usr\/scripts\/Tagging.py\n\nfrom datetime import datetime\n\n# get_tags is the method so com.unraveldata.app.tagging.script.method.name=get_tags \ndef get_tags(app_obj):\n\n tags = {}\n\n# MR apps get the hive_query_id tag\n if app_obj.getAppType() == \"mr\":\n tags[\"hive_query_id\"] = app_obj.getAppConf(\"hive.query.id\")\n\n# every app gets a dept and team tag\n tags[\"dept\"] = app_obj.getAppName() + \"_\" + app_obj.getQueue()\n tags[\"team\"] = app_obj.getUsername()\n\n# Only apps with username=admin get this tag\n if app_obj.getUsername() == \"admin\": \n tags[\"auth\"] = \"admin\"\n\n# Every app gets a scope tag based upon queue they are in\n if app_obj.getQueue() == \"engr\":\n # All apps in the \"engr\" queue get this tag\n tags[\"scope\"] = \"engineering-application\"\n elif app_obj.getQueue() == \"qa\":\n # All apps in the \"qa\" queue get this tag\n tags[\"scope\"] = \"qa-application\"\n else:\n # All apps not in the\"engr\" or \"qa\" queues get this tag\n tags[\"scope\"] = \"daily-application\"\n\n# creates the workflow tags, these are Unravel tags and you should contact support@unraveldata.com before using them\n tags[\"unravel.workflow.name\"] = \"Workflow-\" + tags[\"team\"] \n tags[\"unravel.workflow.utctimestamp\"] = app_obj.getAppType() + \"-\" + str(datetime.utcnow())\n\n\n return tags " }, 
{ "title" : "Tagging workflows", 
"url" : "102386-tagging-workflows.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Tagging \/ Tagging workflows", 
"snippet" : "About Unravel workflow tags You can add two Unravel tags (<key, value> pairs) to mark queries and jobs that belong to a particular workflow: unravel.workflow.name : a string that represents the name of the workflow. Recommended format is TenantName-ProjectName-WorkflowName . unravel.workflow.utctime...", 
"body" : "About Unravel workflow tags You can add two Unravel tags (<key, value> pairs) to mark queries and jobs that belong to a particular workflow: unravel.workflow.name : a string that represents the name of the workflow. Recommended format is TenantName-ProjectName-WorkflowName . unravel.workflow.utctimestamp : a timestamp in yyyyMMddThhmmssZ format that represents the logical time of a run of the workflow in UTC\/ISO format. In UNIX\/LINUX bash. You can get a timestamp in UTC format by running the command \" $(date -u '+%Y%m%dT%H%M%SZ') \". Do not put quotes (\"\") or blank spaces in\/around the tag keys or values. For example: SET unravel.workflow.name=\"ETL-Workflow; [Incorrect syntax] SET unravel.workflow.name=ETL-Workflow; [Correct syntax] Different runs of the same workflow have the same value for unravel.workflow.name but different values for unravel.workflow.utctimestamp . Different workflows have different values for unravel.workflow.name . Hive query example This is a Hive query that was marked as part of the Financial-Tenant-ETL-Workflow workflow that ran on February 1, 2016: SET unravel.workflow.name=Financial-Tenant-ETL-Workflow;\nSET unravel.workflow.utctimestamp=20160201T000000Z;\nSELECT foo FROM table WHERE … Your Hive Query text goes here Easy recipes for tagging workflows Export the workflow name and UTC timestamp from your top-level script that schedules each run of the workflow. Here, we use bash 's date command to generate the timestamp. export WORKFLOW_NAME=Financial-Tenant-ETL-Workflow export UTC_TIME_STAMP=$(date -u '+%Y%m%dT%H%M%SZ') Follow the instructions for your job type. Hive on MR query Hive on Tez query Sqoop job Direct MapReduce job Spark job Pig job Impala Job Examples by job type Finding workflows in Unravel web UI Once your tagged workflows have been run, go log into Unravel Web UI and select Applications | Workflows to start exploring Unravel's Workflow Management features. " }, 
{ "title" : "Hive on MR query using SET commands in Hive", 
"url" : "102386-tagging-workflows.html#UUID-4fcbb561-f999-2182-4c9b-384729998315_N1554683383875", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Tagging \/ Tagging workflows \/ Hive on MR query using SET commands in Hive", 
"snippet" : "hive -f hive\/simple_wf.hql In hive\/simple_wf.hql : SET unravel.workflow.name=Financial-Tenant-ETL-Workflow; SET unravel.workflow.utctimestamp=20160201T000000Z; SELECT foo FROM table WHERE … Your Hive Query text goes here...", 
"body" : "hive -f hive\/simple_wf.hql In hive\/simple_wf.hql : SET unravel.workflow.name=Financial-Tenant-ETL-Workflow; \nSET unravel.workflow.utctimestamp=20160201T000000Z;\nSELECT foo FROM table WHERE … Your Hive Query text goes here " }, 
{ "title" : "Sqoop job using –D command line parameters", 
"url" : "102386-tagging-workflows.html#UUID-4fcbb561-f999-2182-4c9b-384729998315_N1554683433104", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Tagging \/ Tagging workflows \/ Sqoop job using –D command line parameters", 
"snippet" : "sqoop export \\ -D\"unravel.workflow.name=$WORKFLOW_NAME\" -D\"unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" \\ --connect jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod --table settings -m 1 \\ --export-dir \/tmp\/sqoop_test --username unravel --verbose --password foobar Sqoop has bugs related to quotes ....", 
"body" : "sqoop export \\\n -D\"unravel.workflow.name=$WORKFLOW_NAME\" -D\"unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" \\\n --connect jdbc:mysql:\/\/127.0.0.1:3316\/unravel_mysql_prod --table settings -m 1 \\\n --export-dir \/tmp\/sqoop_test --username unravel --verbose --password foobar\n Sqoop has bugs related to quotes . " }, 
{ "title" : "Direct MapReduce job using –D command line parameters", 
"url" : "102386-tagging-workflows.html#UUID-4fcbb561-f999-2182-4c9b-384729998315_N1554683447404", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Tagging \/ Tagging workflows \/ Direct MapReduce job using –D command line parameters", 
"snippet" : "Substitute your file name for \/tmp\/data\/small and \/tmp\/outsmoke . hadoop jar libs\/ooziemr-1.0.jar com.unraveldata.mr.apps.Driver \\ -D\"unravel.workflow.name=$WORKFLOW_NAME\" -D\"unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" \\ -p \/wordcount.properties -input \/tmp\/data\/small -output \/tmp\/outsmoke...", 
"body" : "Substitute your file name for \/tmp\/data\/small and \/tmp\/outsmoke . hadoop jar libs\/ooziemr-1.0.jar com.unraveldata.mr.apps.Driver \\\n-D\"unravel.workflow.name=$WORKFLOW_NAME\" -D\"unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" \\\n-p \/wordcount.properties -input \/tmp\/data\/small -output \/tmp\/outsmoke " }, 
{ "title" : "Spark job using --conf command line parameters", 
"url" : "102386-tagging-workflows.html#UUID-4fcbb561-f999-2182-4c9b-384729998315_N1554683460679", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Tagging \/ Tagging workflows \/ Spark job using --conf command line parameters", 
"snippet" : "For Spark jobs, you must prefix the Unravel tags with \" spark. \". For example, unravel.workflow.name becomes spark.unravel.workflow.name . spark-submit \\ --conf \"spark.unravel.workflow.name=$WORKFLOW_NAME\" --conf \"spark.unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" --conf \"spark.eventLog.enabled=tr...", 
"body" : "For Spark jobs, you must prefix the Unravel tags with \" spark. \". For example, unravel.workflow.name becomes spark.unravel.workflow.name . spark-submit \\\n --conf \"spark.unravel.workflow.name=$WORKFLOW_NAME\" \n --conf \"spark.unravel.workflow.utctimestamp=$UTC_TIME_STAMP\" \n --conf \"spark.eventLog.enabled=true\" \\\n --class org.apache.spark.examples.SparkPi \\\n --master yarn-cluster \\\n --deploy-mode cluster " }, 
{ "title" : "Pig job using –param and SET commands", 
"url" : "102386-tagging-workflows.html#UUID-4fcbb561-f999-2182-4c9b-384729998315_N1554683473560", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Tagging \/ Tagging workflows \/ Pig job using –param and SET commands", 
"snippet" : "pig \\ -param WORKFLOW_NAME=$WORKFLOW_NAME -param UTC_TIME_STAMP=$UTC_TIME_STAMP \\ -x mapreduce -f pig\/simple.pig In pig\/simple.pig : SET unravel.workflow.name $WORKFLOW_NAME; SET unravel.workflow.utctimestamp $UTC_TIME_STAMP; lines = LOAD '\/tmp\/data\/small' using PigStorage('|') AS (line:chararray); ...", 
"body" : "pig \\\n-param WORKFLOW_NAME=$WORKFLOW_NAME -param UTC_TIME_STAMP=$UTC_TIME_STAMP \\\n-x mapreduce -f pig\/simple.pig In pig\/simple.pig : SET unravel.workflow.name $WORKFLOW_NAME; \nSET unravel.workflow.utctimestamp $UTC_TIME_STAMP; \nlines = LOAD '\/tmp\/data\/small' using PigStorage('|') AS (line:chararray); \nwords = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) as word; \ngrouped = GROUP words BY word; \nwordcount = FOREACH grouped GENERATE group, COUNT(words); DUMP wordcount; " }, 
{ "title" : "Impala job using SET commands", 
"url" : "102386-tagging-workflows.html#UUID-4fcbb561-f999-2182-4c9b-384729998315_N1554683484495", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Tagging \/ Tagging workflows \/ Impala job using SET commands", 
"snippet" : "impala-shell -i <impald_host:port> \\ -f simpleImpala.sql \\ --var=workflowname='ourImpalaWorkflow' \\ --var=utctimestamp=$(date -u '+%Y%m%dT%H%M%SZ') In ..\/simpleImpala.sql : SET DEBUG_ACTION=\"::::unravel.workflow.name::${var:workflowname}::::unravel.workflow.utctimestamp::${var:utctimestamp}::::\"; se...", 
"body" : "impala-shell -i <impald_host:port> \\\n -f simpleImpala.sql \\\n --var=workflowname='ourImpalaWorkflow' \\\n --var=utctimestamp=$(date -u '+%Y%m%dT%H%M%SZ') In ..\/simpleImpala.sql : SET \n DEBUG_ACTION=\"::::unravel.workflow.name::${var:workflowname}::::unravel.workflow.utctimestamp::${var:utctimestamp}::::\"; \n select * from usstates;; " }, 
{ "title" : "Tagging a Hive on Tez query", 
"url" : "102387-tagging-workflows-hive-on-tez.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Tagging \/ Tagging a Hive on Tez query", 
"snippet" : "For general information see Tagging Workflows . The following properties must be set in \/usr\/local\/unravel\/etc\/unravel.properties . You should adjust the script path and method name parameters according to your cluster setup. com.unraveldata.tagging.script.enabled=true com.unraveldata.app.tagging.sc...", 
"body" : "For general information see Tagging Workflows . The following properties must be set in \/usr\/local\/unravel\/etc\/unravel.properties . You should adjust the script path and method name parameters according to your cluster setup. com.unraveldata.tagging.script.enabled=true\ncom.unraveldata.app.tagging.script.path= \/usr\/local\/unravel\/etc\/tag_app.py \ncom.unraveldata.app.tagging.script.method.name= get_tags You can create tagged workflows for Tez apps in four ways. Use --hiveconf via hive command. Enter the following the hive command line. hive --hiveconf unravel.workflow.name=my_tez_workflow --hiveconf unravel.workflow.utctimestamp=20180801T000001Z -f tez.sql\n Sample tez.sql . set hive.execution.engine=tez;\nselect count(*) from my_test_table; Use the global Python script for app tagging. Assuming the global script is \/tmp\/tag_app.py , you would add the two workflow tags to the object returned from the main method. Use --hiveconf via beeline command. Enter the following command in the Beeline command line. > beeline -n hive -u 'jdbc:hive2:\/\/ host2.unraveldata.com :10000' --hiveconf unravel.workflow.name=my_tez_workflow --hiveconf unravel.workflow.utctimestamp=20180801T000001Z -f tez.sql\n Use the tez.sql script, then run Beeline. You must define these the two workflow tags in tez.sql : set hive.execution.engine=tez;\nset unravel.workflow.name=my_tez_workflow;\nset unravel.workflow.utctimestamp=20180801T000001Z;\nselect count(*) from my_test_table; Enter the following command in the beeline command line. > beeline -n hive -u 'jdbc:hive2:\/\/ host2.congo6.unraveldata.com :10000'-f tez.sql\n " }, 
{ "title" : "Unravel properties", 
"url" : "102388-properties-page.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties", 
"snippet" : "Properties, unless otherwise noted, are located in \/usr\/local\/unravel\/etc\/unravel.properties . To locate the properties within a file, search on property name.   Properties are typically loaded with a default value. You might need to change the property's default value based upon your environment\/in...", 
"body" : "Properties, unless otherwise noted, are located in \/usr\/local\/unravel\/etc\/unravel.properties . To locate the properties within a file, search on property name.   Properties are typically loaded with a default value. You might need to change the property's default value based upon your environment\/installation. Installation and configuration topics note when you need to set properties to enable specific functionality. Set by user The Set by user column denotes whether you must set the property. The column contains: Required means you must set a value. Optional means you don't need to set a value. Optional properties can become required depending on your environment . For instance, if you want to enable Forecasting and Migration Reports you need to set the cluster properties. Installation and configuration topics note when you need to set optional properties to enable specific functionality. Do not put quotes around the values; this includes all strings, CSL, etc. Unit abbreviations boolean : true or false count : whole number ColSL : colon separated (delimited) list CSL : comma separated (delimited) list min : minutes ms : milliseconds ns : nanoseconds s : seconds path : fully qualified directory path, e.g., \/tmp\/dir\/lower percent : percentage, e.g., 1.2, .5 set member : member of the allowed set of values, i.e., HDP or HDP string : a string of characters, e.g., admin, yarn, password URL : valid IP, or fully qualified domain name " }, 
{ "title" : "Basic", 
"url" : "102389-properties-basic.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Basic", 
"snippet" : "JBDC These properties define the connection to Unravel's MySQL database. Property\/Description Set by user Unit Default unravel.jdbc.username Unravel database user. Required string - unravel.jdbc.password Password for unravel.jdbc.username . Required string - unravel.jdbc.url URL for jdbc, determined...", 
"body" : "JBDC These properties define the connection to Unravel's MySQL database. Property\/Description Set by user Unit Default unravel.jdbc.username Unravel database user. Required string - unravel.jdbc.password Password for unravel.jdbc.username . Required string - unravel.jdbc.url URL for jdbc, determined by your database. Example: jdbc:mysql:\/\/127.0.0.1:3306\/unravel_mysql_prod Required string (path) - Login properties Property\/Description Set by user Unit Default com.unraveldata.login.admins Unravel UI admin. Set during installation. string admin com.unraveldata.login.admins.readonly List of read-only admins. Optional CSL - com.unraveldata.login.mode Mode to use for login. ldap : uses ldap entries for login. saml : uses saml for login. open : users logs directly into Unravel UI.   string open General Property\/Description Set by user Unit Default com.unraveldata.customer.organization Customer name. Used to identify your installation for reporting and notification purposes in Unravel UI. Optional string Not Set com.unraveldata.advertised.url Defines the Unravel Server URL for HTTP traffic. Example: http:\/\/unravelserver.company.com:3000   string http:\/\/{host}:3000 com.com.unraveldata.hdfs.timezone Timezone of HDFS, for example, US\/Eastern, Etc\/GMT-4, America\/New_York. If the timezone is not set then an error message is logged and UTC timezone is used. Possible timezones can be obtained by calling TimeZone.getAvailableIDs() . string - com.unraveldata.tmpdir The base location for Unravel process control files where Unravel's temp files reside. string (path) \/srv\/unravel\/tmp com.unraveldata.history.maxSize.weeks Number of weeks retained for search results in Elastic Search. integer 5 com.unraveldata.retention.max.days Number of days to keep the heaviest data (such as error logs and drill-down details) in the SQL Database. integer 30 HDFS, ElasticSearch, Zookeeper Property\/Description Set by user Unit Default com.unraveldata.hive.hdfs.dir \/user\/unravel\/HOOK_RESULT_DIR Required string - com.unraveldata.es.cluster Unravel elastic search cluster name, e.g., unravel21650. string unravel com.unraveldata.zk.quorum Embedded Zookeeper ensemble in form host1:port1,host2:port2. CSL 127.0.0.1:4181 Kerberos These properties are required when using Kerberos. Property\/Description Set by user Unit Default com.unraveldata.kerberos.principal Name of the Kerberos principal for Unravel daemons to use, along with its host name, domain name, and realm. Example: unravel\/myhost.mydomain@MYREALM string - com.unraveldata.kerberos.keytab.path Path to keytab file, on Unravel Server, corresponding to the Kerberos principal for Unravel daemons to use. Example: \/usr\/local\/unravel\/etc\/unravel.keytab You can verify the principal in a keytab by using klist -kt KETYAB_FILE . The keytab file should have chmod bits 500 and be owned by unravel local user (default) or by the user you want to use, as explained in  Run Unravel Daemons with Custom User . string Deployment location Property\/Description Set by user Unit Default com.unraveldata.onprem Specifies whether the deployment is on premise or on cloud. Important For Azure Databricks, EMR, and HDInsight set to False   boolean true com.unraveldata.supported.platforms Defines the cloud platform when com.unraveldata.onprem =false. Value = EMR | HDI set member - " }, 
{ "title" : "Executor logs", 
"url" : "102390-properties-basic-executor.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Executor logs", 
"snippet" : "Property\/Description Set by user Unit Default com.unraveldata.job.collector.running.load.conf When set to true Running MR jobs are linked to corresponding Hive app if Hive-on-MR app. Auto Action metrics for running hive queries will be sent to AA2 backend. boolean false com.unraveldata.job.collector...", 
"body" : "Property\/Description Set by user Unit Default com.unraveldata.job.collector.running.load.conf When set to true Running MR jobs are linked to corresponding Hive app if Hive-on-MR app. Auto Action metrics for running hive queries will be sent to AA2 backend. boolean false com.unraveldata.job.collector.hive.queries.cache.size This is used to improve the Hive-MR pipeline by caching data so it can be retrieved from cache instead of external API. You should not have to change this value. count 1000 com.unraveldata.max.attempt.log.dir.size.in.bytes Maximum size of the aggregated executor log that are imported and processed by the Spark worker for a successful application. byte 500000000 (~500 MB) com.unraveldata.max.failed.attempt.log.dir.size.in.bytes Maximum size of the aggregated executor log that are imported and processed by the Spark worker for a failed application. byte 2000000000 (~2 GB) com.unraveldata.min.job.duration.for.attempt.log Minimum duration of a successful application or which executor logs are processed (in milliseconds). ms 600000 (10 mins) com.unraveldata.min.failed.job.duration.for.attempt.log Minimum duration of failed\/killed application for which executor logs are processed (in milliseconds). ms 60000 com.unraveldata.attempt.log.max.containers Maximum number of containers for the application. If application has more that configured number of containers then the aggregated executor log isprocessed for the application. ms 500 com.unraveldata.spark.master Default master for spark applications. (Used to download executor log using correct APIs.) Valid Options: yarn , mesos , standalone . string yarn " }, 
{ "title" : "HDFS logs", 
"url" : "102390-properties-basic-executor.html#UUID-c5abc9a4-0aec-0659-ad5f-2cd54c655366_section-5ce44c9aa188d-idm45742691557888", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Executor logs \/ HDFS logs", 
"snippet" : "Property\/Description Set by user Unit Default com.unraveldata.job.collector.done.log.base HDFS path to \"done\" directory of MR logs. Don't include the hdfs:\/\/ prefix For HDP set this to: \/mr-history\/done . string \/user\/history\/done com.unraveldata.job.collector.log.aggregation.base HDFS path to the a...", 
"body" : "Property\/Description Set by user Unit Default com.unraveldata.job.collector.done.log.base HDFS path to \"done\" directory of MR logs. Don't include the hdfs:\/\/ prefix For HDP set this to: \/mr-history\/done . string \/user\/history\/done com.unraveldata.job.collector.log.aggregation.base HDFS path to the aggregated container logs (logs to process). Don't include the hdfs:\/\/ prefix. The log format defaults to TFile. You can specify multiple logs and log formats (TFile or IndexedFormat). Example: TFile:\/tmp\/logs\/*\/logs\/,IndexedFormat:\/tmp\/logs\/*\/logs-ifile\/. For HDP set this to: IndexedFormat:\/app-logs\/*\/logs\/ . CSL \/tmp\/logs\/*\/logs\/ com.unraveldata.spark.eventlog.location Comma-separated list of HDFS paths to the Spark event logs. Each path must include the hdfs:\/\/\/ prefix. For HDP set this to: hdfs:\/\/\/spark1-history\/,hdfs:\/\/\/spark2-history\/ . CSL hdfs:\/\/\/user\/spark\/applicationHistory\/ " }, 
{ "title" : "Airflow", 
"url" : "102391-properties-airflow.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Airflow", 
"snippet" : "Property\/Description Set by user Unit Default com.unraveldata.airflow.available Notes if the airflow is currently available. false : not available true : available boolean false airflow.look.back.num.days The number of days to look back. The look-back days can be specified as a positive or negative ...", 
"body" : "Property\/Description Set by user Unit Default com.unraveldata.airflow.available Notes if the airflow is currently available. false : not available true : available boolean false airflow.look.back.num.days The number of days to look back. The look-back days can be specified as a positive or negative number. For instance -5 or 5 sets the look back number of days to 5. count 1 airflow.look.back.num.hours The look-back time window in hours, which can be either a positive or negative integer. If present, it takes precedence over airflow.look.back.num.days to have finer granularity. Suggested value for large clusters: 2 count 24 com.unraveldata.airflow.http.max.body.size.byte Set maximum number of bytes Unravel fetches data from Airflow Web UI. Default unlimited. bytes 0 com.unraveldata.airflow.login.name Airflow UI login username. You must set this if airflow.server.url = https. Required string - com.unraveldata.airflow.login.password Password for Airflow UI com.unraveldata.airflow.login.name . You must set set this if airflow.login.name is set. Required string - com.unraveldata.airflow.protocol Type of connection, e.g., HTTPS or HTTP. You must set the airflow.login.name and airflow.login.password when this value is https. https com.unraveldata.airflow.server.url Full URL of the airflow server, starting with http:\/\/ and https:\/\/ . url http:\/\/localhost:10080 com.unraveldata.airflow.status.timeout.sec Set Airflow workflow status timeout in Unravel. sec 3600 com.unraveldata.airflow.task.log.parsing.enabled Controls whether to parse the Airflow Task logs. These logs are used to populate the Workflow Instance entities in the \"Jobs - Workflows\" page. boolean true com.unraveldata.airflow.task.log.parsing.operators Controls the Task logs to parse based on the Operator that produced it. Since Unravel only derives insights for Hive, Spark, and MR applications, it is set to only analyze operators that can launch those types of jobs. The values are delimited using a \",\" and it treats \"*\" as a wildcard to many any or no characters. Any special characters like \"\\\" or \".\" will be removed. string BashOperator, PythonOperator, *Hive*, *Spark* com.unraveldata.airflow.task.thread.pool.size Controls whether to process the Airflow Task logs sequentially or in parallel. Process the logs in parallel improves performance. This config takes effect only if airflow.task.log.parsing.enabled =true. Possible values are 1-16. If one, logs are processed sequentially. If greater than one, logs are processed in parallel using a Thread Pool with of the size specified value. count 1 " }, 
{ "title" : "AutoAction", 
"url" : "102392-properties-autoactions.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ AutoAction", 
"snippet" : "Property\/Description Set by user Unit Default com.unraveldata.auto.action.default.snooze.period.ms The time repeated violations are to be ignored for the violator, for example, app, user. If the violation is still occurring when awakened the AutoAction executes the action and the violator is once ag...", 
"body" : "Property\/Description Set by user Unit Default com.unraveldata.auto.action.default.snooze.period.ms The time repeated violations are to be ignored for the violator, for example, app, user. If the violation is still occurring when awakened the AutoAction executes the action and the violator is once again snoozed. An AutoAction containing a kill or move action is never snoozed. 0: snooze is turned off > 0: snooze is on, there is no upper bound ms 3600000 (1 hour) com.unraveldata.auto.action.history.chart.width.ms com.unraveldata.auto.action.include.customer.org com.unraveldata.auto.action.metric.discard.ms The AutoAction discards the arriving metrics messages based on this value, i.e., metrics which arrive after this value ignored. Metrics messages which were published before (current time - (value)) are retained, i.e., once a metric is published this value is irrelevant. ms 3600000 (1 hour) com.unraveldata.auto.action.policy.enforce.period.ms Maximum wait period for policy enforcement to be triggered. ms 180000 (3 min) com.unraveldata.auto.action.publish.finish.apps.metrics.enabled Publish non-running application metrics to auto-action so that auto-action rules will be triggered. boolean false com.unraveldata.auto.action.publish.internal.metrics.enabled Enables the ability to receive alerts when an application runs over. true : enables alerts false : disables alerts boolean true com.unraveldata.auto.action.subject.items.max.count com.unraveldata.smart.auto.action.enabled AutoAction daemon Property\/Description Set by user Unit Default com.unraveldata.auto.action.enable.policy.enforce.in.jcs Enables AutoActions to be processed under the legacy (pre-4.5.2.0) mode of operation when AutoAction enforcement resided in JCS2 daemon. This works for on-prem mode only. When using this mode you can stop AutoAction daemon. boolean false com.unraveldata.auto.action.max.buffered.group.count A message group represents one complete YARN metrics polling cycle. It is incomplete when it has missing messages, messages out of order, etc. This property controls how many message groups can be buffered while waiting for completion of the current group. Once the daemon has buffered the max.buffered.group.count , it either drops or accepts the incomplete group (see max.lost.messages.count ) and moves onto processing the next group. Setting this value higher than 1 may increase latency of enforcement of an Auto Actions but can help to alleviate errors in message transport protocol. count 1 com.unraveldata.auto.action.max.lost.messages.count Maximum number of lost messages a group can have and still be be accepted for aggregation and policy enforcement. Accepting incomplete groups lowers the consistency of triggered violations but lets the daemon to operate with an “unstable” connection. 0: only complete groups are accepted. > 0: groups missing up to X messages are accepted. count 0 com.unraveldata.auto.action.metric.discard.ms When a incoming metric's timestamp is older than this value it is not processed but discarded. This mechanism is designed to deal with Kafka latency, consumer lags and other message delivery delays. It prevents the daemon from acting on outdated data and issuing false-positive violation events that are irrelevant at the present time ms 3600000 (1 hour) com.unraveldata.auto.action.policy.db.update.skip.cycle Defines how often the daemon refreshes the AutoAction policy definitions when aggregating and enforcing per enforcement cycle (see enforce.period.ms ). 0: All policies are read every time the daemon's ready to aggregate and enforce policies. > 0: The policies are read every X cycle. For instance, 1: the daemon reads them every other cycle, 2: every third period and so on. Changing this value makes sense only if there are a lot of policies defined. count 0 com.unraveldata.auto.action.policy.enforce.period.ms Maximum wait period for policy enforcement to be triggered. AutoAction metric aggregation and enforcement cycle is driven by metric producers, i.e., the daemon synchronizes with the metric polling cycles for each monitored cluster. When no YARN metrics are delivered or accepted and therefore policies are not getting evaluated for longer than this period, the daemon will forcibly execute metric aggregation and policy enforcement to process other internal Unravel metrics, such as Workflow, Hive, Tez, Impala metrics. ms 180000 (3 min) com.unraveldata.auto.action.transport.receiver.ttl.ms Identifies how long a transport protocol connection remains active without receiving any data from the cluster. If no metrics are received for longer than this value, the cluster is considered terminated and connection is closed on the receiver (daemon's) side. ms 1800000 (30 minutes) " }, 
{ "title" : "Azure", 
"url" : "102393-properties-azure-452x.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Azure", 
"snippet" : "ABFS These properties are required if you are using an Azure Data Lake Storage Gen2 account. Property\/Description Set by user Unit Default com.unraveldata.azure.storage.abfs.account-name. X Name of the ABFS storage account that the HDInsight cluster uses. You must define this property for each ABFS ...", 
"body" : "ABFS These properties are required if you are using an Azure Data Lake Storage Gen2 account. Property\/Description Set by user Unit Default com.unraveldata.azure.storage.abfs.account-name. X Name of the ABFS storage account that the HDInsight cluster uses. You must define this property for each ABFS storage account. X . X=1 for the first storage account and then is incremented by one for each new account, that is, account numbers must be consecutive. Optional string Azure storage account name. (See finding the storage name .) com.unraveldata.azure.storage.abfs.access-key. X The access key for the corresponding ABFS storage account. Optional string Azure storage account name. (See finding the secret (access key) .) You must define these two properties for each storage account name. X starts with 1 and for each new account is incremented by 1 for each new storage account, that is, the set of properties must be consecutively numbered, (1, 2, 3, ...). For example, if you have two storage accounts you would define two sets. com.unraveldata.azure.storage.abfs.account-name.1=abfsAccountName1\ncom.unraveldata.azure.storage.abfs.access-key.1=abfsAccessKey1\ncom.unraveldata.azure.storage.abfs.account-name.2=abfsAccountName2\ncom.unraveldata.azure.storage.abfs.access-key.2=abfsAccessKey2 Data Lake These properties are required if you are using an Azure Data Lake Storage. Property\/Description Set by user Unit Default com.unraveldata.azure.storage.adl.account-name. X The Azure Data Lake Gen1 storage account. The name does not need to be fully qualified. For instance, you can use mydatalake or mydatalake.azuredatalakestore.net . You must define this property for each storage account. X starts with 1 and then is incremented by 1 for each additional account. The account numbers must be consecutive. Optional string Azure storage account name. (See finding the storage name .) com.unraveldata.azure.storage.adl.client-id. X An application ID. An application registration has to be created in the Azure Active Directory. Optional string Azure application id. (See finding the application Id .) com.unraveldata.azure.storage.adl.client-key. X An application's \"secret\" (key) described in the ADL Gen1 client-id field. Optional string Azure storage secret. (See finding the secret (access key) .) com.unraveldata.azure.storage.adl.access-token-endpoint. X The OAUTH 2.0 Access Token Endpoint. It is obtained from the application registration tab on Azure portal. Optional string Azure OAUTH 2.0 token endpoint (See finding the OAUTH endpoint .) You must define these four properties for each storage account name. X starts with 1 and is incremented by 1 for each new storage account, that is, the set of properties must be consecutively numbered, (1, 2, 3, ...). For example, if you have two storage accounts you would define two sets. com.unraveldata.azure.storage.adl.account-name.1=adlAccountName1\ncom.unraveldata.azure.storage.adl.client-id.1=adlClientId1\ncom.unraveldata.azure.storage.adl.client-key.1=adlClientKey1\ncom.unraveldata.azure.storage.adl.access-token-endpoint.1=adlTokenEndpoint1\ncom.unraveldata.azure.storage.adl.account-name.2=adlAccountName2\ncom.unraveldata.azure.storage.adl.client-id.2=adlClientId2\ncom.unraveldata.azure.storage.adl.client-key.2=adlClientKey2\ncom.unraveldata.azure.storage.adl.access-token-endpoint.2=adlTokenEndpoint2 4.5.0.5 Property\/Description Set by user Unit Default com.unraveldata.adl.accountFQDN The data lake's fully qualified domain name, for example, mydatalake.azuredatalakestore.net. Optional string Azure storage account name. (See finding the storage name .) com.unraveldata.adl.clientId An application ID. An application registration has to be created in the Azure Active Directory. Optional string Azure application id. (See finding the application Id .) com.unraveldata.adl.clientKey An application access key which can be created after registering an application. Optional string Azure storage access key. (See finding the storage access key .) com.unraveldata.adl.accessTokenEndpoint The OAUTH 2.0 Access Token Endpoint. It is obtained from the application registration tab on Azure portal. Optional string Azure OAUTH 2.0 token endpoint (See finding the OAUTH endpoint .) com.unraveldata.adl.clientRootPath The path in the Data lake store where the target cluster has been given access. Optional string URL Azure CONTAINER\/DIRECTORY path for storage account name. (See finding the container path .) Databricks Property\/Description Set by user Unit Default com.unraveldata.databricks.HTTP.conn.timeout Databricks HTTP connection timeout in seconds. s 1000 com.unraveldata.databricks.http.read.timeout Databricks http read timeout in seconds. s 6000 com.unraveldata.databricks.http.poll.parallelism Databricks poll parallelism; expressed as a fraction of the total cores. percent .75 com.unraveldata.databricks.workspaces Databricks workspace to monitor. CSL - WASB\/HDInsight These properties are required if you are using a WASB storage account. Property\/Description Set by user Unit Default com.unraveldata.azure.storage.wasb.account-name. X Name of the WASB storage account that the HDInsight cluster uses. You must define this property for each WASB storage account. X . X=1 for the first storage account and the is incremented by one for each new account, that is, account numbers must be consecutive. Optional string Azure storage account name. (See finding the storage name .) com.unraveldata.azure.storage.wasb.access-key. X WASB storage account key. For each storage account defined you must define the storage access key. If you have two keys, pick one to use here. Optional string Azure storage account access key. (See finding the access key .) You must define these two properties for each storage account name. X starts with 1 and is incremented by 1 for each new storage account, that is, the set of properties must be consecutively numbered (1, 2, 3, ...). For example, if you have three storage accounts you would define three sets. com.unraveldata.azure.storage.wasb.account-name.1=Storage1\ncom.unraveldata.azure.storage.wasb.access-key.1=Storage1AccessKey\ncom.unraveldata.azure.storage.wasb.account-name.2=Storage2\ncom.unraveldata.azure.storage.wasb.access-key.2=Storage2AccessKey\ncom.unraveldata.azure.storage.wasb.account-name.3=Storage3\ncom.unraveldata.azure.storage.wasb.access-key.3=Storage3AccessKey 4.5.0.5 Property\/Description Set by user Unit Default com.unraveldata.hdinsight.storage-account. X Storage account name that a HDInsight cluster uses. You must define this property for each storage account. X starts with 1 and then is incremented by 1 for each additional account. The account numbers must be consecutive. Optional string Azure storage account name. (See finding the storage name .) com.unraveldata.hdinsight.access-key. X Storage account key. For each storage-account. X you must define access-key. X If you have two access keys, pick one to use here. Optional string Azure storage account key. (See finding the access key .) You must define these two properties for each storage account name. X starts with 1 and is incremented by 1 for each new storage account, that is, the set of properties must be consecutively numbered (1, 2, 3, ...). For example, if you have three storage accounts you would define three sets. \ncom.unraveldata.hdinsight.storage-account.1=Storage1\ncom.unraveldata.hdinsight.access-key.1=Storage1AccessKey\ncom.unraveldata.hdinsight.storage-account.2=Storage2\ncom.unraveldata.hdinsight.access-key.2=Storage2AccessKey\ncom.unraveldata.hdinsight.storage-account.3=Storage3\ncom.unraveldata.hdinsight.access-key.3=Storage3AccessKey\n " }, 
{ "title" : "Btrace", 
"url" : "102394-properties-btrace.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Btrace", 
"snippet" : "EMDB Property\/Description Set by user Unit Default unravel.emdb.jdbc.driver Driver name. string org.postgresql.Driver unravel.emdb.jdbc.url string (URL) - unravel.emdb.jdbc.user string unravel unravel.emdb.jdbc.password string - unravel.emdb.path.bin Path to binary executables for embedded PostgreSQ...", 
"body" : "EMDB Property\/Description Set by user Unit Default unravel.emdb.jdbc.driver Driver name. string org.postgresql.Driver unravel.emdb.jdbc.url string (URL) - unravel.emdb.jdbc.user string unravel unravel.emdb.jdbc.password string - unravel.emdb.path.bin Path to binary executables for embedded PostgreSQL string \/usr\/local\/unravel\/pgsql\/bin Metrics Property\/Description Set by user Unit Default com.unraveldata.metrics.backup.dir Parent directory for database backup files. string \/srv\/unravel\/metBkup com.unraveldata.metrics.doArchive When true , archive retired partitions. boolean false com.unraveldata.metrics.loadLatency.sec Latency for loading metrics of a job. number 600 com.unraveldata.metrics.loadfile.dir Parent directory for metric load files. string \/srv\/unravel\/metLoad com.unraveldata.metrics.partition.cnt Partitions available for future metrics. number 4 com.unraveldata.metrics.partition.sec Duration of a partition (1 week). number 604800 com.unraveldata.metrics.retention.sec Tthreshold to retire partitions (1 month). s 2592000 com.unraveldata.metrics.statFrequency.sec How often to write load statistics to log. number 600 " }, 
{ "title" : "Celery", 
"url" : "102395-properties-celery.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Celery", 
"snippet" : "Property\/Description Set by user Unit Default com.unraveldata.ngui.proxy.celery string (URL) http:\/\/localhost:5000 unravel.celery.broker.url Optional string (URL) - unravel.celery.result.backend Optional -...", 
"body" : "Property\/Description Set by user Unit Default com.unraveldata.ngui.proxy.celery string (URL) http:\/\/localhost:5000 unravel.celery.broker.url Optional string (URL) - unravel.celery.result.backend Optional - " }, 
{ "title" : "Cluster", 
"url" : "102396-properties-cluster.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Cluster", 
"snippet" : "Make sure the cluster type is set correctly, the default is CDH. Property\/Description Set by user Unit Default com.unraveldata.cluster.name Cluster to connect to if multiple options exist. Required in multi-cluster environments only. Unravel first attempts to match on the cluster ID, and then falls ...", 
"body" : "Make sure the cluster type is set correctly, the default is CDH. Property\/Description Set by user Unit Default com.unraveldata.cluster.name Cluster to connect to if multiple options exist. Required in multi-cluster environments only. Unravel first attempts to match on the cluster ID, and then falls back to matching on the display name. For Ambari, the cluster ID and the display name are equivalent, which is the \"cluster_name\" attribute from the \"\/clusters\" endpoint, for example, http:\/\/HOST:8080\/api\/v1\/clusters\/. For Cloudera Manager, the cluster ID is the \"name\" attribute from the \"\/clusters\" endpoint, for example, http:\/\/HOST:7180\/api\/v17\/clusters\/. Required CSL default com.unraveldata.cluster.type Possible values are DB , CDH , HDP , or MAPR .   set member CDH " }, 
{ "title" : "Cluster manager", 
"url" : "102397-properties-cluster-manager.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Cluster manager", 
"snippet" : "These properties need to be set for Migration Planning (Formerly Cloud Reports) Forecasting Report Impala The following properties are defined by the Cluster manager tool. Be sure to only set the properties for your cluster manager tool, either Ambari or Cloudera. Property\/Description Set by user Un...", 
"body" : "These properties need to be set for Migration Planning (Formerly Cloud Reports) Forecasting Report Impala The following properties are defined by the Cluster manager tool. Be sure to only set the properties for your cluster manager tool, either Ambari or Cloudera. Property\/Description Set by user Unit Default com.unraveldata.ambari.manager.url URL of Ambari Manager, for example, https:\/\/$ambariserver:8083. It must start with http:\/\/ or https:\/\/. Required string - com.unraveldata.ambari.manager.username Ambari username. Required string - com.unraveldata.ambari.manager.password Ambari password. Required string - Property\/Description Set by user Unit Default com.unraveldata.cloudera.manager.url URL of Cluster Manager, for example, http:\/\/$clouderaserver In order to properly track Impala jobs, make sure that the value of this property does not contain a port number since there is already a separate config for the port. Required string - com.unraveldata.cloudera.manager.username Cloudera manager username. Required string - com.unraveldata.cloudera.manager.password Cloudera manager password. Required string - com.unraveldata.cloudera.manager.port You only need to specify this if your Cloudera Manager is not on port 7180. As of c4.5.4.0 this property is deprecated and the only way to specifiy the CM URL is with com.unraveldata.cloudera.manager.url . Required integer - com.unraveldata.cloudera.manager.api_version Optional and only valid for Cloudera Manager. Specifies the API version number to use, such as \"17\". When not set, Unravel attempts to auto discover the version within the range 16-19. Do not set this property if you are using Impala. Optional integer - " }, 
{ "title" : "Custom banner", 
"url" : "102398-properties-custom-banner.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Custom banner", 
"snippet" : "Property\/Description Set by user Unit Default com.unraveldata.custom.banner.display Displays a banner at the top of the Unravel UI. true : banner displays text until end.date . false : no change to UI. boolean false com.unraveldata.custom.banner.text Text to display when display = true . The text an...", 
"body" : "Property\/Description Set by user Unit Default com.unraveldata.custom.banner.display Displays a banner at the top of the Unravel UI. true : banner displays text until end.date . false : no change to UI. boolean false com.unraveldata.custom.banner.text Text to display when display = true . The text and end.date must both be defined for the banner to be displayed. The banner displays the text until end.date . Optional string - com.unraveldata.custom.banner.end.date Date and Time to stop displaying the banner. There is no date\/time limit. The text and end.date must both be defined for the banner to be displayed. Format: YYYYMMDDTHHMMSSZ-000000 Optional string (date) - " }, 
{ "title" : "Data insights", 
"url" : "102399-properties-datainsights.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Data insights", 
"snippet" : "Property\/Description Set by user Unit Default com.unraveldata.datainsights.get.dfs.size Toggles the size calculations for tables and partitions in Data Insights. true : calculates the table and partition sizes. boolean true com.unraveldata.datapage.batchsize The record number to retrieve in a batch ...", 
"body" : "Property\/Description Set by user Unit Default com.unraveldata.datainsights.get.dfs.size Toggles the size calculations for tables and partitions in Data Insights. true : calculates the table and partition sizes. boolean true com.unraveldata.datapage.batchsize The record number to retrieve in a batch from the database for the Data Insights Details page. Range: 100000 ≤ value ≥ 400000. range 300000 com.unraveldata.fsimage.updates.dfs.size true : Fsimage is used to calculate sizes for Table and Partition which then populate Data Insights Table and Partition size information. As of now, this calculation is restricted to Managed Table and Partitions, i.e., those paths which are under Hive warehouse directory. This property takes precedence over com.unraveldata.hdfs.connector.updates.dfs.size . If set to true, and Hive Metastore and Small Files Report accesses are configured correctly, then no other configuration is required for FSimage to upload partition\/table size info to the Data Page boolean true com.unraveldata.hdfs.connector.updates.dfs.size Unravel will only try to fetch the sizes from HDFS if it is unable to fetch the information from Small Files. true : tries to get table and partition sizes from HDFS API\/CLI (hdfs dus). boolean true " }, 
{ "title" : "Email", 
"url" : "102400-properties-email.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Email", 
"snippet" : "Property\/Description Set by user Unit Default com.unraveldata.monitoring.alert.email.enabled Enables email alerts. true : enables alerts false : disables boolean true com.unraveldata.report.user.email.domain Default email domain used for email alerts, localhost.local. [empty] string localhost.local ...", 
"body" : "Property\/Description Set by user Unit Default com.unraveldata.monitoring.alert.email.enabled Enables email alerts. true : enables alerts false : disables boolean true com.unraveldata.report.user.email.domain Default email domain used for email alerts, localhost.local. [empty] string localhost.local mail.smtp.from Used for email \"from\" and \"reply-to\" headers. [empty] string unravel.noreply@unraveldata.com mail.smtp2.from Used for email \"from\" and \"reply-to\" headers. [empty] string unravel.noreply@unraveldata.com mail.smtp.port smtp mail port. integer 25 mail.smtp.auth Enable\/ SMTP authentication. Note : If true then mail.smtp.user and mail.smtp.pw must be set as they are used when connecting. boolean false mail.smtp.starttls.enable Use start-TLS. boolean false mail.smtp.ssl.enable Use SSL right from the start.string. boolean false mail.smtp.user Username for SMTP authentication Note : If mail.smtp.auth =true you must set this property. Optional string - mail.smtp.pw Password for SMTP authentication Note : If mail.smtp.auth =true you must set this property. Optional string - mail.smtp.host Host for SMTP server. string localhost mail.smtp.localhost A domain name for apparent sender; must have at least one dot (for example, organization.com). string localhost.local mail.smtp.debug Enable debug mode. boolean false " }, 
{ "title" : "Experimental", 
"url" : "102401-properties-experimental.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Experimental", 
"snippet" : "These properties should not to be used in production. Property\/Description Set by user Unit Default com.unraveldata.action.killmove.enabled Allows users to kill and move apps. true : users can kill and move appls false : only admins can kill and move apps. boolean true com.unraveldata.sregistry.host...", 
"body" : "These properties should not to be used in production. Property\/Description Set by user Unit Default com.unraveldata.action.killmove.enabled Allows users to kill and move apps. true : users can kill and move appls false : only admins can kill and move apps. boolean true com.unraveldata.sregistry.hostport Service Registry host:port. string ${com.unraveldata.zk.quorum} com.unraveldata.sensor.polling.secs The base polling period of Unravel reactive sensors in seconds. s 30 com.unraveldata.appevents.emitters.exclude.list Comma separated list of the application event emitter IDs which will be disabled\/excluded. CSL - com.unraveldata.multicluster.enabled Allow Unravel to operate in multi-cluster mode. In this mode a service registry will be used to discover and access all registered (local and remote) clusters. true : operate in multi-cluster mode. false : operate in single cluster mode. boolean false " }, 
{ "title" : "File reports", 
"url" : "102402-properties-file-reports.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ File reports", 
"snippet" : "Property\/Description Set by user Unit Default unravel.python.reporting.files.huge_files_threshold_size File size must be greater than or equal to the threshold_size for it to be counted. bytes 100GB unravel.python.reporting.files.huge_files_min_files The minimum number of files which must meet the t...", 
"body" : "Property\/Description Set by user Unit Default unravel.python.reporting.files.huge_files_threshold_size File size must be greater than or equal to the threshold_size for it to be counted. bytes 100GB unravel.python.reporting.files.huge_files_min_files The minimum number of files which must meet the threshold_size for the directory to be included. integer 1 unravel.python.reporting.files.huge_files_top_n_dirs Number of directories to show results for. That is only display the results for the top N directories. integer 10 unravel.python.reporting.files.medium_files_max_threshold_size The file size must be less than or equal to the max_threshold_size for it to be counted. bytes 10GB unravel.python.reporting.files.medium_files_min_threshold_size The file size must be greater than or equal to the min_threshold_size for it to be counted. integer 5GB unravel.python.reporting.files.medium_files_min_files The minimum number of files which must fall with the min and max threshold sizes for the directory to be included. integer 5 unravel.python.reporting.files.medium_files_top_n_dirs Maximum number of directories to display, that is, only display the results for the top N directories. integer 20 unravel.python.reporting.files.tiny_files_threshold_size The file size must be less than or equal to the max_threshold_size for it to be counted. bytes 100KB unravel.python.reporting.files.tiny_files_min_files The minimum number of files which must less than or equal to the the threshold size for the directory to be included. integer 10 unravel.python.reporting.files.tiny_files_top_n_dirs Maximum number of directories to display, in other wowrds, only display the results for the top N directories. integer 30 unravel.python.reporting.files.empty_files_min_files The minimum number of files that must empty for the directory to be included. integer 10 unravel.python.reporting.files.empty_files_top_n_dirs Maximum number of directories to display, in other words, only display the results for the top N directories. integer 3 The following four properties are defined per file size type; Size : huge, medium, tiny, or empty Property\/Description Set by user Unit Default Size _files_use_avg_file_size_flag true : average of all the files is used against the threshold criteria and either all the files are accepted and counted or rejected and not counted as per the criteria. false :  absolute file size is used against the threshold criteria and a file is accepted\/counted or rejected\/not counted as per the criteria. boolean false Size _files_min_parent_dir_depth Directory depth to end the search at. For instance, if depth=2, search begins below two levels. Give given HDFS_root\/one\/two the search starts in the directory two integer 0 Size _files_max_parent_dir_depth Directory depth to end search at. Maximum is 50. For instance, if depth=5 given HDFS_root\/one\/two\/three\/four\/five\/six\/seven the search ends at five . integer 10 Size _files_drill_down_subdirs_flag When true a file is accounted for (listed) in all of its ancestors.   false : a file is accounted in only its immediate parent. This allows Unravel to find a specific directory with maximum number of files matching the size criteria. true : lists each file with its ancestors. For example given the directory structure is \/one\/two false : \/ - lists files in \/. \/one - lists files in one. \/one\/two - lists files in \/one\/two. true : \/ - lists files in \/, \/one, and \/one\/two. \/one - lists files in \/one, and \/one\/two. \/one\/two - lists files in \/one\/two. boolean false " }, 
{ "title" : "Forecasting report", 
"url" : "102403-properties-forecasting.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Forecasting report", 
"snippet" : "See Cluster and Cluster Manager for properties which must be configured for this report to be generated. Property\/Description Set by user Unit Default com.unraveldata.capacityforecasting.hdfs.entitynames Overrides the \"metadata\/entityName\" value for Cloudera Manager clusters. The HDFS Capacity Forec...", 
"body" : "See Cluster and Cluster Manager for properties which must be configured for this report to be generated. Property\/Description Set by user Unit Default com.unraveldata.capacityforecasting.hdfs.entitynames Overrides the \"metadata\/entityName\" value for Cloudera Manager clusters. The HDFS Capacity Forecasting report matches this value when querying the capacity_used metric from the timeseries REST endpoint. The value should be equivalent to the desired HDFS nameservice name from the hdfs-site config \"dfs.nameservices\" Example: HDFS, hdfsdevns1 CSL HDFS " }, 
{ "title" : "FSimage", 
"url" : "102404-properties-fsimage.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ FSimage", 
"snippet" : "You must restart the unravel_ondemand daemon for any changes to take effect Property\/Description Set by user Unit Default com.unraveldata.ngui.sfhivetable.schedule.interval Frequency, in days, in which to trigger FSimage extraction, for example, every 3 days The scheduler schedules extraction relati...", 
"body" : "You must restart the unravel_ondemand daemon for any changes to take effect Property\/Description Set by user Unit Default com.unraveldata.ngui.sfhivetable.schedule.interval Frequency, in days, in which to trigger FSimage extraction, for example, every 3 days The scheduler schedules extraction relative to the 1st of the month and then sets each extraction such that it 1st, 1st + X days, 1st + 2X days until 1st + nX days crosses into the next month, at which point the schedule resets to the 1st. See below for an example. Format: X d day 1d com.unraveldata.ngui.sfhivetable.schedule.time Specify the time to download in hours (using 24 hour time) the FSimage. Format: two digits between 00 and 23. two digits (member of set) 00 unravel.python.reporting.files.external_fsimage_dir Directory for fsimage when skip_fetch_fsimage =true. The fsimage externally fetched is expected to be in this directory. Unravel uses the latest file in this directory which starts with \" fsimage_\". This directory must be different than the Unravel's internal directory, i.e., \/srv\/unravel\/tmp\/reports\/fsimage. string - unravel.python.reporting.files.skip_fetch_fsimage If hdfs admin privileges can not be granted, set this to true to allow Unravel's Ondemand process to use an externally fetched FSimage. true : Ondemand etl_fsimage process does not fetch FSimage from name node. Instead, the FSimage is expected to be available in directory specified by unravel.python.reporting.files.external_fsimage_dir . boolean false The scheduler always calculates the extraction schedule from the first of the month. The extraction time is specified by com.unraveldata.ngui.sfhivetable.schedule.time . The FSImage extraction is triggered on the first of the month and further extractions are calucated 1 + X days, 1 + 2X days, etc. When the interval crosses over into the next month, it is ignored. FSImage extraction occurs on the 1st of the new month and then scheduled every X days. For example: the installation is on 23 July 07.00. com.unraveldata.ngui.sfhivetable.schedule.time =02 com.unraveldata.ngui.sfhivetable.schedule.interval =9d. The next extraction is at 28 July 02:00 (1 + 9 day * 3). The process begins again with a 1 August extraction then 10 August, 19 August, 28 August, 1 September and so on. " }, 
{ "title" : "HBase", 
"url" : "102405-properties-hbase.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ HBase", 
"snippet" : "Name\/Description Set By User Unit Default com.unraveldata.hbase.source.type Source of metrics. Supported values: AMBARI , CDH , and JMX . Required set member - com.unraveldata.hbase.clusters Cluster names to monitor. If source.type = CDH | AMBARI , this must match the cluster name as per rest api. F...", 
"body" : "Name\/Description Set By User Unit Default com.unraveldata.hbase.source.type Source of metrics. Supported values: AMBARI , CDH , and JMX . Required set member - com.unraveldata.hbase.clusters Cluster names to monitor. If source.type = CDH | AMBARI , this must match the cluster name as per rest api. Format: clustername1,clustername2,... Required CSL - com.unraveldata.hbase.metric.poll.interval Polling interval in minutes. min 5 com.unraveldata.hbase.http.conn.timeout Polling connection timeout in seconds s 5 com.unraveldata.hbase.http.read.timeout Polling read timeout in seconds. count 10 com.unraveldata.hbase.http.poll.parallelism Polling parallelism, number of cores. count 2 com.unraveldata.hbase.alert.average.threshold Threshold factor above average value for alerts. percent 1.2 (120%) HBase - source.type=JMX Name\/Description Set By User Unit Default com.unraveldata.hbase. cluster_name .node.http.apis HBase node Web UI. cluster_name is a HBase cluster. Define this property for each HBase cluster. Format: http[s]:\/\/host:port,http[s]:\/\/host:port,... Example: com.unraveldata.hbase.FirstCluster.node.http.apis=http:\/\/your.main.server:16010,http:\/\/your.region.server:16030 com.unraveldata.hbase.SecondCluster.node.http.apis=http:\/\/your.main.server:16020,http:\/\/your.region.server:16040 Required CSL - HBase - source.type=AMBARI | CDH Name\/Description Set By User Unit Default com.unraveldata.hbase.rest.url HBase rest base url of source. Ambari or Cloudera Manager base URL. You must specify a port if you are not using the default port (http=80 and https=443). Format: http[s]:\/\/your.ambari.server[:port]\" Example: http:\/\/your.ambari.server http:\/\/your.ambari.server:88 Required integer - com.unraveldata.hbase.rest.user Username for rest api. Required string - com.unraveldata.hbase.rest.pwd Password for rest api. Required string - com.unraveldata.hbase.rest.ssl.enabled hbase.ssl.enabled property value. boolean - com.unraveldata.hbase.master.port hbase.master.info.port property value. For AMBARI: 16010 For CDH: 60010 Optional integer - com.unraveldata.hbase.regionserver.port hbase.master.info.port property value. For AMBARI: 16030 For CDH: 60030 Optional integer - com.unraveldata.hbase.service.name HBase service name if not the default - “HBASE”. Format: clustername1=servicename1,clustername2=servicename2,... Optional CSL - " }, 
{ "title" : "Hive Hook SSL", 
"url" : "102406-properties-ssl-hivehook.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Hive Hook SSL", 
"snippet" : "Set these properties in hive-site-xml or in Hive CLI (using --hiveconf ) Property\/Description Set by user Unit Default com.unraveldata.hive.hook.insecure.ssl false : SSL certificate is issued and signed by a trusted signing authority or certificate is self-signed and must be added into trust store. ...", 
"body" : "Set these properties in hive-site-xml or in Hive CLI (using --hiveconf ) Property\/Description Set by user Unit Default com.unraveldata.hive.hook.insecure.ssl false : SSL certificate is issued and signed by a trusted signing authority or certificate is self-signed and must be added into trust store. true : certificate is not validated, trust store not needed. boolean false com.unraveldata.hive.hook.use.ssl Enables SSL. boolean false com.unraveldata.hive.hook.ssl.trust_store Trust store. string - com.unraveldata.hive.hook.ssl.trust_store_password Trust store password as plain text. string - com.unraveldata.hive.hook.ssl.trust_store_password_file Path to file of containing Trust store password. If both this and the trust_store_password are set. The password in this file takes precedence. string - com.unraveldata.host The hostname. string com.unraveldata.port The port number. integer 4043 " }, 
{ "title" : "Hive Metastore access", 
"url" : "102407-properties-hive-metastore-javax.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Hive Metastore access", 
"snippet" : "Required for Data Insights tab to populate its information correctly. Property\/Description Set by user Unit Default javax.jdo.option.ConnectionDriverName JDBC Driver class name for the data store containing the metadata. Examples: MySQL: com.mysql.jdbc.Driver Oracle: oracle.jdbc.driver.OracleDriver ...", 
"body" : "Required for Data Insights tab to populate its information correctly. Property\/Description Set by user Unit Default javax.jdo.option.ConnectionDriverName JDBC Driver class name for the data store containing the metadata. Examples: MySQL: com.mysql.jdbc.Driver Oracle: oracle.jdbc.driver.OracleDriver Microsoft: com.microsoft.sqlserver.jdbc.SQLServerDriver Required string - javax.jdo.option.ConnectionPassword Password used to access the data store. Required string - javax.jdo.option.ConnectionUserName Username used to access the data store. Required string - javax.jdo.option.ConnectionURL JDBC connection string for the data store containing the metadata of the form: jdbc: DB_Driver :\/\/ HOST : PORT \/hive Example: Oracle: jdbc:oracle:thin:@prodHost:1521:ORCL Microsoft: jdbc:sqlserver:\/\/ jdbc_url Requited string (URL) - " }, 
{ "title" : "Hive Metastore JDBC", 
"url" : "102408-properties-hive-metastore-jdbc.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Hive Metastore JDBC", 
"snippet" : "You may optionally configure the following properties to manage the Hive Metastore JDBC connection pooling. Unravel uses the c3p0 library to manage the pooling. Property\/Description Set by user Unit Default com.unraveldata.metastore.db.c3p0.acquireRetryAttempts Controls how many times c3p0 tries to ...", 
"body" : "You may optionally configure the following properties to manage the Hive Metastore JDBC connection pooling. Unravel uses the c3p0 library to manage the pooling. Property\/Description Set by user Unit Default com.unraveldata.metastore.db.c3p0.acquireRetryAttempts Controls how many times c3p0 tries to obtain an connection from the database before giving up. For MapR you must set this value to 0. count 30 com.unraveldata.metastore.db.c3p0.acquireRetryDelay Controls how much waiting time is between each retry attempts in milliseconds. For MapR you must set this value to 0. ms 1000 com.unraveldata.metastore.db.c3p0.breakafteracquirefailure Allows you to mark data source as broken and permanently be closed if a connection cannot be obtained from database. The default value is false . boolean false com.unraveldata.metastore.db.c3p0.maxconnectionage The maximum number of seconds any connections were forced to be released from the pool. If the default value (0) is used the connections will never be released. sec 0 com.unraveldata.metastore.db.c3p0.maxidletimeexcessconnections The number of seconds that connections are permitted to remain idle in the pool before being released. If the default value (0) is used the connections will never be released. sec 0 com.unraveldata.metastore.db.c3p0.maxpoolsize The maximum connections in the connection pool. count 5 com.unraveldata.metastore.db.c3p0.idleconnectiontestperiod   0 com.unraveldata.metastore.databasePattern   string dname* com.unraveldata.print.metastore.stats   boolean false com.unraveldata.metastore.use.jdbc Enables read-only access to retrieve data from HiveMetastore with simple JDBC calls. true : read-only access for HiveMetastore data retrieval. false : read-write access for HiveMetastore data retrieval. boolean false " }, 
{ "title" : "HiveServer2", 
"url" : "102409-properties-hiveserver2.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ HiveServer2", 
"snippet" : "Property\/Description Set by user Unit Default unravel.hive.server2.host FQDN or IP-Address of the HiveServer2 instance. string - unravel.hive.server2.port Port for the HiveServer2 instance. number 10000 Property\/Description Set by user Unit Default unravel.hive.server2.authentication Define the auth...", 
"body" : "Property\/Description Set by user Unit Default unravel.hive.server2.host FQDN or IP-Address of the HiveServer2 instance. string - unravel.hive.server2.port Port for the HiveServer2 instance. number 10000 Property\/Description Set by user Unit Default unravel.hive.server2.authentication Define the authentication type. Possible values are: KERBEROS , LDAP , NOSASL , NONE , or CUSTOM . When set to KERBEROS you must also set kerberos.service.name =hive. set member - unravel.hive.server2.kerberos.service.name Set only when unravel.hive.server2.authentication= KERBEROS . This must be set to hive to run the various reports in a kerberos enviornment. string - unravel.hive.server2.password Use only when unravel.hive.server2.authentication= LDAP or CUSTOM . string - unravel.hive.server2.thrift.transport For \"TTransportBase\" for custom advanced usage. - If unravel.hive.server2.authentication = KERBEROS you must also define these additional Kereberos properties. " }, 
{ "title" : "Impala", 
"url" : "102410-properties-impala.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Impala", 
"snippet" : "Property\/Definition Set by user Unit Default com.unraveldata.data.source Can be cm or impalad.   cm com.unraveldata.impalad.nodes Node list in the form of ip:port,ip:port,ip:port . Required CSL - com.unraveldata.impala.ddl Controls whether DDL statements will be imported. boolean false Cloudera Mana...", 
"body" : "Property\/Definition Set by user Unit Default com.unraveldata.data.source Can be cm or impalad.   cm com.unraveldata.impalad.nodes Node list in the form of ip:port,ip:port,ip:port . Required CSL - com.unraveldata.impala.ddl Controls whether DDL statements will be imported. boolean false Cloudera Manager Properties com.unraveldata.cloudera.manager.impala.num.queries.limit Maximum number of queries that will be returned by a poll to the Cloudera Manager API. integer 1000 com.unraveldata.cloudera.manager.impala.poll.interval.millis Interval between consecutive polls to the Cloudera Manager API measured in milliseconds. ms 60000 com.unraveldata.cloudera.manager.impala.look.back.minutes Number of minutes to look back when polling the Cloudera Manager API. min -5 com.unraveldata.impala.skip.duration.millis Queries with duration shorter than this threshold will get captured but not analyzed. ms 1000 The following properties defaults should be fine and shouldn't need to be changed. com.unraveldata.impala.events.stalestats.threshold.bytes bytes 1000 com.unraveldata.impala.events.stalestats.ratio percent 0.2 com.unraveldata.impala.events.longop.time.millis ms 2000 com.unraveldata.impala.events.longop.ratio percent 0.2 com.unraveldata.impala.events.cost.diff.bytes bytes 500000000 com.unraveldata.impala.events.skew.time.millis ms 500000000 com.unraveldata.impala.events.skew percent 1.5 " }, 
{ "title" : "Kafka", 
"url" : "102411-properties-kafka.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Kafka", 
"snippet" : "Kafka - cluster Property\/Definition Set by user Unit Default com.unraveldata.ext.kafka.clusters Cluster list. These user-defined names are used to clearly identify the Kafka cluster in the Unravel UI. Use a comma separated list for multiple clusters. Required CSL - com.unraveldata.ext.kafka. cluster...", 
"body" : "Kafka - cluster Property\/Definition Set by user Unit Default com.unraveldata.ext.kafka.clusters Cluster list. These user-defined names are used to clearly identify the Kafka cluster in the Unravel UI. Use a comma separated list for multiple clusters. Required CSL - com.unraveldata.ext.kafka. cluster . bootstrap_servers List of brokers that is used to retrieve initial information about the kafka cluster. For each cluster in com.unraveldata.ext.kafka.clusters you must define the associated brokers. Use a comma separated list for multiple brokers. For example, com.unraveldata.ext.kafka.East.bootstrap_servers=localhost:9092,localhost:9093. Required CSL - com.unraveldata.ext.kafka. cluster . jmx_servers Aliases for each kafka nodes in the clusters with JMX ports exposed. You must assign aliases the cluster nodes. Use a comma separated list for multiple nodes. For example, unraveldata.ext.kafka.East.jmx_servers=kNode-1, kNode-2\/. Required CSL - com.unraveldata.ext.kafka. cluster . jmx. kNode-1 . host The host for a node in the cluster. You must define a host for each node in each cluster. For example, com.unraveldata.ext.kafka.East.kNode1=localhost com.unraveldata.ext.kafka.East.kNode2=localhost. Required - com.unraveldata.ext.kafka. cluster . jmx . kNode-1 . port For each node in each cluster you must assign a port. For example, com.unraveldata.ext.kafka.East.jmx.kNode1.port=5005. Required number - To locate Kafka and JMX ports: Cloudera Manager . Navigate to: Clusters → Kafka → Configuration → Ports and Addresses. Alternatively, you may lookup up the information in the broker nodes of Zookeeper CLI. HDP : For Protocol and broker port navigate to: Kafka → Configs → Kafka Broker. JMX port navigate to: Kafka → Configs → Advanced kafka-env → kafka-env template. General Property\/Definition Set by user Unit Default com.unraveldata.ext.kafka.aggs.broker integer 100 com.unraveldata.ext.kafka.aggs.consumer_group integer 1000 com.unraveldata.ext.kafka.aggs.broker_metrics integer 1000 com.unraveldata.ext.kafka.insight.interval_min min 15 com.unraveldata.ext.kafka.insight.lag_threshold integer 100 com.unraveldata.ext.kafka.insight.num_ignored_intervals integer 2 com.unraveldata.ext.kafka.insight.sw_size integer 30 com.unraveldata.ext.kafka.servers Use a comma separated list for multiple servers. Required CSL kafka-test1,kafka-test2 com.unraveldata.kafka.broker_list 127.0.0.1:4091 com.unraveldata.monitoring.kafka.check.interval How often Kafka metrics should be queried. Set 0 for disabling Kafka monitoring. sec 30 com.unraveldata.monitoring.kafka.history.size How many data sets consider in computing average values. integer 5 com.unraveldata.monitoring.kafka.ignore.topics Topics to ignore. Use a comma separated list for multiple topics.   CSL __consumer_offsets, connect-configs, connect-offsets, connect-status " }, 
{ "title" : "LDAP", 
"url" : "102412-properties-ldap.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ LDAP", 
"snippet" : "These properties are required when com.unraveldata.login.mode =ldap. Property\/Description Set by user Unit Default com.unraveldata.ldap.domain string - com.unraveldata.ldap.baseDN LDAP base DN; use your rootDN value if a custom LDAP query is applied. Needed for Open LDAP. See also com.unraveldata.ld...", 
"body" : "These properties are required when com.unraveldata.login.mode =ldap. Property\/Description Set by user Unit Default com.unraveldata.ldap.domain string - com.unraveldata.ldap.baseDN LDAP base DN; use your rootDN value if a custom LDAP query is applied. Needed for Open LDAP. See also com.unraveldata.ldap.userDNPattern below as an alternative. string - com.unraveldata.ldap.customLDAPQuery A full LDAP query that LDAP Atn provider uses to execute against LDAP Server. If this query returns a null result set, the LDAP Provider fails the Authentication request, succeeds if the user is part of the resultset. If this property is set, filtering and group properties are ignored. string - com.unraveldata.ldap.groupClassKey LDAP attribute name on the group entry that is to be used in LDAP group searches. string  - com.unraveldata.ldap.groupDNPattern COLON-separated list of patterns to use to find DNs for group entities in this directory. Use %s where the actual group name is to be substituted for. Each pattern should be fully qualified. When using this property you must unset com.unraveldata.ldap.domain . string  - com.unraveldata.ldap.groupFilter COMMA-separated list of LDAP Group names (short name not full DNs). If you wish to have LDAP admins, you must define at least one group of admins. See com.unraveldata.login.admins.ldap.groups . Example: secs-lab-admins,secs-lab-users CSL - com.unraveldata.ldap.groupMembershipKey LDAP attribute name on the user entry that references a group that the user belongs to. Default is 'member'. string - com.unraveldata.ldap.guidKey LDAP attribute name whose values are unique in this LDAP server. Default is \"uid\"; not used when custom query is specified. string - com.unraveldata.login.admins.ldap.groups The group(s) of LDAP admins. The group(s) must be listed in com.unraveldata.ldap.groupFilter . Example: secs-lab-admins CSL - com.unraveldata.ldap.mailAttribute The mail attribute name in the LDAP response that Unravel server uses to extract the ldap user's email address. If not configured, Unravel server uses the attribute name \"mail\". string - com.unraveldata.ldap.realUserAttribute Enables a secondary LDAP lookup. When the AD object does not have the available email string, Unravel needs to do a second lookup to retrive the user's email address. This email address is used by AutoActions when sending a email to the apps old. string - com.unraveldata.ldap.userDNPattern COLON-separated list of patterns to use to find DNs for users in this directory. Use %s where the actual group name is to be substituted for. This is used like a list of baseDNs and baseDN is ignored if this is set. string - com.unraveldata.ldap.userFilter COMMA-separated list of LDAP usernames (just short names, not full DNs). string - com.unraveldata.ldap.url The URL for the LDAP server. Can be multiple servers with a space separator. Standard port is used if unspecified. Example: ldap:\/\/host ldaps:\/\/hostldap:\/\/host:9999 ldaps:\/\/host1:9999  ldaps:\/\/host2:9999 string - com.unraveldata.ldap.verbose Enables verbose logging. Grep for \"Ldap\" entries in the  unravel_ngui.log  file under \/usr\/local\/unravel\/logs\/ . true : user names and group names can appear in this log, but raw passwords are not logged. false : logging turned off. boolean false " }, 
{ "title" : "Cloud migration reports", 
"url" : "102413-properties-migration.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Cloud migration reports", 
"snippet" : "See Cluster and Cluster Manager for properties which must be configured for these reports to be generated. Note: As of 29 July 2019, the default is false for new installations of Unravel 4.5.3.0 and later. Property\/Description Set by user Unit Default unravel.python.reporting.cloudreport.enable Cont...", 
"body" : "See Cluster and Cluster Manager for properties which must be configured for these reports to be generated. Note: As of 29 July 2019, the default is false for new installations of Unravel 4.5.3.0 and later. Property\/Description Set by user Unit Default unravel.python.reporting.cloudreport.enable Controls the generation of Migration Reports. true : Reports can be created. false : Reports cannot be created. boolean true (See note.) Services and version compatibility Property\/Description Set by user Unit Default com.unraveldata.reporting.cloudreport.versioncompatibility.max_services.show The max number of services and versions to show in the MIGRATION PLANNING > SERVICES AND VERSIONS COMPATIBILITY matrix. Value 1 - 100 count - " }, 
{ "title" : "Miscellaneous", 
"url" : "102414-properties-misc.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Miscellaneous", 
"snippet" : "Property\/Description Set by user Unit Default com.unraveldata.action.killmove.enabled Allows users to kill and move apps. true : users can kill and move appls false : only admins can kill and move apps. boolean false...", 
"body" : "Property\/Description Set by user Unit Default com.unraveldata.action.killmove.enabled Allows users to kill and move apps. true : users can kill and move appls false : only admins can kill and move apps. boolean false " }, 
{ "title" : "Oozie", 
"url" : "102415-properties-oozie.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Oozie", 
"snippet" : "Property\/Description Set by user Unit Default oozie.server.url The Oozie server URL to be monitored by Unravel. If multiple servers exist, the server url can be a comma separated string where each part is ip of 1 Oozie server, e.g., http:\/\/ip-10-0-0-110.ec2.internal:11000,http:\/\/ip-10-0-0-114.ec2.in...", 
"body" : "Property\/Description Set by user Unit Default oozie.server.url The Oozie server URL to be monitored by Unravel. If multiple servers exist, the server url can be a comma separated string where each part is ip of 1 Oozie server, e.g., http:\/\/ip-10-0-0-110.ec2.internal:11000,http:\/\/ip-10-0-0-114.ec2.internal:11000 Required CSL - oozie.server.username Optional string - oozie.server.password Optional string - oozie.log.length The maximum number of characters in Oozie workflow log that Unravel fetches. Any log longer than this is trimmed from the beginning and only last x characters are kept. bytes 1000000 (1MB) com.unraveldata.oozie.disable Whether to disable bringing in Oozie workflows into Unravel. The underlying jobs will not be affected. boolean false com.unraveldata.oozie.fetch.num Number of workflows to pull in each API call. count 100 com.unraveldata.oozie.fetch.interval.sec Controls the rate the Oozie REST server is polled. Seconds between intervals for fetching Oozie workflow status. s 120 com.unraveldata.oozie.retry.sec s 600 " }, 
{ "title" : "Queue analysis", 
"url" : "102416-properties-q-analysis.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Queue analysis", 
"snippet" : "Property\/Description Set by user Unit Default com.unraveldata.report.queue.metrics.sensor.enabled Enables or disables queue metric sensor. boolean true com.unraveldata.report.queue.http.retries YARN Resource Manager HTTP connection retries. count 3 com.unraveldata.report.queue.http.retry.period.msec...", 
"body" : "Property\/Description Set by user Unit Default com.unraveldata.report.queue.metrics.sensor.enabled Enables or disables queue metric sensor. boolean true com.unraveldata.report.queue.http.retries YARN Resource Manager HTTP connection retries. count 3 com.unraveldata.report.queue.http.retry.period.msec YARN Resource Manager HTTP connection retry wait period. ms 0 com.unraveldata.report.queue.http.timeout.msec YARN Resource Manager HTTP connection timeout. ms 10000 com.unraveldata.report.queue.poll.interval.msec How often queue metric sensor polls polls YARN Resource Manager. ms 60000 unravel.python.queueanalysis.daterange.span UI report date picker range. days 30 unravel.python.queueanalysis.metrics.scale UI rendered graph metrics scale factor. integer 1000 " }, 
{ "title" : "Role Based Access Control (RBAC)", 
"url" : "102417-properties-rbac.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Role Based Access Control (RBAC)", 
"snippet" : "See RBAC configuration . RBAC is reliant upon the tagging; if you are not familiar with the concept or how it is used see What is tagging . Property\/Description Set by user Unit Default com.unraveldata.rbac.enabled Enables Role Based Access Control. true : RBAC turned on. false : RBAC turned off. bo...", 
"body" : "See RBAC configuration . RBAC is reliant upon the tagging; if you are not familiar with the concept or how it is used see What is tagging . Property\/Description Set by user Unit Default com.unraveldata.rbac.enabled Enables Role Based Access Control. true : RBAC turned on. false : RBAC turned off. boolean true com.unraveldata.rbac.default Determines how a end-user's views are filtered when no specific tags are set them. userName : filters view by the user's name. user : filters view by the real user's name. set member userName com.unraveldata.rbac.tagcmd The command to use to get the list of LDAP groups for an user. string \/usr\/local\/unravel\/etc\/apptag.py com.unraveldata.ngui.user.mode Determines the UI pages the end-user can access when RBAC is enabled. Value: extended | restricted extended a user can access Application | Applications Operations | Usage Details | Infrastructure Operations | Usage Details | Impala Usage Reports | Operational Insights | Chargeback restricted a user can only access Application | Applications string extended LDAP Define these properties if com.unraveldata.login.mode =ldap. See LDAP properties for defining LDAP admins and read-only admins. Property\/Description Set by user Unit Default com.unraveldata.rbac.ldap.tags A comma separated list of the prefix of LDAP groups to be used as tag_key . For example, PROJECT,DEPT. - CLS - com.unraveldata.rbac.ldap.tag. TAG_KEY .regex.find Defines regular expression used to parse LDAP groups to generating the tag_value for a given tag_key . Value = TAG_KEY - REGEX where TAG_KEY is defined in com.unraveldata.rbac.ldap.tags REGEX is a regular expression. For example, com.unraveldata.rbac.ldap.tag.PROJECT.regex.find=PROJECT-(.*) Note: The LDAP group is processed until a match is found. If you have defined more than one group in your definition only the first group pre is processed with the remaining ignored. The best practice is to define each tag_value tag in it's own LDAP group. - CLS - SAML Define these properties if com.unraveldata.login.mode =saml. See SAML properties for defining SAML admins and read-only admins. Property\/Description Set by user Unit Default com.unraveldata.rbac.saml.tags A comma separated list of the prefix of SAML groups to be used as tag_key (s). For example, PROJECT,DEPT. - CLS - com.unraveldata.rbac.saml.tag. TAG_KEY .regex.find Defines regular expression used to parse SAML groups to generating the tag_value(s) for a given tag_key . Value = TAG_KEY - REGEX where TAG_KEY is defined in com.unraveldata.rbac.ldap.tags . For example, com.unraveldata.rbac.ldap.tag.PROJECT.regex.find=PROJECT-(.*) Note: The SAML group is processed until a match is found. If you have defined more than one group in your definition only the first group pre is processed with the remaining ignored. The best practice is to define each tag_value tag in it's own LDAP group. - CLS - " }, 
{ "title" : "S3 monitoring", 
"url" : "102418-properties-s3.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ S3 monitoring", 
"snippet" : "Property\/Description Set by user Unit Default com.unraveldata.s3.batch.monitoring.interval.sec Defines the monitoring frequency. Set this property to 60 for lower latency. s 180...", 
"body" : "Property\/Description Set by user Unit Default com.unraveldata.s3.batch.monitoring.interval.sec Defines the monitoring frequency. Set this property to 60 for lower latency. s 180 " }, 
{ "title" : "SAML", 
"url" : "102419-properties-saml.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ SAML", 
"snippet" : "These properties are required when com.unraveldata.login.mode =saml. Property\/Description Set by user Unit Default com.unraveldata.login.saml.config Fully qualified path to saml configuration file. Optional string (path) - com.unraveldata.login.admins.saml.groups Grants read\/write admin access to an...", 
"body" : "These properties are required when com.unraveldata.login.mode =saml. Property\/Description Set by user Unit Default com.unraveldata.login.saml.config Fully qualified path to saml configuration file. Optional string (path) - com.unraveldata.login.admins.saml.groups Grants read\/write admin access to an AD user who belongs to a specified group. Value: a comma separated list of groups. c - com.unraveldata.login.admins.readonly.saml.groups Grants read-only admin access to an AD user who belongs to a specified group. Value: a comma separated list of groups. CSL - These properties are sent in the login.saml.config file that specified by com.unraveldata.login.saml.config . Property\/Description Set by user Unit Default entryPoint Identity provider entry point, It must be specified in order to be spec-compliant when the request is signed. Example: \"http:\/\/c24.unravel.com:9080\/simplesaml\/saml2\/idp\/SSOService.php\" Optional - issuer Issuer string to supply to identity provider (Environment name). Should match the name configured in ldp. Example: “Congo24”, “Localhost” , Optional - cert IDP's public signing certificate. Example: Idp Cert String Optional - cert IDP's public signing certificate. Example: Idp Cert String Optional - unravel_mapping Mapping SAML attributes to Unravel attributes. Specific to unravel Integration. Example: \n{\n \"username\":\"userid\",\n \"groups\":\"ds_groups\"\n}\n - " }, 
{ "title" : "Sensor", 
"url" : "102420-properties-sensors.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Sensor", 
"snippet" : "Most Sensor properties are set via JVM arguments; the file name is noted when the properties can be set in a file. All Sensors Specified by adding -D<propertyName>=<propertyValue> to the list of JVM args, e.g.-Dclient.rest.conntimeout.ms=0 Property\/Description Set by user Unit Default com.unraveldat...", 
"body" : "Most Sensor properties are set via JVM arguments; the file name is noted when the properties can be set in a file. All Sensors Specified by adding -D<propertyName>=<propertyValue> to the list of JVM args, e.g.-Dclient.rest.conntimeout.ms=0 Property\/Description Set by user Unit Default com.unraveldata.client.rest.queue The queue length for outgoing REST HTTP requests. 20000 com.unraveldata.client.rest.retryfail Cool-down period after unsuccessful attempt to make REST HTTP request in nanoseconds. ns 30 sec com.unraveldata.client.rest.conn.timeout.ms REST HTTP request timeout in milliseconds. ms 100 com.unraveldata.client.rest.shutdown.ms Maximum time to wait for orderly shutdown of the REST client (if exceeded some messages still in the queue will be lost). ms 10 com.unraveldata.client.rest.dns.ttl The period to refresh the DNS info in milliseconds - IP is pre-resolved and kept until the next refresh if no failures are observed. ms 6 hours com.unraveldata.client.rest.priority.retries Certain critical messages have priority flag and their transmission will be reattempted this many times. 5 unravel.server.hostport Unravel server host:port information. - HiveHooks Specified in hive-conf, or in the hive CLI or beeline session. (CDH 5.14 only.) Property\/Description Set by user Unit Default com.unraveldata.com.hive.spark.enabled Set to true to allow Hive and Spark applications to be linked. Only available for CDH 5.14. boolean true Resource usage sensor Specified by adding -D<propertyName>=<propertyValue> to the list of JVM args, e.g.-Dclient.rest.conntimeout.ms=0 Property\/Description Set by user Unit Default com.unraveldata.agent.metrics.enabled_keys Comma separated list of metric type names which are enabled for collection. CSL availableMemory,cpuUtilization,processCpuLoad,systemCpuLoad,maxHeap,usedHeap,vmRss,gcLoad unravel.metrics.factor Sampling period scale down factor. 1 Agent arguments are added directly to the agent definition - anything after -javaagent:btrace-agent.jar= is considered to be the agent argument. The arguments are delimited by comma with no extra space. Agent argument\/definition Set by user Unit Default metricsCaptureFilter Format allow specifying single ordinals for component IDs as well as ranges and enumerations. Fore example: metricsCaptureFilter=1,2,5-10,turns on metrics collection for components 1, 2, 5 to 10. 0-1500 Spark shutdown delay Specified in spark-defaults.conf. Property\/Description Set by user Unit Default com.unraveldata.spark.shutdown.delay.ms Amount of time to delay shutdown so the last messages are processed (allows Btrace sensor to send all the data before the spark driver exits). ms 30 Spark sensor Specified by adding -D<propertyName>=<propertyValue>; to the list of JVM args, e.g.-Dclient.rest.conntimeout.ms=0 Property\/Description Set by user Unit Default com.unraveldata.spark.sensor.enableLiveUpdates Enable live updates for Spark apps. boolean False com.unraveldata.spark.sensor.enableCachingInfo Enable tracking caching info for Spark apps. boolean False com.unraveldata.spark.sensor.enableSampling Enable data sampling between operators for Spark apps. boolean False Agent arguments are added directly to the agent definition - anything after -javaagent:btrace-agent.jar= is considered to be the agent argument. The arguments are delimited by comma with no extra space. Agent argument\/definition Set by user Unit Default com.unraveldata.spark.sensor.clusterID The cluster ID;, currently only used in Spark. string - " }, 
{ "title" : "Sessions", 
"url" : "102421-properties-sessions.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Sessions", 
"snippet" : "Property\/Description Set by user Unit Default com.unraveldata.session.enabled Enables Sessions features tab in the UI. true : Sessions enabled. false : Sessions disabled. You must restart the ngui and ondemand daemons when changing the value. boolean true com.unraveldata.session.max.autotune.runs Ma...", 
"body" : "Property\/Description Set by user Unit Default com.unraveldata.session.enabled Enables Sessions features tab in the UI. true : Sessions enabled. false : Sessions disabled. You must restart the ngui and ondemand daemons when changing the value. boolean true com.unraveldata.session.max.autotune.runs Maximum number of runs allowed in an auto-tune session. count 8 com.unraveldata.session.dynamicAllocation.enabled If set to true Spark sessions recommends dynamic allocation. If users don't want that they need to explicitly set it to false. boolean true com.unraveldata.session.spark1.submit.command The command to use for Spark1 applications while applying the session run. string spark-submit com.unraveldata.session.spark2.submit.command The command to use for Spark2 applications while applying the session run. string spark2-submit " }, 
{ "title" : "Small files report", 
"url" : "102422-properties-small-files.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Small files report", 
"snippet" : "You must restart the unravel_ondemand daemon for any changes to take effect. Property\/Description Set by user Unit Default unravel.python.reporting.small_files_use_avg_file_size_flag true : average of all the files is used against the threshold criteria and either all the files are accepted and coun...", 
"body" : "You must restart the unravel_ondemand daemon for any changes to take effect. Property\/Description Set by user Unit Default unravel.python.reporting.small_files_use_avg_file_size_flag true : average of all the files is used against the threshold criteria and either all the files are accepted and counted or rejected and not counted as per the criteria. false :  absolute file size is used against the threshold criteria and a file is accepted\/counted or rejected\/not counted as per the criteria. boolean true unravel.python.reporting.files.small_files_min_parent_dir_depth Directory depth to end the search at. For instance, if depth=2, search begins below two levels. Give given HDFS_root\/one\/two the search starts in the directory two count 0 unravel.python.reporting.files.small_files_max_parent_dir_depth Directory depth to end search at. Maximum is 50. For instance, if depth=5 given HDFS_root\/one\/two\/three\/four\/five\/six\/seven the search ends at five . count 10 unravel.python.reporting.files.small_files_drill_down_subdirs_flag When true a file is accounted for (listed) in all of its ancestors.   false : a file is accounted in only its immediate parent. This allows Unravel to find a specific directory with maximum number of files matching the size criteria. true : lists each file with its ancestors. For example given the directory structure is \/one\/two false : \/ - lists files in \/. \/one - lists files in one. \/one\/two - lists files in \/one\/two. true : \/ - lists files in \/, \/one, and \/one\/two. \/one - lists files in \/one, and \/one\/two. \/one\/two - lists files in \/one\/two. boolean false " }, 
{ "title" : "Small files and file reports", 
"url" : "102423-properties-smallnfiles.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Small files and file reports", 
"snippet" : "You must restart the unravel_ondemand daemon for any changes to take effect Property\/Description Set by user Unit Default com.unraveldata.ngui.sfhivetable.schedule.interval Frequency, in days, in which to trigger FSimage extraction, for example, every 3 days The scheduler schedules extraction relati...", 
"body" : "You must restart the unravel_ondemand daemon for any changes to take effect Property\/Description Set by user Unit Default com.unraveldata.ngui.sfhivetable.schedule.interval Frequency, in days, in which to trigger FSimage extraction, for example, every 3 days The scheduler schedules extraction relative to the 1st of the month and then sets each extraction such that it 1st, 1st + X days, 1st + 2X days until 1st + nX days crosses into the next month, at which point the schedule resets to the 1st. See below for an example. Format: X d day 1d com.unraveldata.ngui.sfhivetable.schedule.time Specify the time to download in hours (using 24 hour time) the FSimage. Format: two digits between 00 and 23. two digits (member of set) 00 unravel.python.reporting.files.external_fsimage_dir Directory for fsimage when skip_fetch_fsimage =true. The fsimage externally fetched is expected to be in this directory. Unravel uses the latest file in this directory which starts with \" fsimage_\". This directory must be different than the Unravel's internal directory, i.e., \/srv\/unravel\/tmp\/reports\/fsimage. string - unravel.python.reporting.files.skip_fetch_fsimage If hdfs admin privileges can not be granted, set this to true to allow Unravel's Ondemand process to use an externally fetched FSimage. true : Ondemand etl_fsimage process does not fetch FSimage from name node. Instead, the FSimage is expected to be available in directory specified by unravel.python.reporting.files.external_fsimage_dir . boolean false In order for the configuration changes to take effect, unravel_ondemand and unravel_ngui daemons need to be restarted. \/etc\/init.d\/unravel_ngui restart\n\/etc\/init.d\/unravel_ondemand restart Property\/Description Set by user Unit Default unravel.python.reporting.files.disable Enables or disables Unravel ability to generate Small Files and File Reports. false : enables the Small Files and Files reports in both the backend and UI. true : disables the Small Files and Files reports. in both the backend and UI. boolean false unravel.python.reporting.files.hive_database Hive Database where Ondemand creates five Hive tables (four temporary, one permanent) for Small Files\/File Reports. When not set, tables are created in the default Hive Database. In addition, the Hive queries used for this feature run against default MR queue. It must point to a valid Hive database. string default database unravel.python.reporting.files.hive_mr_queue The Hcive queries ran by Ondemand process run against this MR queue. It must point to a valid MR queue. string default These are global properties which apply to both Small Files Report and File Reports. They each have equivalent \"local\" properties. The \"local\" property takes precedence over the equivalent global property. Should you unset\/delete any of the below properties and their equivalent properties in Small Files Report or Files Report, Unravel has hard-coded values to ensure your reports are generated. Property\/Description Set by user Unit Default unravel.python.reporting.files.files_use_avg_file_size_flag true : average of all the files is used against the threshold criteria and either all the files are accepted and counted or rejected and not counted as per the criteria. false :  absolute file size is used against the threshold criteria and a file is accepted\/counted or rejected\/not counted as per the criteria. Optional boolean - unravel.python.reporting.files.min_parent_dir_depth Directory depth to end the search at. For instance, if depth=2, search begins below two levels. Give given HDFS_root\/one\/two the search starts in the directory two Optional count - unravel.python.reporting.files.max_parent_dir_depth Directory depth to end search at. Maximum is 50. For instance, if depth=5 given HDFS_root\/one\/two\/three\/four\/five\/six\/seven the search ends at five . Optional count - unravel.python.reporting.drill_down_subdirs_flag When true a file is accounted for (listed) in all of its ancestors.   false : a file is accounted in only its immediate parent. This allows Unravel to find a specific directory with maximum number of files matching the size criteria. true : lists each file with its ancestors. For example given the directory structure is \/one\/two false : \/ - lists files in \/. \/one - lists files in one. \/one\/two - lists files in \/one\/two. true : \/ - lists files in \/, \/one, and \/one\/two. \/one - lists files in \/one, and \/one\/two. \/one\/two - lists files in \/one\/two. Optional boolean false " }, 
{ "title" : "Spark", 
"url" : "102424-properties-spark.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Spark", 
"snippet" : "Live pipeline Property\/Description Set by user Unit Default com.unraveldata.spark.live.pipeline.enabled Specifies if Unravel should process the live job data coming from sensor or not. true: The live job data will be processed as soon as it is received. false: Live job data will not be processed. bo...", 
"body" : "Live pipeline Property\/Description Set by user Unit Default com.unraveldata.spark.live.pipeline.enabled Specifies if Unravel should process the live job data coming from sensor or not. true: The live job data will be processed as soon as it is received. false: Live job data will not be processed. boolean true com.unraveldata.spark.live.pipeline.maxStoredStages Maximum number of jobs\/stages stored in the DB. If an application has (# jobs\/stages) > maxStoredStages only the last maxStoredStages are stored. This setting affects only the live pipeline . When processing the event log file (after the application has completed its execution) this property is not considered. count 1000 com.unraveldata.spark.master Default spark master mode to be used if not available from Sensor. Possible values: local, standalone or yarn (default) set member yarn Event log processing Property\/Description Set by user Unit Default com.unraveldata.spark.eventlog.location All the possible locations of the event log files. Multiple locations are supported as a comma separated list of values. This property is used only when the Unravel sensor is not enabled. When the sensor is enabled, the event log path is taken from the application configuration at runtime. string hdfs:\/\/\/user\/spark\/applicationHistory\/ com.unraveldata.spark.eventlog.maxSize Maximum size of the event log file that will be processed by the Spark worker daemon. Event logs larger than MaxSize will not be processed. bytes 1000000000 (~1GB) com.unraveldata.spark.eventlog.appDuration.mins Maximum duration (in minutes) of application to pull Spark event log. min 1440 (1 day) com.unraveldata.spark.hadoopFsMulti.useFilteredFiles Specifies how to search the event log files. true : prefix search false : prefix + suffix search Prefix + suffix search is faster as it avoids listFiles() API which may take a long time for large directories on HDFS. This search requires that all the possible suffixes for the event log files are known. Possible suffixes are specified by com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes. . boolean false com.unraveldata.spark.hadoopFsMulti.eventlog.suffixes Specifies suffixes used for prefix+suffix search of the event logs when com.unraveldata.spark.hadoopFsMulti.useFilteredFiles = false . NOTE : the empty suffix (,,) be part of this value for uncompressed event log files. CSL ,,.lz4,.snappy,.inprogres com.unraveldata.spark.appLoading.maxAttempts Maximum number of attempts for loading the event log file from HDFS\/S3\/ ADL\/WASB etc. count 3 com.unraveldata.spark.appLoading.delayForRetry Delay used among consecutive retries when loading the event log files. The actual delay is not constant, it increases progressively by 2^attempt * delayForRetry . ms 2000 (2 s) com.unraveldata.spark.tasks.inMemoryLimit Number of tasks to be kept in memory and DB per stage. All stats are calculated for all the task attempts but only the configured number of tasks will be kept in memory\/DB. count 1000 Events Related com.unraveldata.spark.events.enableCaching Enables logic for executing caching events. boolean false Other properties Property\/Description Set by user Unit Default com.unraveldata.spark.appLoading.maxConcurrentApps This is the number of applications Unravel keep metadata in Spark worker daemon memory. count 5 ccom.unraveldata.spark.time.histogram Specifies whether the timeline histogram is generated or not. Note: Timeline histogram generation is memory intensive. boolean false Properties defined in spark-default.conf Property\/Description Set by user Unit Default com.unraveldata.spark.shutdown.delay.ms Amount of time to delay shutdown so the last messages are processed (allows Btrace sensor to send all the data before the spark driver exits). ms 0 com.unraveldata.spark.live.interval.sec This is the interval in seconds after which live application data is updated. It allows for tracking of Spark tasks. The Spark APM updates on Task completion in addition Job start, and Job and Stage completion. s 60 " }, 
{ "title" : "Spark S3", 
"url" : "102425-properties-spark-s3.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Spark S3", 
"snippet" : "Property\/Description Set by user Unit Default com.unraveldata.s3.profile.config.file.path The path to the s3 profile file, e.g., \/usr\/local\/unravel\/etc\/s3ro.properties . string - com.unraveldata.spark.s3.profilesToBuckets Comma separated list of profile to bucket mappings in the following format: <s...", 
"body" : "Property\/Description Set by user Unit Default com.unraveldata.s3.profile.config.file.path The path to the s3 profile file, e.g., \/usr\/local\/unravel\/etc\/s3ro.properties . string - com.unraveldata.spark.s3.profilesToBuckets Comma separated list of profile to bucket mappings in the following format: <s3_profile>:<s3_bucket>, for example, com.unraveldata.spark.s3.profileToBuckets =profile-prod:com.unraveldata.dev,profile-dev:com.unraveldata.dev. Note Ensure that the profiles defined in the property above are actually present in the s3 properties file and that each profile has associated a corresponding pair of credentials aws_access_key and aws_secret_access_key . The old format: access_key\/secretKey is no longer supported.) CSL - " }, 
{ "title" : "SSL\/TLS", 
"url" : "102426-properties-ssl-tls.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ SSL\/TLS", 
"snippet" : "BTrace Property\/Description Set by user Unit Default com.unraveldata.ssl.insecure true : make connection insecure. boolean false com.unraveldata.client.rest.ssl.enabled true : enables SSL for BTrace. boolean false com.unraveldata.client.rest.ssl.trust_store Trust store. string - com.unraveldata.clie...", 
"body" : "BTrace Property\/Description Set by user Unit Default com.unraveldata.ssl.insecure true : make connection insecure. boolean false com.unraveldata.client.rest.ssl.enabled true : enables SSL for BTrace. boolean false com.unraveldata.client.rest.ssl.trust_store Trust store. string - com.unraveldata.client.rest.ssl.trust_store_password_file File of containing Trust store password. If both this and the trust_store_password are set. The password in this file takes precedence. string - com.unraveldata.client.rest.ssl.trust_store_password Trust store password as plain text. string - BTrace and Hive Hook Property\/Description Set by user Unit Default unravel.port Port number.   number 4043 Cluster access See server SSL for KeyStore properties. See log network activity for enabling the log for debugging purposes. Property\/Description Set by user Unit Default com.unraveldata.cluster_access.host Cluster Access Service host (where the service is bound). string (URL) 0.0.0.0 com.unraveldata.cluster_access.port Cluster Access Service port.   number 4020 com.unraveldata.cluster_access.ssl.enabled SSL enabled for cluster access boolean false Cluster access connectors Property\/Description Set by user Unit Default com.unraveldata.cluster_access.grpc_client.ssl.enabled true : SSL enabled for cluster connectors. boolean false com.unraveldata.cluster_access.grpc_client.ssl.trust_store_path Trust store file path. string - com.unraveldata.cluster_access.grpc_client.ssl.trust_store_password Trust store password as plain text. string - com.unraveldata.cluster_access.grpc_client.log_network_activity Logs network activity. Use only for debugging purposes. boolean false Data Store See server SSL for KeyStore properties. See log network activity for enabling the log for debugging purposes. Property\/Description Set by user Unit Default com.unraveldata.datastore.port HTTP server port.   number 4020 com.unraveldata.datastore.port.https HTTPS server port. < 0:  HTTPS server is disabled. > 0: SSL enabled. number -1 com.unraveldata.ngui.proxy.unravelds DataStore URL for NGUI. string http:\/\/localhost:4020\/ Document storage Property\/Description Set by user Unit Default docstorage.ssl true : enables SSL for Document Storage client.   boolean false Hive Hook Set these properties in hive-site-xml or in Hive CLI (using --hiveconf ) Property\/Description Set by user Unit Default com.unraveldata.hive.hook.insecure.ssl false : SSL certificate is issued and signed by a trusted signing authority or certificate is self-signed and must be added into trust store. true : certificate is not validated, trust store not needed. boolean false com.unraveldata.hive.hook.use.ssl Enables SSL. boolean false com.unraveldata.hive.hook.ssl.trust_store Trust store. string - com.unraveldata.hive.hook.ssl.trust_store_password Trust store password as plain text. string - com.unraveldata.hive.hook.ssl.trust_store_password_file Path to file of containing Trust store password. If both this and the trust_store_password are set. The password in this file takes precedence. string - com.unraveldata.host The hostname. string com.unraveldata.port The port number. integer 4043 Live log receiver See server SSL for KeyStore properties. See log network activity for enabling the log for debugging purposes. Property\/Description Set by user Unit Default com.unraveldata.live.logreceiver.port.https HTTPS server port. Negative value means HTTPS server is disabled. number -1 com.unraveldata.live.logreceiver.port HTTP server port. number 4043 Log network activity Property\/Description Set by user Unit Default com.unraveldata.server.log_network_activity Enables the logging of all network activity. Only enable this when you are debugging. true : enables logging. Cluster Access Connectors has its own log flag.   boolean false Server Property\/Description Set by user Unit Default com.unraveldata.server.ssl.cert_path KeyStore file path, e.g., \/usr\/local\/unravel\/cert.jks. Required string - com.unraveldata.server.ssl.cert_password KeyStore password. Required string - " }, 
{ "title" : "Tagging", 
"url" : "102427-properties-tagging.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Tagging", 
"snippet" : "Property\/Description Set by user Unit Default com.unraveldata.tagging.enabled Enables tagging functionality. boolean true com.unraveldata.tagging.script.enabled Enables tagging. boolean false com.unraveldata.app.tagging.script.path Specifies tagging script path to use when enabled =true. string (pat...", 
"body" : "Property\/Description Set by user Unit Default com.unraveldata.tagging.enabled Enables tagging functionality. boolean true com.unraveldata.tagging.script.enabled Enables tagging. boolean false com.unraveldata.app.tagging.script.path Specifies tagging script path to use when enabled =true. string (path) \/usr\/local\/unravel\/etc\/apptag.py com.unraveldata.app.tagging.script.method.name The name of the method in the python script that generates the tagging dictionary. string generate_unravel_tags " }, 
{ "title" : "Tez", 
"url" : "102428-properties-tez.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Tez", 
"snippet" : "Property\/Description Set by user Unit Default yarn.ats.webapp.username Username required for authentication to the Application Timeline Server (if authentication is required). Optional string - yarn.ats.webapp.password Password required for authentication to the Application Timeline Server (if authe...", 
"body" : "Property\/Description Set by user Unit Default yarn.ats.webapp.username Username required for authentication to the Application Timeline Server (if authentication is required). Optional string - yarn.ats.webapp.password Password required for authentication to the Application Timeline Server (if authentication is required). Optional string - Property\/Description Set by user Unit Default com.unraveldata.yarn.timeline-service.webapp.address The http address of the Timeline service web application. Optional string (URL) - com.unraveldata.yarn.timeline-service.port Timeline service port. number 8188 Property\/Descripton Set by user Unit Default com.unraveldata.tez.app.ats.connect.timeout.millis HTTP Connect timeout for ATS connections. ms 300000 com.unraveldata.tez.app.ats.read.timeout.millis HTTP Read timeout for ATS connections. ms 300000 com.unraveldata.tez.app.ats.poll.timeout.millis Controls the timeout after which we will stop trying to poll ATS if the polling is failing. ms 120000 com.unraveldata.tez.ats.poll.interval.millis Interval between consecutive polls of ATS if the polling fails. ms 10000 com.unraveldata.tez.ats.poll.max.retries Maximum number of retries if the polling of ATS fails. integer 30 The following properties defaults shouldn't need to be changed. com.unraveldata.tez.events.low.tasks integer 25 com.unraveldata.tez.events.high.tasks integer 50 com.unraveldata.tez.events.min.task.millis ms 2000 com.unraveldata.tez.events.max.task.millis ms 50000 com.unraveldata.tez.events.task.percentage percent 0.2 " }, 
{ "title" : "Yarn Timeline", 
"url" : "102429-properties-timeline.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Yarn Timeline", 
"snippet" : "Application Timeline Server (ATS) authentication Property\/Description Set by user Unit Default yarn.ats.webapp.username Username required for authentication to the Application Timeline Server (if authentication is required). Optional string - yarn.ats.webapp.password Password required for authentica...", 
"body" : "Application Timeline Server (ATS) authentication Property\/Description Set by user Unit Default yarn.ats.webapp.username Username required for authentication to the Application Timeline Server (if authentication is required). Optional string - yarn.ats.webapp.password Password required for authentication to the Application Timeline Server (if authentication is required). Optional string - Yarn Timeline service Property\/Description Set by user Unit Default com.unraveldata.yarn.timeline-service.webapp.address The http address of the Timeline service web application. Optional string (URL) - com.unraveldata.yarn.timeline-service.port Timeline service port. number 8188 " }, 
{ "title" : "Top X", 
"url" : "102430-properties-topx.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Advanced topics \/ Unravel properties \/ Top X", 
"snippet" : "Property\/Description Set by user Unit Default com.unraveldata.ngui.topx.enabled Controls the generation of Top X reports. true : report enabled. false : report are disabled. boolean true...", 
"body" : "Property\/Description Set by user Unit Default com.unraveldata.ngui.topx.enabled Controls the generation of Top X reports. true : report enabled. false : report are disabled. boolean true " }, 
{ "title" : "Appendices", 
"url" : "102431-appx.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Appendices", 
"snippet" : "...", 
"body" : "" }, 
{ "title" : "Unravel server reference", 
"url" : "102432-appx-server-daemon.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Appendices \/ Unravel server reference", 
"snippet" : "This reference covers both on-premises and cloud deployments. Some daemons and properties only apply to on-premises deployments....", 
"body" : "This reference covers both on-premises and cloud deployments. Some daemons and properties only apply to on-premises deployments. " }, 
{ "title" : "Daemons", 
"url" : "102432-appx-server-daemon.html#UUID-4ef0b514-12de-f43e-9193-fd56b9fca26c_id_ServerDaemonReference-_bwaxof3tb14eUnravelServerDaemons", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Appendices \/ Unravel server reference \/ Daemons", 
"snippet" : "The Unravel service is composed of many daemons which are summarized in the next table. The single script \/etc\/init.d\/unravel_all.sh can be used with a start or stop or restart argument to control all the daemons on a host, in the correct order. The suffix _N means 1, 2, 3, or 4 separate daemons. Da...", 
"body" : "The Unravel service is composed of many daemons which are summarized in the next table. The single script \/etc\/init.d\/unravel_all.sh can be used with a start or stop or restart argument to control all the daemons on a host, in the correct order. The suffix _N means 1, 2, 3, or 4 separate daemons. Daemon Logical Name Description unravel_db bundled database (on a custom port) unravel_ds Datastore REST API HTTP server unravel_ew _N Event Worker unravel_hhwe Hive Hook Worker EMR unravel_hl Hitdoc Loader unravel_host _N Host monitor unravel_ja \"Job Analyzer\" summarizes jobs unravel_jcs2 Job Collector Sensor YARN unravel_jcse2 Job Collector Sensor YARN for EMR unravel_jcw2 _N Job Collector Sensor Worker YARN unravel_k bundled Kafka (on a custom port) unravel_km Kafka Monitor unravel_lr Log Receiver unravel_ma _N Metrics Analyzer unravel_ngui aNGular Web UI unravel_os4 Oozie v4 Sensor unravel_pw Partition Worker unravel_s _N Elasticsearch unravel_sw _N Spark Worker unravel_tc bundled TomCat (port 4020), internal REST API unravel_td \"Tidy Dir\" cleans up and archives hdfs directories, db retention cleaner. unravel_tw Table Worker unravel_ud User Digest (report generator) unravel_us _N Universal sensor\/Impala unravel_zk _N bundled Zookeeper (on a custom port) " }, 
{ "title" : "Adjustable properties", 
"url" : "102432-appx-server-daemon.html#UUID-4ef0b514-12de-f43e-9193-fd56b9fca26c_id_ServerDaemonReference-_ranitvt6e5pgUnravelServerAdjustableProperties", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Appendices \/ Unravel server reference \/ Adjustable properties", 
"snippet" : "The file \/usr\/local\/unravel\/etc\/unravel.properties contains settings that can be preserved during an RPM upgrade. These properties are described in the following table: Property Type\/Property Name\/Description Default Value General Unravel com.unraveldata.tmpdir The base location for Unravel process ...", 
"body" : "The file \/usr\/local\/unravel\/etc\/unravel.properties contains settings that can be preserved during an RPM upgrade. These properties are described in the following table: Property Type\/Property Name\/Description Default Value General Unravel com.unraveldata.tmpdir The base location for Unravel process control files where Unravel's temp files reside. \/srv\/unravel\/tmp HDFS com.unraveldata.hdfs.batch.monitoring.interval.sec Number of seconds between checks for presence of hive queries and MR logs to load into Unravel for batch visibility; should be between 300 and 1800 (inclusive). 300 com.unraveldata.hdfs.interactive.monitoring.interval.sec Number of seconds between checks for presence of hive queries and MR logs to load into Unravel for interactive visibility; should be between 5 and 60 (inclusive). 30 JDBC unravel.jdbc.username Unravel database user. Set by user unravel.jdbc.password Password for unravel.jdbc.username . Set by user unravel.jdbc.url URL for jdbc, determined by your database. Example: jdbc:mysql:\/\/127.0.0.1:3306\/unravel_mysql_prod Set by user Kafka com.unraveldata.kafka.broker_list   127.0.0.1:4091 Oozie com.unraveldata.oozie.fetch.interval.sec Controls the rate the Oozie REST server is polled. Seconds between intervals for fetching Oozie workflow status. 120 com.unraveldata.oozie.fetch.num Number of workflows to pull in each API call. 100 oozie.server.url The Oozie server URL to be monitored by Unravel. If multiple servers exist, the server url can be a comma separated string where each part is ip of 1 Oozie server, e.g., http:\/\/ip-10-0-0-110.ec2.internal:11000,http:\/\/ip-10-0-0-114.ec2.internal:11000 - Zookeeper com.unraveldata.zk.quorum Embedded Zookeeper ensemble in form host1:port1,host2:port2. 127.0.0.1:4181 " }, 
{ "title" : "Adjustable environment settings", 
"url" : "102432-appx-server-daemon.html#UUID-4ef0b514-12de-f43e-9193-fd56b9fca26c_id_ServerDaemonReference-_dhjhvxwiog21AdjustableEnvironmentSettings", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Appendices \/ Unravel server reference \/ Adjustable environment settings", 
"snippet" : "Unravel runs the source command on \/usr\/local\/unravel\/etc\/unravel.ext.sh to allow site-specific environment variables to be set. The following table shows possible choices: Environment Variable Description Default JAVA_HOME The standard way to specify the home directory of Oracle Java so that $JAVA_...", 
"body" : "Unravel runs the source command on \/usr\/local\/unravel\/etc\/unravel.ext.sh to allow site-specific environment variables to be set. The following table shows possible choices: Environment Variable Description Default JAVA_HOME The standard way to specify the home directory of Oracle Java so that $JAVA_HOME\/bin\/java is the jvm. See the Compatibility Matrix for compatible JAVA versions. compmatrix-platform Should use update-alternatives to make correct Java first choice. JAVA_EXT_OPTS Last chance arguments to jvm to override other settings. Unset HADOOP_CONF_DIR The directory containing the hadoop config files core-site.xml , hdfs-site.xml , and mapred-site.xml . As discovered by running hadoop fs -ls . UNRAVEL_DATA_DIR A base directory owned by user unravel . During installation Unravel creates multiple sub directories for holding persistent data ( db_data , k_data, , zk_data ), and also tmp_data if property com.unraveldata.tmpdir is not set. \/srv\/unravel UNRAVEL_LISTEN_PORT The Web UI port on the primary or standalone Unravel installation ( service unravel_ngui ) which listens on 0.0.0.0. 3000 UNRAVEL_LOG_DIR A destination directory owned by run-as user for log files. \/usr\/local\/unravel\/logs " }, 
{ "title" : "Adjustable root environment settings", 
"url" : "102432-appx-server-daemon.html#UUID-4ef0b514-12de-f43e-9193-fd56b9fca26c_id_ServerDaemonReference-AdjustableRootEnvironmentSettings", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Appendices \/ Unravel server reference \/ Adjustable root environment settings", 
"snippet" : "Unravel's init.d script runs the source command on \/etc\/unravel_ctl to allow site-specific environment variables for the \"run-as\" and group membership of the daemons that Unravel Server runs: Environment Variable Description Default if not Set RUN_AS The \/etc\/init.d\/unravel_* initialization scripts ...", 
"body" : "Unravel's init.d script runs the source command on \/etc\/unravel_ctl to allow site-specific environment variables for the \"run-as\" and group membership of the daemons that Unravel Server runs: Environment Variable Description Default if not Set RUN_AS The \/etc\/init.d\/unravel_* initialization scripts use this to determine which user runs the daemons. unravel USE_GROUP The primary group membership of the user that runs the daemons. unravel " }, 
{ "title" : "Directories and files", 
"url" : "102432-appx-server-daemon.html#UUID-4ef0b514-12de-f43e-9193-fd56b9fca26c_id_ServerDaemonReference-_37le18boksr8UnravelServerDirectoriesandFiles", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Appendices \/ Unravel server reference \/ Directories and files", 
"snippet" : "The following is a cross-reference of notable directories and files used by Unravel Server: Path\/Purpose Expected Size Notes \/etc\/unravel_ctl Control file for run-as. n\/a Must be owned by root for security reasons. \/etc\/init.d\/unravel_all.sh Convenience script for Unravel start, restart, status, and...", 
"body" : "The following is a cross-reference of notable directories and files used by Unravel Server: Path\/Purpose Expected Size Notes \/etc\/unravel_ctl Control file for run-as. n\/a Must be owned by root for security reasons. \/etc\/init.d\/unravel_all.sh Convenience script for Unravel start, restart, status, and stop ; controls daemons in proper order. n\/a For multi-host installations, run this script on both primary and secondary Unravel host. \/usr\/local\/unravel Unravel Server installation directory. 1-2.5GB This directory is created by installing the Unravel RPM; this is a fixed destination. \/usr\/local\/unravel\/etc\/unravel.ext.sh An optional file for overriding JAVA_HOME or other settings as shown in table above. n\/a Optional; example syntax: export JAVA_HOME=\/path \/usr\/local\/unravel\/etc\/unravel.properties Site-specific settings for Unravel. n\/a Keep a \"golden\" copy of this file; rpm -U upgrades don't overwrite this. \/usr\/local\/unravel\/etc\/unravel.version.properties Version-specific values for Unravel like version number, build timestamp. n\/a Updated during upgrades; do not modify this reference file in order to preserve traceability \/usr\/local\/unravel\/logs Logs written by Unravel daemons. ~3.5GB max Each daemon has a maximum of 100MB of logs, auto-rolled; use a symlink to put on another partition. \/srv\/unravel\/ Base directory for Unravel Server data kept separately from installation directory; contains messaging data for process coordination, bundled db, Elasticsearch indexes, temporary files. 2-900GB; depending on activity level, retention This directory or its subdirectories can be a symlink(s) to other volumes for disk io performance reasons to distribute load over multiple volumes. If this is an EBS on AWS, then it must be provisioned for max available IOPS and the Unravel Server must be EBS optimized. " }, 
{ "title" : "Troubleshooting", 
"url" : "102433-trouble.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Troubleshooting", 
"snippet" : "If you can't reach Unravel Server, ping your LAN DNS....", 
"body" : "If you can't reach Unravel Server, ping your LAN DNS. " }, 
{ "title" : "Sending Diagnostics to Unravel Support", 
"url" : "102434-trouble-diag.html", 
"breadcrumbs" : "Unravel 4.5.2 Documentation \/ Troubleshooting \/ Sending Diagnostics to Unravel Support", 
"snippet" : "You must have access to, and an Id for, Unravel at Exavault . If you do not have or know your ID contact Unravel Support . You can collect the data via the UX or command line. Gathering diagnostic data From the UX Click admin-username > Manage . Select the Run Diagnostics tab and click Download Supp...", 
"body" : "You must have access to, and an Id for, Unravel at Exavault . If you do not have or know your ID contact Unravel Support . You can collect the data via the UX or command line. Gathering diagnostic data From the UX Click admin-username > Manage . Select the Run Diagnostics tab and click Download Support Bundle . Note: to examine the diagnostic information in the UI click Load Latest Diagnostics . The diagnostics file is named unravel-suppport-bundle.txt . From the command line Run \/usr\/local\/unravel\/install_bin\/pushlogs.sh to bundle the diagnostic data and logs in a tar file. Upon completion the file is saved to \/srv\/unravel\/tmp . Your file is name something similar to unravel_logs_-_cdh2_20490924t1302.tar.gz . Uploading the diagnostics Download the generated file. Log on to Unravel at Exavault . Click on Upload to upload the diagnostic file. " }
]
$(document).trigger('search.ready');
});